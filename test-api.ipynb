{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các dòng vượt quá độ dài cho phép:\n",
      "  - Dòng 7, cột 'output:': Độ dài 5120, 50 ký tự đầu: '{\n",
      "  \"Conference dates\": \"March 31 - April 2, 2025\"'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def kiem_tra_do_dai_csv(duong_dan_file_csv):\n",
    "    \"\"\"\n",
    "    Đọc file CSV và kiểm tra độ dài chuỗi trong cột \"input:\" và \"output:\".\n",
    "\n",
    "    Args:\n",
    "        duong_dan_file_csv (str): Đường dẫn đến file CSV.\n",
    "\n",
    "    Returns:\n",
    "        list: Một danh sách các thông báo lỗi.  Mỗi thông báo lỗi là một tuple\n",
    "              (so_dong, cot, do_dai_thuc_te, chuoi_ngan).  Trả về danh sách rỗng nếu không có lỗi.\n",
    "    \"\"\"\n",
    "\n",
    "    cac_loi = []\n",
    "\n",
    "    try:\n",
    "        with open(duong_dan_file_csv, 'r', encoding='utf-8') as file_csv:\n",
    "            doc_csv = csv.DictReader(file_csv)\n",
    "            for so_dong, dong in enumerate(doc_csv, start=2):  # Bắt đầu từ dòng 2 (bỏ qua header)\n",
    "                try:\n",
    "                    input_text = dong.get('input:')\n",
    "                    output_text = dong.get('output:')\n",
    "\n",
    "                    if input_text is not None and len(input_text) > 40000:\n",
    "                        chuoi_ngan = input_text[:50]\n",
    "                        cac_loi.append((so_dong, 'input:', len(input_text), chuoi_ngan))\n",
    "                    \n",
    "                    if output_text is not None and len(output_text) > 5000:\n",
    "                        chuoi_ngan = output_text[:50]\n",
    "                        cac_loi.append((so_dong, 'output:', len(output_text), chuoi_ngan))\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Lỗi khi xử lý dòng {so_dong}: {e}\")\n",
    "                    cac_loi.append((so_dong, 'Lỗi xử lý dòng', str(e), \"\"))  # Thêm thông tin lỗi vào danh sách\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file CSV tại '{duong_dan_file_csv}'.\")\n",
    "        return [(\"File\", \"Không tìm thấy file\", duong_dan_file_csv, \"\")] #Trả về một list với thông báo lỗi\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định khi đọc file CSV: {e}\")\n",
    "        return [(\"File\", \"Lỗi đọc file\", str(e), \"\")]\n",
    "\n",
    "    return cac_loi\n",
    "\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "duong_dan_file = './extract_infor.csv'  # Thay đổi đường dẫn file của bạn ở đây\n",
    "cac_loi = kiem_tra_do_dai_csv(duong_dan_file)\n",
    "\n",
    "if cac_loi:\n",
    "    print(\"Các dòng vượt quá độ dài cho phép:\")\n",
    "    for so_dong, cot, do_dai, chuoi_ngan in cac_loi:\n",
    "        print(f\"  - Dòng {so_dong}, cột '{cot}': Độ dài {do_dai}, 50 ký tự đầu: '{chuoi_ngan}'\")\n",
    "else:\n",
    "    print(\"Không có dòng nào vượt quá độ dài cho phép.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Không có lỗi nào được tìm thấy.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def kiem_tra_csv_va_json(duong_dan_file_csv, context_size=50):\n",
    "    \"\"\"\n",
    "    Đọc file CSV, kiểm tra xem cột \"output:\" có phải là JSON hợp lệ hay không,\n",
    "    và log dòng lỗi ra cùng với context xung quanh vị trí lỗi.\n",
    "\n",
    "    Args:\n",
    "        duong_dan_file_csv (str): Đường dẫn đến file CSV.\n",
    "        context_size (int): Số lượng ký tự trước và sau vị trí lỗi để hiển thị.\n",
    "\n",
    "    Returns:\n",
    "        list: Một danh sách các thông báo lỗi. Mỗi thông báo lỗi là một tuple\n",
    "              (so_dong, cot, thong_tin_loi). Trả về danh sách rỗng nếu không có lỗi.\n",
    "    \"\"\"\n",
    "\n",
    "    cac_loi = []\n",
    "\n",
    "    try:\n",
    "        with open(duong_dan_file_csv, 'r', encoding='utf-8') as file_csv:\n",
    "            doc_csv = csv.DictReader(file_csv)\n",
    "            for so_dong, dong in enumerate(doc_csv, start=2):  # Bắt đầu từ dòng 2 (bỏ qua header)\n",
    "                try:\n",
    "                    input_text = dong.get('input:')\n",
    "                    output_text = dong.get('output:')\n",
    "\n",
    "                    # Kiểm tra JSON\n",
    "                    if output_text is not None:\n",
    "                        try:\n",
    "                            json.loads(output_text)\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            # Log dòng lỗi ra và context\n",
    "                            position = e.pos\n",
    "                            start = max(0, position - context_size)\n",
    "                            end = min(len(output_text), position + context_size)\n",
    "                            context = output_text[start:end]\n",
    "\n",
    "                            print(f\"Lỗi JSON ở dòng {so_dong}: {e}\")\n",
    "                            print(f\"Vị trí lỗi: {position}\")\n",
    "                            print(f\"Context: ...{context}...\")\n",
    "                            cac_loi.append((so_dong, 'output:', f\"Lỗi JSON: {e}\"))\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Lỗi khi xử lý dòng {so_dong}: {e}\")\n",
    "                    cac_loi.append((so_dong, 'Lỗi xử lý dòng', str(e)))  # Thêm thông tin lỗi vào danh sách\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file CSV tại '{duong_dan_file_csv}'.\")\n",
    "        return [(\"File\", \"Không tìm thấy file\", duong_dan_file_csv)]  # Trả về một list với thông báo lỗi\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định khi đọc file CSV: {e}\")\n",
    "        return [(\"File\", \"Lỗi đọc file\", str(e))]\n",
    "\n",
    "    return cac_loi\n",
    "\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "duong_dan_file = './fine_tune_extract_infor.csv'  # Thay đổi đường dẫn file của bạn ở đây\n",
    "cac_loi = kiem_tra_csv_va_json(duong_dan_file)\n",
    "\n",
    "if cac_loi:\n",
    "    print(\"Các lỗi được tìm thấy:\")\n",
    "    for so_dong, cot, thong_tin_loi in cac_loi:\n",
    "        print(f\"  - Dòng {so_dong}, cột '{cot}': {thong_tin_loi}\")\n",
    "else:\n",
    "    print(\"Không có lỗi nào được tìm thấy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ACS/IEEE - AICCSA2024\n",
      "Link: https://aiccsa.net/AICCSA2024/\n",
      "Snippet: The ACS/IEEE 21st International Conference on Computer Systems and Applications (AICCSA 2024) · 22-26 October 2024 – Sousse, Tunisia.\n",
      "\n",
      "Title: Proceedings ACS/IEEE International Conference on Computer ...\n",
      "Link: https://ieeexplore.ieee.org/xpl/conhome/7431/proceeding\n",
      "Snippet: Read all the papers in Proceedings ACS/IEEE International Conference on Computer Systems and Applications | IEEE Conference | IEEE Xplore.\n",
      "\n",
      "Title: AICCSA – ACS/IEEE\n",
      "Link: https://aiccsa.net/\n",
      "Snippet: The ACS/IEEE International Conference on Computer Systems and Applications (AICCSA) is the premier research conference on computing in the Arab region. It is co ...\n",
      "\n",
      "Title: AICCSA: ACS/IEEE International Conference on Computer Systems ...\n",
      "Link: http://www.wikicfp.com/cfp/program?id=98\n",
      "Snippet: AICCSA: ACS/IEEE International Conference on Computer Systems and Applications 2026 2025 2024 ...\n",
      "\n",
      "Title: NIST Leaders Address the 21st International ACS/IEEE International ...\n",
      "Link: https://www.nist.gov/news-events/news/2024/12/nist-leaders-address-21st-international-acsieee-international-conference\n",
      "Snippet: Dec 1, 2024 ... NIST Leaders Address the 21st International ACS/IEEE International Conference on Computer Systems and Applications in Tunisia. December 1, 2024 ...\n",
      "\n",
      "Title: ACS/IEEE International Conference on Computer Systems and ... - dblp\n",
      "Link: https://dblp.org/db/conf/aiccsa/index\n",
      "Snippet: ACS/IEEE International Conference on Computer Systems and Applications (AICCSA) · Venue statistics · 21st AICCSA 2024: Sousse, Tunisia · 20th AICCSA 2023: Giza, ...\n",
      "\n",
      "Title: 2024 IEEE/ACS 21st International Conference on Computer ...\n",
      "Link: https://www.computer.org/csdl/proceedings/aiccsa/2024/251xa7vWYAo\n",
      "Snippet: 2024 IEEE/ACS 21st International Conference on Computer Systems and Applications (AICCSA). Oct. 22 2024 to Oct. 26 2024. Sousse, Tunisia. ISBN: 979-8-3315- ...\n",
      "\n",
      "Title: Anomaly Based Intrusion Detection using Large Language Models ...\n",
      "Link: https://www.nist.gov/publications/anomaly-based-intrusion-detection-using-large-language-models\n",
      "Snippet: Jun 15, 2024 ... The ACS/IEEE 21st International Conference on Computer Systems and Applications (AICCSA 2024). Conference Dates. October 22-26, 2024. Conference ...\n",
      "\n",
      "Title: AICCSA 2025: 22nd ACS/IEEE International Conference on ...\n",
      "Link: https://easychair.org/cfp/AICCSA2025\n",
      "Snippet: AICCSA 2025: 22nd ACS/IEEE International Conference on Computer Systems and Applications. Doha, Qatar, October 19-22, 2025. Conference website, http://aiccsa.\n",
      "\n",
      "Title: Proceedings of IEEE/ACS International Conference on Computer ...\n",
      "Link: https://www.scimagojr.com/journalsearch.php?q=21100198533&tip=sid&clean=0\n",
      "Snippet: The ACS/IEEE International Conference on Computer Systems and Applications (AICCSA) is the premier conference covering all contemporary areas in computer ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"AIzaSyDK5eYs2PBVYP3l9UH0YMIFshxBMq6EsE8\"  # API Key từ Google Cloud\n",
    "CX = \"45ff1f0418c594bde\"  # Search Engine ID từ Google Custom Search Engine\n",
    "query = \"ACS/IEEE International Conference on Computer Systems and Applications (AICCSA) conference 2025 OR 2026 OR 2024\"\n",
    "# or_terms = \"2024 OR 2025\"\n",
    "\n",
    "url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={API_KEY}&cx={CX}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "# print(data)\n",
    "\n",
    "# In kết quả tìm kiếm\n",
    "for item in data.get(\"items\", []):\n",
    "    print(f\"Title: {item['title']}\")\n",
    "    print(f\"Link: {item['link']}\")\n",
    "    print(f\"Snippet: {item['snippet']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_KEY = \"AIzaSyDK5eYs2PBVYP3l9UH0YMIFshxBMq6EsE8\"  # Thay thế bằng API Key của bạn từ Google Cloud\n",
    "CX = \"45ff1f0418c594bde\"  # Thay thế bằng Search Engine ID của bạn từ Google Custom Search Engine\n",
    "ISSN = \"1471-0080\"\n",
    "# query = f'\"{ISSN}\"'  # Tìm kiếm chính xác ISSN.  Dấu ngoặc kép để tìm chính xác.\n",
    "query = ISSN # Tìm chính xác ISSN\n",
    "\n",
    "# Thêm các tham số cho tìm kiếm hình ảnh\n",
    "url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={API_KEY}&cx={CX}&searchType=image\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raises HTTPError for bad requests (4xx or 5xx)\n",
    "    data = response.json()\n",
    "\n",
    "    # In kết quả tìm kiếm hình ảnh\n",
    "    if \"items\" in data:\n",
    "        for item in data[\"items\"]:\n",
    "            print(f\"Title: {item.get('title', 'No title')}\")  # Xử lý trường hợp không có tiêu đề\n",
    "            print(f\"Image Link: {item.get('link', 'No link')}\") # Lấy link ảnh\n",
    "            print(f\"Context Link: {item.get('image', {}).get('contextLink', 'No context link')}\") # link trang chứa ảnh (nếu cần)\n",
    "            print(f\"Snippet: {item.get('snippet', 'No snippet')}\\n\")\n",
    "    else:\n",
    "        print(\"No image results found.\")\n",
    "\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    if response:\n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        try:\n",
    "            print(f\"Response content: {response.json()}\")  # In nội dung JSON nếu có\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Response content (not JSON): {response.text}\")\n",
    "except Exception as e:\n",
    "      print(f\"An unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đọc file './evaluate_all.csv'...\n",
      "Đang đọc file './crawl_again.csv'...\n",
      "Chuẩn hóa cột 'title' và 'acronym' (loại bỏ '(...)' và khoảng trắng thừa)...\n",
      "  - Đang xử lý file 'evaluate.csv'\n",
      "    - Đã làm sạch cột 'title'\n",
      "    - Đã làm sạch cột 'acronym'\n",
      "  - Đang xử lý file 'crawl_again.csv'\n",
      "    - Đã làm sạch cột 'title'\n",
      "    - Đã làm sạch cột 'acronym'\n",
      "Đang tiến hành cập nhật dữ liệu...\n",
      "Đã xử lý 79 dòng từ './crawl_again.csv'.\n",
      "Đã thực hiện 78 lượt cập nhật dòng (có thể bao gồm các dòng trùng lặp trong evaluate.csv).\n",
      "Đang lưu kết quả vào file './evaluate_updated.csv'...\n",
      "Hoàn thành!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os # Thêm thư viện os để kiểm tra file tồn tại\n",
    "\n",
    "# --- Cấu hình ---\n",
    "evaluate_file = './evaluate_all.csv'\n",
    "crawl_again_file = './crawl_again.csv'\n",
    "output_file = './evaluate_updated.csv' # Lưu vào file mới để tránh ghi đè file gốc khi chưa chắc chắn\n",
    "# Nếu muốn ghi đè trực tiếp file evaluate.csv, đổi output_file = evaluate_file\n",
    "\n",
    "# --- Kiểm tra sự tồn tại của file ---\n",
    "if not os.path.exists(evaluate_file):\n",
    "    print(f\"Lỗi: Không tìm thấy file '{evaluate_file}'\")\n",
    "    exit()\n",
    "if not os.path.exists(crawl_again_file):\n",
    "    print(f\"Lỗi: Không tìm thấy file '{crawl_again_file}'\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Đang đọc file '{evaluate_file}'...\")\n",
    "try:\n",
    "    df_evaluate = pd.read_csv(evaluate_file)\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi đọc file '{evaluate_file}': {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Đang đọc file '{crawl_again_file}'...\")\n",
    "try:\n",
    "    df_crawl_again = pd.read_csv(crawl_again_file)\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi đọc file '{crawl_again_file}': {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Kiểm tra các cột cần thiết ---\n",
    "required_columns = ['title', 'acronym']\n",
    "if not all(col in df_evaluate.columns for col in required_columns):\n",
    "    print(f\"Cảnh báo: File '{evaluate_file}' có thể thiếu cột 'title' hoặc 'acronym'.\")\n",
    "    # Không exit() để cố gắng xử lý nếu cột vẫn tồn tại trong file kia\n",
    "if not all(col in df_crawl_again.columns for col in required_columns):\n",
    "    print(f\"Lỗi: File '{crawl_again_file}' thiếu cột 'title' hoặc 'acronym'. Không thể tiến hành khớp.\")\n",
    "    exit()\n",
    "\n",
    "# --- Chuẩn hóa cột 'title' và 'acronym' trong cả hai DataFrame ---\n",
    "print(\"Chuẩn hóa cột 'title' và 'acronym' (loại bỏ '(...)' và khoảng trắng thừa)...\")\n",
    "\n",
    "regex_pattern = r'\\s*\\(.*\\)\\s*' # Regex để xóa (...) và khoảng trắng xung quanh\n",
    "columns_to_clean = ['title', 'acronym']\n",
    "dfs_to_clean = {'evaluate': df_evaluate, 'crawl_again': df_crawl_again}\n",
    "\n",
    "for df_name, df in dfs_to_clean.items():\n",
    "    print(f\"  - Đang xử lý file '{df_name}.csv'\")\n",
    "    for col in columns_to_clean:\n",
    "        if col in df.columns:\n",
    "            # 1. Xử lý NaN và chuyển thành string\n",
    "            df[col] = df[col].fillna('').astype(str)\n",
    "            # 2. Loại bỏ (...)\n",
    "            df[col] = df[col].str.replace(regex_pattern, '', regex=True)\n",
    "            # 3. Dọn dẹp khoảng trắng\n",
    "            df[col] = df[col].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "            print(f\"    - Đã làm sạch cột '{col}'\")\n",
    "        elif df_name == 'crawl_again': # Chỉ báo lỗi nếu cột thiếu trong crawl_again\n",
    "             print(f\"    - Lỗi: Cột '{col}' không tìm thấy trong {df_name}.csv.\")\n",
    "             exit()\n",
    "        else: # Chỉ cảnh báo nếu cột thiếu trong evaluate\n",
    "             print(f\"    - Cảnh báo: Cột '{col}' không tìm thấy trong {df_name}.csv.\")\n",
    "\n",
    "\n",
    "# --- Đảm bảo các cột dùng để join là kiểu string sau khi làm sạch ---\n",
    "# (Quan trọng để tránh lỗi khớp loại dữ liệu)\n",
    "try:\n",
    "    df_evaluate['title'] = df_evaluate['title'].astype(str)\n",
    "    df_evaluate['acronym'] = df_evaluate['acronym'].astype(str)\n",
    "    df_crawl_again['title'] = df_crawl_again['title'].astype(str)\n",
    "    df_crawl_again['acronym'] = df_crawl_again['acronym'].astype(str)\n",
    "except KeyError as e:\n",
    "    print(f\"Lỗi: Không tìm thấy cột '{e}' sau khi cố gắng chuẩn hóa. Kiểm tra lại tên cột.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Đang tiến hành cập nhật dữ liệu...\")\n",
    "\n",
    "# Tạo bản sao của df_evaluate để cập nhật, hoặc cập nhật trực tiếp\n",
    "# Cập nhật trực tiếp df_evaluate trong trường hợp này là ổn\n",
    "# df_updated = df_evaluate.copy() # Không cần thiết nếu cập nhật inplace\n",
    "\n",
    "# Đặt 'title' và 'acronym' (đã chuẩn hóa) làm index để khớp hiệu quả\n",
    "# drop=False giữ lại các cột title/acronym trong DataFrame sau khi set index\n",
    "# verify_integrity=False cho phép index không duy nhất\n",
    "try:\n",
    "    # Quan trọng: Đặt index cho df_evaluate *trước* khi cập nhật\n",
    "    df_evaluate.set_index(['title', 'acronym'], inplace=True, drop=False, verify_integrity=False)\n",
    "    df_crawl_again.set_index(['title', 'acronym'], inplace=True, drop=False, verify_integrity=False)\n",
    "except KeyError as e:\n",
    "     print(f\"Lỗi: Không tìm thấy cột để đặt làm index: {e}\")\n",
    "     # Reset index nếu bước trước đó gây lỗi, tránh trạng thái không nhất quán\n",
    "     df_evaluate.reset_index(drop=True, inplace=True, errors='ignore')\n",
    "     df_crawl_again.reset_index(drop=True, inplace=True, errors='ignore')\n",
    "     exit()\n",
    "except Exception as e:\n",
    "     print(f\"Lỗi không xác định khi đặt index: {e}\")\n",
    "     df_evaluate.reset_index(drop=True, inplace=True, errors='ignore')\n",
    "     df_crawl_again.reset_index(drop=True, inplace=True, errors='ignore')\n",
    "     exit()\n",
    "\n",
    "# Lấy danh sách các cột từ df_crawl_again để đảm bảo cập nhật đúng các cột\n",
    "update_columns = df_crawl_again.columns\n",
    "\n",
    "# Biến đếm số dòng đã cập nhật\n",
    "updated_rows_count = 0\n",
    "processed_crawl_rows = 0\n",
    "\n",
    "# Duyệt qua từng dòng trong df_crawl_again (đã chuẩn hóa và có index)\n",
    "for index, row_crawl in df_crawl_again.iterrows():\n",
    "    processed_crawl_rows += 1\n",
    "    # Index bây giờ là tuple (cleaned_title, cleaned_acronym)\n",
    "    if index in df_evaluate.index:\n",
    "        # Nếu tìm thấy index trong df_evaluate\n",
    "        # Cập nhật toàn bộ các cột có trong df_crawl_again cho dòng tương ứng trong df_evaluate\n",
    "        # Sử dụng .loc để truy cập và gán giá trị theo index\n",
    "        # row_crawl đã chứa dữ liệu đã chuẩn hóa title/acronym\n",
    "        try:\n",
    "            # Gán trực tiếp vào df_evaluate vì nó đã được set_index\n",
    "            df_evaluate.loc[index, update_columns] = row_crawl[update_columns].values\n",
    "            # Nếu có nhiều dòng trùng index trong df_evaluate, tất cả sẽ được cập nhật\n",
    "            # Đếm số lượng dòng thực sự bị ảnh hưởng trong df_evaluate\n",
    "            # Lưu ý: .loc[index] có thể trả về Series hoặc DataFrame nếu index trùng lặp\n",
    "            if isinstance(df_evaluate.loc[index], pd.DataFrame):\n",
    "                updated_rows_count += len(df_evaluate.loc[index])\n",
    "            else:\n",
    "                 updated_rows_count += 1 # Chỉ có 1 dòng được cập nhật cho index này\n",
    "            # print(f\"Đã cập nhật: Title='{index[0]}', Acronym='{index[1]}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi cập nhật index {index}: {e}\")\n",
    "            # Có thể thêm xử lý lỗi cụ thể hơn ở đây nếu cần\n",
    "    # else:\n",
    "        # print(f\"Không tìm thấy để cập nhật: Title='{index[0]}', Acronym='{index[1]}'\")\n",
    "\n",
    "# Cần điều chỉnh lại cách đếm vì cách trên có thể đếm lặp nếu crawl_again có trùng index\n",
    "# Đếm lại sau khi cập nhật xong thì chính xác hơn, nhưng cách trên cho ước lượng\n",
    "print(f\"Đã xử lý {processed_crawl_rows} dòng từ '{crawl_again_file}'.\")\n",
    "# Cách đếm chính xác hơn: so sánh df gốc và df sau cập nhật, nhưng phức tạp.\n",
    "# Thông báo dựa trên số lần gọi gán giá trị có thể chấp nhận được.\n",
    "print(f\"Đã thực hiện {updated_rows_count} lượt cập nhật dòng (có thể bao gồm các dòng trùng lặp trong evaluate.csv).\")\n",
    "\n",
    "\n",
    "# Đặt lại index về dạng số thứ tự mặc định cho df_evaluate\n",
    "df_evaluate.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# --- Lưu kết quả ---\n",
    "print(f\"Đang lưu kết quả vào file '{output_file}'...\")\n",
    "try:\n",
    "    # index=False để không ghi cột index của pandas vào file CSV\n",
    "    df_evaluate.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(\"Hoàn thành!\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi lưu file '{output_file}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đọc file CSV 1 (./1111.csv) thành công.\n",
      "Đọc file CSV 2 (./evaluate_updated.csv) thành công.\n",
      "Các cột bổ sung từ CSV 2 sẽ được giữ lại: ['cfpLink', 'impLink', 'information', 'conferenceDates', 'year', 'location', 'cityStateProvince', 'country', 'continent', 'type', 'submissionDate', 'notificationDate', 'cameraReadyDate', 'registrationDate', 'otherDate', 'topics', 'publisher', 'summary', 'callForPapers']\n",
      "Làm sạch dữ liệu tên và link.\n",
      "Gộp dữ liệu từ hai file CSV.\n",
      "Đã xử lý 921 dòng (bao gồm cả dòng mới từ file 2).\n",
      "Đã bỏ qua 36 dòng chỉ tồn tại trong file CSV 1.\n",
      "Tạo DataFrame kết quả với đầy đủ cột hoàn tất.\n",
      "Hoàn thành! Kết quả đã được ghi vào file: ./ket_qua_so_sanh_link_full_v5_csv_only.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # Cần thiết để xử lý giá trị NaN\n",
    "\n",
    "# --- Cấu hình ---\n",
    "# ----- THAY ĐỔI Ở ĐÂY -----\n",
    "csv_file1_path = './1111.csv'  # File CSV chứa dữ liệu \"cũ\" (thay cho Excel)\n",
    "csv_file2_path = './evaluate_updated.csv'    # File CSV chứa dữ liệu \"mới\"\n",
    "# Đổi tên file output nếu muốn\n",
    "output_csv_path = './ket_qua_so_sanh_link_full_v5_csv_only.csv'\n",
    "\n",
    "# Các cột cơ bản cần thiết trong mỗi file CSV\n",
    "cols_file1 = ['title', 'acronym', 'link'] # Tên cột mong đợi trong file CSV 1\n",
    "cols_file2 = ['title', 'acronym', 'link'] # Tên cột mong đợi trong file CSV 2\n",
    "\n",
    "# --- Đọc dữ liệu ---\n",
    "try:\n",
    "    # ----- THAY ĐỔI Ở ĐÂY -----\n",
    "    # Đọc file CSV 1 (dữ liệu cũ)\n",
    "    df_csv1 = pd.read_csv(\n",
    "        csv_file1_path,\n",
    "        usecols=cols_file1 # Chỉ đọc các cột cần thiết\n",
    "    )\n",
    "    # Đổi tên cột để phân biệt\n",
    "    df_csv1.columns = ['title_file1', 'acronym_file1', 'link_file1_raw']\n",
    "    print(f\"Đọc file CSV 1 ({csv_file1_path}) thành công.\")\n",
    "\n",
    "    # Đọc file CSV 2 (dữ liệu mới) - Đọc TẤT CẢ các cột\n",
    "    df_csv2 = pd.read_csv(csv_file2_path)\n",
    "    # Lưu lại danh sách các cột gốc từ CSV 2\n",
    "    original_csv2_columns = list(df_csv2.columns)\n",
    "    # Xác định các cột cơ bản và cột bổ sung trong CSV 2\n",
    "    base_columns_to_handle_csv2 = ['title', 'acronym', 'link']\n",
    "    extra_csv_columns = [col for col in original_csv2_columns if col not in base_columns_to_handle_csv2]\n",
    "\n",
    "    # Đổi tên các cột cơ bản trong CSV 2 để phân biệt và chuẩn bị merge\n",
    "    df_csv2.rename(columns={'title': 'title_file2',\n",
    "                            'acronym': 'acronym_file2',\n",
    "                            'link': 'link_file2'}, inplace=True)\n",
    "\n",
    "    print(f\"Đọc file CSV 2 ({csv_file2_path}) thành công.\")\n",
    "    print(f\"Các cột bổ sung từ CSV 2 sẽ được giữ lại: {extra_csv_columns}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Lỗi: Không tìm thấy một trong các file CSV:\\n- File 1: {csv_file1_path}\\n- File 2: {csv_file2_path}\")\n",
    "    print(f\"Chi tiết: {e}\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"Lỗi: Có vẻ cột trong file CSV không đúng như mong đợi. Chi tiết: {e}\")\n",
    "    print(f\"Hãy đảm bảo file '{csv_file1_path}' có các cột: {cols_file1}\")\n",
    "    print(f\"Hãy đảm bảo file '{csv_file2_path}' có các cột cơ bản: {cols_file2}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi không xác định khi đọc file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Chuẩn bị dữ liệu để khớp ---\n",
    "# Làm sạch cột tên hội nghị\n",
    "df_csv1['title_file1'] = df_csv1['title_file1'].astype(str).str.strip()\n",
    "df_csv2['title_file2'] = df_csv2['title_file2'].astype(str).str.strip()\n",
    "\n",
    "# ----- THAY ĐỔI TÊN HÀM CHO RÕ NGHĨA HƠN -----\n",
    "# --- Hàm xử lý và làm sạch danh sách link từ CSV 1 (có thể chứa nhiều link) ---\n",
    "def clean_multi_links(link_string):\n",
    "    \"\"\"Làm sạch chuỗi link có thể chứa nhiều link cách nhau bằng dấu phẩy.\"\"\"\n",
    "    if pd.isna(link_string):\n",
    "        return []\n",
    "    # Xử lý cả trường hợp có khoảng trắng thừa quanh dấu phẩy\n",
    "    links = [link.strip() for link in str(link_string).split(',') if link.strip()]\n",
    "    return links\n",
    "\n",
    "# --- Hàm làm sạch link đơn từ CSV 2 ---\n",
    "def clean_single_link(link):\n",
    "    \"\"\"Làm sạch một link đơn.\"\"\"\n",
    "    if pd.isna(link):\n",
    "        return None\n",
    "    cleaned = str(link).strip()\n",
    "    return cleaned if cleaned else None # Trả về None nếu link sau khi strip là rỗng\n",
    "\n",
    "# Áp dụng hàm làm sạch link cho CSV 2\n",
    "df_csv2['link_file2_cleaned'] = df_csv2['link_file2'].apply(clean_single_link)\n",
    "\n",
    "print(\"Làm sạch dữ liệu tên và link.\")\n",
    "\n",
    "# --- Gộp (Merge) hai DataFrame ---\n",
    "# Merge dựa trên tên hội nghị đã làm sạch từ hai file\n",
    "# Outer join để giữ tất cả các dòng ban đầu\n",
    "merged_df = pd.merge(\n",
    "    df_csv2, # File CSV 2 (mới) là bên trái\n",
    "    df_csv1, # File CSV 1 (cũ) là bên phải\n",
    "    left_on='title_file2',\n",
    "    right_on='title_file1',\n",
    "    how='outer',\n",
    "    indicator=True # Thêm cột _merge để biết dòng đến từ đâu\n",
    ")\n",
    "\n",
    "print(\"Gộp dữ liệu từ hai file CSV.\")\n",
    "\n",
    "# --- Xử lý kết quả sau khi gộp ---\n",
    "results = []\n",
    "processed_count = 0\n",
    "skipped_file1_only = 0\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    # Bỏ qua các dòng chỉ có trong file CSV 1 (dữ liệu cũ không có trong file mới)\n",
    "    # Giống logic ban đầu khi bỏ qua 'right_only' (chỉ có trong Excel)\n",
    "    if row['_merge'] == 'right_only':\n",
    "        skipped_file1_only += 1\n",
    "        continue # Bỏ qua\n",
    "\n",
    "    processed_count += 1\n",
    "\n",
    "    # Lấy link mới (file 2) đã được làm sạch\n",
    "    link_moi_sach = row['link_file2_cleaned'] # Sử dụng cột đã làm sạch\n",
    "\n",
    "    # Lấy và xử lý danh sách link cũ (file 1)\n",
    "    link_cu_raw_value = row['link_file1_raw'] # Giá trị gốc từ CSV 1\n",
    "    # Sử dụng hàm clean_multi_links vì file CSV cũ có thể có nhiều link\n",
    "    link_cu_list_sach = clean_multi_links(link_cu_raw_value)\n",
    "\n",
    "    # Xác định trạng thái kiểm tra link\n",
    "    check_status = ''\n",
    "    found_match = False\n",
    "\n",
    "    # Trường hợp có ở cả 2 file\n",
    "    if row['_merge'] == 'both':\n",
    "        if link_moi_sach is not None:\n",
    "            # Kiểm tra xem link mới có trong danh sách link cũ không\n",
    "            if link_moi_sach in link_cu_list_sach:\n",
    "                found_match = True\n",
    "                check_status = 'Giống'\n",
    "            elif not link_cu_list_sach: # Link mới có, link cũ không có (rỗng/thiếu)\n",
    "                 check_status = 'Khác (link cũ thiếu/trống)'\n",
    "            else: # Link mới có, link cũ có nhưng không khớp\n",
    "                check_status = 'Khác'\n",
    "        elif not link_cu_list_sach: # Cả link mới và cũ đều thiếu/rỗng\n",
    "             check_status = 'Thiếu cả 2 link'\n",
    "        else: # Link mới thiếu/rỗng, link cũ có\n",
    "             check_status = 'Khác (link mới thiếu)'\n",
    "\n",
    "    # Trường hợp chỉ có ở file CSV 2 (dữ liệu mới)\n",
    "    elif row['_merge'] == 'left_only':\n",
    "        check_status = 'Mới (chỉ có trong file 2)'\n",
    "        link_cu_raw_value = None # Đảm bảo link cũ là rỗng cho dòng mới\n",
    "\n",
    "    # Tạo dictionary chứa kết quả cho dòng hiện tại\n",
    "    # Ưu tiên lấy tên và viết tắt từ file 2 nếu có, nếu không lấy từ file 1\n",
    "    result_dict = {\n",
    "        'title': row['title_file2'] if pd.notna(row['title_file2']) else row['title_file1'],\n",
    "        'acronym': row['acronym_file2'] if pd.notna(row['acronym_file2']) else row['acronym_file1'],\n",
    "        'link_cu': str(link_cu_raw_value) if pd.notna(link_cu_raw_value) else '', # Hiển thị link cũ gốc\n",
    "        'link_moi': str(row['link_file2']) if pd.notna(row['link_file2']) else '', # Hiển thị link mới gốc\n",
    "        'check': check_status\n",
    "    }\n",
    "\n",
    "    # Thêm tất cả các cột bổ sung từ file CSV 2 gốc vào dictionary\n",
    "    # Giá trị được lấy trực tiếp từ `row` của merged_df\n",
    "    for col_title in extra_csv_columns:\n",
    "        # Cần kiểm tra xem cột đó có tồn tại trong dòng hiện tại không\n",
    "        # (quan trọng nếu merge tạo ra NaN cho các cột này ở dòng 'right_only', mặc dù ta đã skip)\n",
    "        result_dict[col_title] = row[col_title] if col_title in row and pd.notna(row[col_title]) else ''\n",
    "\n",
    "\n",
    "    results.append(result_dict)\n",
    "\n",
    "print(f\"Đã xử lý {processed_count} dòng (bao gồm cả dòng mới từ file 2).\")\n",
    "if skipped_file1_only > 0:\n",
    "    print(f\"Đã bỏ qua {skipped_file1_only} dòng chỉ tồn tại trong file CSV 1.\")\n",
    "\n",
    "# --- Tạo DataFrame kết quả cuối cùng ---\n",
    "# Xác định thứ tự cột mong muốn cho file output\n",
    "# Bắt đầu bằng các cột đã xử lý, tiếp theo là các cột bổ sung từ CSV 2 gốc\n",
    "output_column_order = [\n",
    "    'title', 'acronym', 'link_cu', 'link_moi', 'check'\n",
    "] + extra_csv_columns\n",
    "\n",
    "# Tạo DataFrame từ danh sách các dictionary\n",
    "# Chỉ định `columns` để đảm bảo đúng thứ tự và xử lý trường hợp thiếu cột\n",
    "df_output = pd.DataFrame(results, columns=output_column_order)\n",
    "\n",
    "# --- Xử lý giá trị NaN/None trong DataFrame kết quả ---\n",
    "# Thay thế NaN bằng chuỗi rỗng '' để file CSV dễ đọc hơn\n",
    "df_output.fillna('', inplace=True)\n",
    "\n",
    "print(\"Tạo DataFrame kết quả với đầy đủ cột hoàn tất.\")\n",
    "\n",
    "# --- Ghi kết quả ra file CSV mới ---\n",
    "try:\n",
    "    df_output.to_csv(output_csv_path, index=False, encoding='utf-8-sig') # utf-8-sig để Excel đọc tiếng Việt tốt\n",
    "    print(f\"Hoàn thành! Kết quả đã được ghi vào file: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi ghi file CSV đầu ra: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Không tìm thấy dòng nào chỉ có trong evaluate.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re  # Để sử dụng regular expression\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch chuỗi văn bản bằng cách loại bỏ nội dung trong ngoặc đơn và khoảng trắng thừa.\"\"\"\n",
    "    if pd.isna(text): # Check NaN trước khi làm bất cứ điều gì\n",
    "        return ''\n",
    "    text = str(text)  # Chắc chắn rằng nó là một chuỗi\n",
    "    regex_pattern = r'\\s*\\(.*\\)\\s*'\n",
    "    text = re.sub(regex_pattern, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_unique_rows(evaluate_file, crawl_again_file):\n",
    "    \"\"\"\n",
    "    Tìm các dòng trong evaluate.csv mà không có bản sao tương ứng trong\n",
    "    crawl_again.csv, dựa trên 'title' và 'acronym' đã được làm sạch.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(evaluate_file):\n",
    "        print(f\"Lỗi: Không tìm thấy file '{evaluate_file}'\")\n",
    "        return None\n",
    "    if not os.path.exists(crawl_again_file):\n",
    "        print(f\"Lỗi: Không tìm thấy file '{crawl_again_file}'\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df_evaluate = pd.read_csv(evaluate_file)\n",
    "        df_crawl_again = pd.read_csv(crawl_again_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file: {e}\")\n",
    "        return None\n",
    "\n",
    "    required_columns = ['title', 'acronym']\n",
    "    for df_name, df in [('evaluate.csv', df_evaluate), ('crawl_again.csv', df_crawl_again)]:\n",
    "      if not all(col in df.columns for col in required_columns):\n",
    "        print(f\"Lỗi: File '{df_name}' thiếu cột 'title' hoặc 'acronym'.\")\n",
    "        return None\n",
    "\n",
    "    # Làm sạch các cột 'title' và 'acronym' trong cả hai DataFrame\n",
    "    df_evaluate['cleaned_title'] = df_evaluate['title'].apply(clean_text)\n",
    "    df_evaluate['cleaned_acronym'] = df_evaluate['acronym'].apply(clean_text)\n",
    "\n",
    "    df_crawl_again['cleaned_title'] = df_crawl_again['title'].apply(clean_text)\n",
    "    df_crawl_again['cleaned_acronym'] = df_crawl_again['acronym'].apply(clean_text)\n",
    "\n",
    "    # Tạo khóa duy nhất từ các cột đã làm sạch\n",
    "    df_evaluate['key'] = df_evaluate['cleaned_title'] + '||' + df_evaluate['cleaned_acronym']\n",
    "    df_crawl_again['key'] = df_crawl_again['cleaned_title'] + '||' + df_crawl_again['cleaned_acronym']\n",
    "\n",
    "    # Tìm các khóa chỉ có trong df_evaluate\n",
    "    unique_keys = df_evaluate[~df_evaluate['key'].isin(df_crawl_again['key'])]\n",
    "\n",
    "    # Get the rows BEFORE dropping the 'key' column\n",
    "    result = df_evaluate[df_evaluate['key'].isin(unique_keys['key'])].copy()  # Important: Use .copy()!\n",
    "\n",
    "    # Loại bỏ các cột tạm thời\n",
    "    df_evaluate.drop(columns=['cleaned_title', 'cleaned_acronym', 'key'], inplace=True)\n",
    "    df_crawl_again.drop(columns=['cleaned_title', 'cleaned_acronym', 'key'], inplace=True)\n",
    "    result.drop(columns=['cleaned_title', 'cleaned_acronym', 'key'], inplace=True) # Drop from result as well\n",
    "\n",
    "\n",
    "    return result # Chỉ trả về các cột gốc\n",
    "\n",
    "# --- Main ---\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_file = './CORE_PORTAL_2023.csv'\n",
    "    crawl_again_file = './standard_evaluate.csv'\n",
    "    unique_rows = find_unique_rows(evaluate_file, crawl_again_file)\n",
    "\n",
    "    if unique_rows is not None:\n",
    "        if not unique_rows.empty:\n",
    "            print(\"Các dòng chỉ có trong evaluate.csv (dựa trên title/acronym đã làm sạch):\")\n",
    "            print(unique_rows)  # In ra toàn bộ DataFrame\n",
    "            # Hoặc, in từng dòng một nếu DataFrame quá lớn\n",
    "            # for index, row in unique_rows.iterrows():\n",
    "            #    print(f\"Dòng {index}: {row.to_dict()}\")\n",
    "        else:\n",
    "            print(\"Không tìm thấy dòng nào chỉ có trong evaluate.csv.\")\n",
    "    else:\n",
    "        print(\"Có lỗi xảy ra trong quá trình xử lý.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
