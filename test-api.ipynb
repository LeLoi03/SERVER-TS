{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"AIzaSyDK5eYs2PBVYP3l9UH0YMIFshxBMq6EsE8\"  # API Key từ Google Cloud\n",
    "CX = \"45ff1f0418c594bde\"  # Search Engine ID từ Google Custom Search Engine\n",
    "query = \"ACS/IEEE International Conference on Computer Systems and Applications (AICCSA) conference 2025 OR 2026 OR 2024\"\n",
    "# or_terms = \"2024 OR 2025\"\n",
    "\n",
    "url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={API_KEY}&cx={CX}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "# print(data)\n",
    "\n",
    "# In kết quả tìm kiếm\n",
    "for item in data.get(\"items\", []):\n",
    "    print(f\"Title: {item['title']}\")\n",
    "    print(f\"Link: {item['link']}\")\n",
    "    print(f\"Snippet: {item['snippet']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_KEY = \"AIzaSyDK5eYs2PBVYP3l9UH0YMIFshxBMq6EsE8\"  # Thay thế bằng API Key của bạn từ Google Cloud\n",
    "CX = \"45ff1f0418c594bde\"  # Thay thế bằng Search Engine ID của bạn từ Google Custom Search Engine\n",
    "ISSN = 'Journal of Finance ISSN \"1540-6261\"'\n",
    "# query = f'\"{ISSN}\"'  # Tìm kiếm chính xác ISSN.  Dấu ngoặc kép để tìm chính xác.\n",
    "query = ISSN # Tìm chính xác ISSN\n",
    "\n",
    "# Thêm các tham số cho tìm kiếm hình ảnh\n",
    "url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={API_KEY}&cx={CX}&searchType=image\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raises HTTPError for bad requests (4xx or 5xx)\n",
    "    data = response.json()\n",
    "\n",
    "    # In kết quả tìm kiếm hình ảnh\n",
    "    if \"items\" in data:\n",
    "        for item in data[\"items\"]:\n",
    "            print(f\"Title: {item.get('title', 'No title')}\")  # Xử lý trường hợp không có tiêu đề\n",
    "            print(f\"Image Link: {item.get('link', 'No link')}\") # Lấy link ảnh\n",
    "            print(f\"Context Link: {item.get('image', {}).get('contextLink', 'No context link')}\") # link trang chứa ảnh (nếu cần)\n",
    "            print(f\"Snippet: {item.get('snippet', 'No snippet')}\\n\")\n",
    "    else:\n",
    "        print(\"No image results found.\")\n",
    "\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    if response:\n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        try:\n",
    "            print(f\"Response content: {response.json()}\")  # In nội dung JSON nếu có\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Response content (not JSON): {response.text}\")\n",
    "except Exception as e:\n",
    "      print(f\"An unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os # Thêm thư viện os để kiểm tra file tồn tại\n",
    "\n",
    "# --- Cấu hình ---\n",
    "evaluate_file = './evaluate_all.csv'\n",
    "crawl_again_file = './crawl_again.csv'\n",
    "output_file = './evaluate_updated.csv' # Lưu vào file mới để tránh ghi đè file gốc khi chưa chắc chắn\n",
    "# Nếu muốn ghi đè trực tiếp file evaluate.csv, đổi output_file = evaluate_file\n",
    "\n",
    "# --- Kiểm tra sự tồn tại của file ---\n",
    "if not os.path.exists(evaluate_file):\n",
    "    print(f\"Lỗi: Không tìm thấy file '{evaluate_file}'\")\n",
    "    exit()\n",
    "if not os.path.exists(crawl_again_file):\n",
    "    print(f\"Lỗi: Không tìm thấy file '{crawl_again_file}'\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Đang đọc file '{evaluate_file}'...\")\n",
    "try:\n",
    "    df_evaluate = pd.read_csv(evaluate_file)\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi đọc file '{evaluate_file}': {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Đang đọc file '{crawl_again_file}'...\")\n",
    "try:\n",
    "    df_crawl_again = pd.read_csv(crawl_again_file)\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi đọc file '{crawl_again_file}': {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Kiểm tra các cột cần thiết ---\n",
    "required_columns = ['title', 'acronym']\n",
    "if not all(col in df_evaluate.columns for col in required_columns):\n",
    "    print(f\"Cảnh báo: File '{evaluate_file}' có thể thiếu cột 'title' hoặc 'acronym'.\")\n",
    "    # Không exit() để cố gắng xử lý nếu cột vẫn tồn tại trong file kia\n",
    "if not all(col in df_crawl_again.columns for col in required_columns):\n",
    "    print(f\"Lỗi: File '{crawl_again_file}' thiếu cột 'title' hoặc 'acronym'. Không thể tiến hành khớp.\")\n",
    "    exit()\n",
    "\n",
    "# --- Chuẩn hóa cột 'title' và 'acronym' trong cả hai DataFrame ---\n",
    "print(\"Chuẩn hóa cột 'title' và 'acronym' (loại bỏ '(...)' và khoảng trắng thừa)...\")\n",
    "\n",
    "regex_pattern = r'\\s*\\(.*\\)\\s*' # Regex để xóa (...) và khoảng trắng xung quanh\n",
    "columns_to_clean = ['title', 'acronym']\n",
    "dfs_to_clean = {'evaluate': df_evaluate, 'crawl_again': df_crawl_again}\n",
    "\n",
    "for df_name, df in dfs_to_clean.items():\n",
    "    print(f\"  - Đang xử lý file '{df_name}.csv'\")\n",
    "    for col in columns_to_clean:\n",
    "        if col in df.columns:\n",
    "            # 1. Xử lý NaN và chuyển thành string\n",
    "            df[col] = df[col].fillna('').astype(str)\n",
    "            # 2. Loại bỏ (...)\n",
    "            df[col] = df[col].str.replace(regex_pattern, '', regex=True)\n",
    "            # 3. Dọn dẹp khoảng trắng\n",
    "            df[col] = df[col].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "            print(f\"    - Đã làm sạch cột '{col}'\")\n",
    "        elif df_name == 'crawl_again': # Chỉ báo lỗi nếu cột thiếu trong crawl_again\n",
    "             print(f\"    - Lỗi: Cột '{col}' không tìm thấy trong {df_name}.csv.\")\n",
    "             exit()\n",
    "        else: # Chỉ cảnh báo nếu cột thiếu trong evaluate\n",
    "             print(f\"    - Cảnh báo: Cột '{col}' không tìm thấy trong {df_name}.csv.\")\n",
    "\n",
    "\n",
    "# --- Đảm bảo các cột dùng để join là kiểu string sau khi làm sạch ---\n",
    "# (Quan trọng để tránh lỗi khớp loại dữ liệu)\n",
    "try:\n",
    "    df_evaluate['title'] = df_evaluate['title'].astype(str)\n",
    "    df_evaluate['acronym'] = df_evaluate['acronym'].astype(str)\n",
    "    df_crawl_again['title'] = df_crawl_again['title'].astype(str)\n",
    "    df_crawl_again['acronym'] = df_crawl_again['acronym'].astype(str)\n",
    "except KeyError as e:\n",
    "    print(f\"Lỗi: Không tìm thấy cột '{e}' sau khi cố gắng chuẩn hóa. Kiểm tra lại tên cột.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Đang tiến hành cập nhật dữ liệu...\")\n",
    "\n",
    "# Tạo bản sao của df_evaluate để cập nhật, hoặc cập nhật trực tiếp\n",
    "# Cập nhật trực tiếp df_evaluate trong trường hợp này là ổn\n",
    "# df_updated = df_evaluate.copy() # Không cần thiết nếu cập nhật inplace\n",
    "\n",
    "# Đặt 'title' và 'acronym' (đã chuẩn hóa) làm index để khớp hiệu quả\n",
    "# drop=False giữ lại các cột title/acronym trong DataFrame sau khi set index\n",
    "# verify_integrity=False cho phép index không duy nhất\n",
    "try:\n",
    "    # Quan trọng: Đặt index cho df_evaluate *trước* khi cập nhật\n",
    "    df_evaluate.set_index(['title', 'acronym'], inplace=True, drop=False, verify_integrity=False)\n",
    "    df_crawl_again.set_index(['title', 'acronym'], inplace=True, drop=False, verify_integrity=False)\n",
    "except KeyError as e:\n",
    "     print(f\"Lỗi: Không tìm thấy cột để đặt làm index: {e}\")\n",
    "     # Reset index nếu bước trước đó gây lỗi, tránh trạng thái không nhất quán\n",
    "     df_evaluate.reset_index(drop=True, inplace=True, errors='ignore')\n",
    "     df_crawl_again.reset_index(drop=True, inplace=True, errors='ignore')\n",
    "     exit()\n",
    "except Exception as e:\n",
    "     print(f\"Lỗi không xác định khi đặt index: {e}\")\n",
    "     df_evaluate.reset_index(drop=True, inplace=True, errors='ignore')\n",
    "     df_crawl_again.reset_index(drop=True, inplace=True, errors='ignore')\n",
    "     exit()\n",
    "\n",
    "# Lấy danh sách các cột từ df_crawl_again để đảm bảo cập nhật đúng các cột\n",
    "update_columns = df_crawl_again.columns\n",
    "\n",
    "# Biến đếm số dòng đã cập nhật\n",
    "updated_rows_count = 0\n",
    "processed_crawl_rows = 0\n",
    "\n",
    "# Duyệt qua từng dòng trong df_crawl_again (đã chuẩn hóa và có index)\n",
    "for index, row_crawl in df_crawl_again.iterrows():\n",
    "    processed_crawl_rows += 1\n",
    "    # Index bây giờ là tuple (cleaned_title, cleaned_acronym)\n",
    "    if index in df_evaluate.index:\n",
    "        # Nếu tìm thấy index trong df_evaluate\n",
    "        # Cập nhật toàn bộ các cột có trong df_crawl_again cho dòng tương ứng trong df_evaluate\n",
    "        # Sử dụng .loc để truy cập và gán giá trị theo index\n",
    "        # row_crawl đã chứa dữ liệu đã chuẩn hóa title/acronym\n",
    "        try:\n",
    "            # Gán trực tiếp vào df_evaluate vì nó đã được set_index\n",
    "            df_evaluate.loc[index, update_columns] = row_crawl[update_columns].values\n",
    "            # Nếu có nhiều dòng trùng index trong df_evaluate, tất cả sẽ được cập nhật\n",
    "            # Đếm số lượng dòng thực sự bị ảnh hưởng trong df_evaluate\n",
    "            # Lưu ý: .loc[index] có thể trả về Series hoặc DataFrame nếu index trùng lặp\n",
    "            if isinstance(df_evaluate.loc[index], pd.DataFrame):\n",
    "                updated_rows_count += len(df_evaluate.loc[index])\n",
    "            else:\n",
    "                 updated_rows_count += 1 # Chỉ có 1 dòng được cập nhật cho index này\n",
    "            # print(f\"Đã cập nhật: Title='{index[0]}', Acronym='{index[1]}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi cập nhật index {index}: {e}\")\n",
    "            # Có thể thêm xử lý lỗi cụ thể hơn ở đây nếu cần\n",
    "    # else:\n",
    "        # print(f\"Không tìm thấy để cập nhật: Title='{index[0]}', Acronym='{index[1]}'\")\n",
    "\n",
    "# Cần điều chỉnh lại cách đếm vì cách trên có thể đếm lặp nếu crawl_again có trùng index\n",
    "# Đếm lại sau khi cập nhật xong thì chính xác hơn, nhưng cách trên cho ước lượng\n",
    "print(f\"Đã xử lý {processed_crawl_rows} dòng từ '{crawl_again_file}'.\")\n",
    "# Cách đếm chính xác hơn: so sánh df gốc và df sau cập nhật, nhưng phức tạp.\n",
    "# Thông báo dựa trên số lần gọi gán giá trị có thể chấp nhận được.\n",
    "print(f\"Đã thực hiện {updated_rows_count} lượt cập nhật dòng (có thể bao gồm các dòng trùng lặp trong evaluate.csv).\")\n",
    "\n",
    "\n",
    "# Đặt lại index về dạng số thứ tự mặc định cho df_evaluate\n",
    "df_evaluate.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# --- Lưu kết quả ---\n",
    "print(f\"Đang lưu kết quả vào file '{output_file}'...\")\n",
    "try:\n",
    "    # index=False để không ghi cột index của pandas vào file CSV\n",
    "    df_evaluate.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(\"Hoàn thành!\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi lưu file '{output_file}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # Cần thiết để xử lý giá trị NaN\n",
    "\n",
    "# --- Cấu hình ---\n",
    "# ----- THAY ĐỔI Ở ĐÂY -----\n",
    "csv_file1_path = './1111.csv'  # File CSV chứa dữ liệu \"cũ\" (thay cho Excel)\n",
    "csv_file2_path = './evaluate_updated.csv'    # File CSV chứa dữ liệu \"mới\"\n",
    "# Đổi tên file output nếu muốn\n",
    "output_csv_path = './ket_qua_so_sanh_link_full_v5_csv_only.csv'\n",
    "\n",
    "# Các cột cơ bản cần thiết trong mỗi file CSV\n",
    "cols_file1 = ['title', 'acronym', 'link'] # Tên cột mong đợi trong file CSV 1\n",
    "cols_file2 = ['title', 'acronym', 'link'] # Tên cột mong đợi trong file CSV 2\n",
    "\n",
    "# --- Đọc dữ liệu ---\n",
    "try:\n",
    "    # ----- THAY ĐỔI Ở ĐÂY -----\n",
    "    # Đọc file CSV 1 (dữ liệu cũ)\n",
    "    df_csv1 = pd.read_csv(\n",
    "        csv_file1_path,\n",
    "        usecols=cols_file1 # Chỉ đọc các cột cần thiết\n",
    "    )\n",
    "    # Đổi tên cột để phân biệt\n",
    "    df_csv1.columns = ['title_file1', 'acronym_file1', 'link_file1_raw']\n",
    "    print(f\"Đọc file CSV 1 ({csv_file1_path}) thành công.\")\n",
    "\n",
    "    # Đọc file CSV 2 (dữ liệu mới) - Đọc TẤT CẢ các cột\n",
    "    df_csv2 = pd.read_csv(csv_file2_path)\n",
    "    # Lưu lại danh sách các cột gốc từ CSV 2\n",
    "    original_csv2_columns = list(df_csv2.columns)\n",
    "    # Xác định các cột cơ bản và cột bổ sung trong CSV 2\n",
    "    base_columns_to_handle_csv2 = ['title', 'acronym', 'link']\n",
    "    extra_csv_columns = [col for col in original_csv2_columns if col not in base_columns_to_handle_csv2]\n",
    "\n",
    "    # Đổi tên các cột cơ bản trong CSV 2 để phân biệt và chuẩn bị merge\n",
    "    df_csv2.rename(columns={'title': 'title_file2',\n",
    "                            'acronym': 'acronym_file2',\n",
    "                            'link': 'link_file2'}, inplace=True)\n",
    "\n",
    "    print(f\"Đọc file CSV 2 ({csv_file2_path}) thành công.\")\n",
    "    print(f\"Các cột bổ sung từ CSV 2 sẽ được giữ lại: {extra_csv_columns}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Lỗi: Không tìm thấy một trong các file CSV:\\n- File 1: {csv_file1_path}\\n- File 2: {csv_file2_path}\")\n",
    "    print(f\"Chi tiết: {e}\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"Lỗi: Có vẻ cột trong file CSV không đúng như mong đợi. Chi tiết: {e}\")\n",
    "    print(f\"Hãy đảm bảo file '{csv_file1_path}' có các cột: {cols_file1}\")\n",
    "    print(f\"Hãy đảm bảo file '{csv_file2_path}' có các cột cơ bản: {cols_file2}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi không xác định khi đọc file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Chuẩn bị dữ liệu để khớp ---\n",
    "# Làm sạch cột tên hội nghị\n",
    "df_csv1['title_file1'] = df_csv1['title_file1'].astype(str).str.strip()\n",
    "df_csv2['title_file2'] = df_csv2['title_file2'].astype(str).str.strip()\n",
    "\n",
    "# ----- THAY ĐỔI TÊN HÀM CHO RÕ NGHĨA HƠN -----\n",
    "# --- Hàm xử lý và làm sạch danh sách link từ CSV 1 (có thể chứa nhiều link) ---\n",
    "def clean_multi_links(link_string):\n",
    "    \"\"\"Làm sạch chuỗi link có thể chứa nhiều link cách nhau bằng dấu phẩy.\"\"\"\n",
    "    if pd.isna(link_string):\n",
    "        return []\n",
    "    # Xử lý cả trường hợp có khoảng trắng thừa quanh dấu phẩy\n",
    "    links = [link.strip() for link in str(link_string).split(',') if link.strip()]\n",
    "    return links\n",
    "\n",
    "# --- Hàm làm sạch link đơn từ CSV 2 ---\n",
    "def clean_single_link(link):\n",
    "    \"\"\"Làm sạch một link đơn.\"\"\"\n",
    "    if pd.isna(link):\n",
    "        return None\n",
    "    cleaned = str(link).strip()\n",
    "    return cleaned if cleaned else None # Trả về None nếu link sau khi strip là rỗng\n",
    "\n",
    "# Áp dụng hàm làm sạch link cho CSV 2\n",
    "df_csv2['link_file2_cleaned'] = df_csv2['link_file2'].apply(clean_single_link)\n",
    "\n",
    "print(\"Làm sạch dữ liệu tên và link.\")\n",
    "\n",
    "# --- Gộp (Merge) hai DataFrame ---\n",
    "# Merge dựa trên tên hội nghị đã làm sạch từ hai file\n",
    "# Outer join để giữ tất cả các dòng ban đầu\n",
    "merged_df = pd.merge(\n",
    "    df_csv2, # File CSV 2 (mới) là bên trái\n",
    "    df_csv1, # File CSV 1 (cũ) là bên phải\n",
    "    left_on='title_file2',\n",
    "    right_on='title_file1',\n",
    "    how='outer',\n",
    "    indicator=True # Thêm cột _merge để biết dòng đến từ đâu\n",
    ")\n",
    "\n",
    "print(\"Gộp dữ liệu từ hai file CSV.\")\n",
    "\n",
    "# --- Xử lý kết quả sau khi gộp ---\n",
    "results = []\n",
    "processed_count = 0\n",
    "skipped_file1_only = 0\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    # Bỏ qua các dòng chỉ có trong file CSV 1 (dữ liệu cũ không có trong file mới)\n",
    "    # Giống logic ban đầu khi bỏ qua 'right_only' (chỉ có trong Excel)\n",
    "    if row['_merge'] == 'right_only':\n",
    "        skipped_file1_only += 1\n",
    "        continue # Bỏ qua\n",
    "\n",
    "    processed_count += 1\n",
    "\n",
    "    # Lấy link mới (file 2) đã được làm sạch\n",
    "    link_moi_sach = row['link_file2_cleaned'] # Sử dụng cột đã làm sạch\n",
    "\n",
    "    # Lấy và xử lý danh sách link cũ (file 1)\n",
    "    link_cu_raw_value = row['link_file1_raw'] # Giá trị gốc từ CSV 1\n",
    "    # Sử dụng hàm clean_multi_links vì file CSV cũ có thể có nhiều link\n",
    "    link_cu_list_sach = clean_multi_links(link_cu_raw_value)\n",
    "\n",
    "    # Xác định trạng thái kiểm tra link\n",
    "    check_status = ''\n",
    "    found_match = False\n",
    "\n",
    "    # Trường hợp có ở cả 2 file\n",
    "    if row['_merge'] == 'both':\n",
    "        if link_moi_sach is not None:\n",
    "            # Kiểm tra xem link mới có trong danh sách link cũ không\n",
    "            if link_moi_sach in link_cu_list_sach:\n",
    "                found_match = True\n",
    "                check_status = 'Giống'\n",
    "            elif not link_cu_list_sach: # Link mới có, link cũ không có (rỗng/thiếu)\n",
    "                 check_status = 'Khác (link cũ thiếu/trống)'\n",
    "            else: # Link mới có, link cũ có nhưng không khớp\n",
    "                check_status = 'Khác'\n",
    "        elif not link_cu_list_sach: # Cả link mới và cũ đều thiếu/rỗng\n",
    "             check_status = 'Thiếu cả 2 link'\n",
    "        else: # Link mới thiếu/rỗng, link cũ có\n",
    "             check_status = 'Khác (link mới thiếu)'\n",
    "\n",
    "    # Trường hợp chỉ có ở file CSV 2 (dữ liệu mới)\n",
    "    elif row['_merge'] == 'left_only':\n",
    "        check_status = 'Mới (chỉ có trong file 2)'\n",
    "        link_cu_raw_value = None # Đảm bảo link cũ là rỗng cho dòng mới\n",
    "\n",
    "    # Tạo dictionary chứa kết quả cho dòng hiện tại\n",
    "    # Ưu tiên lấy tên và viết tắt từ file 2 nếu có, nếu không lấy từ file 1\n",
    "    result_dict = {\n",
    "        'title': row['title_file2'] if pd.notna(row['title_file2']) else row['title_file1'],\n",
    "        'acronym': row['acronym_file2'] if pd.notna(row['acronym_file2']) else row['acronym_file1'],\n",
    "        'link_cu': str(link_cu_raw_value) if pd.notna(link_cu_raw_value) else '', # Hiển thị link cũ gốc\n",
    "        'link_moi': str(row['link_file2']) if pd.notna(row['link_file2']) else '', # Hiển thị link mới gốc\n",
    "        'check': check_status\n",
    "    }\n",
    "\n",
    "    # Thêm tất cả các cột bổ sung từ file CSV 2 gốc vào dictionary\n",
    "    # Giá trị được lấy trực tiếp từ `row` của merged_df\n",
    "    for col_title in extra_csv_columns:\n",
    "        # Cần kiểm tra xem cột đó có tồn tại trong dòng hiện tại không\n",
    "        # (quan trọng nếu merge tạo ra NaN cho các cột này ở dòng 'right_only', mặc dù ta đã skip)\n",
    "        result_dict[col_title] = row[col_title] if col_title in row and pd.notna(row[col_title]) else ''\n",
    "\n",
    "\n",
    "    results.append(result_dict)\n",
    "\n",
    "print(f\"Đã xử lý {processed_count} dòng (bao gồm cả dòng mới từ file 2).\")\n",
    "if skipped_file1_only > 0:\n",
    "    print(f\"Đã bỏ qua {skipped_file1_only} dòng chỉ tồn tại trong file CSV 1.\")\n",
    "\n",
    "# --- Tạo DataFrame kết quả cuối cùng ---\n",
    "# Xác định thứ tự cột mong muốn cho file output\n",
    "# Bắt đầu bằng các cột đã xử lý, tiếp theo là các cột bổ sung từ CSV 2 gốc\n",
    "output_column_order = [\n",
    "    'title', 'acronym', 'link_cu', 'link_moi', 'check'\n",
    "] + extra_csv_columns\n",
    "\n",
    "# Tạo DataFrame từ danh sách các dictionary\n",
    "# Chỉ định `columns` để đảm bảo đúng thứ tự và xử lý trường hợp thiếu cột\n",
    "df_output = pd.DataFrame(results, columns=output_column_order)\n",
    "\n",
    "# --- Xử lý giá trị NaN/None trong DataFrame kết quả ---\n",
    "# Thay thế NaN bằng chuỗi rỗng '' để file CSV dễ đọc hơn\n",
    "df_output.fillna('', inplace=True)\n",
    "\n",
    "print(\"Tạo DataFrame kết quả với đầy đủ cột hoàn tất.\")\n",
    "\n",
    "# --- Ghi kết quả ra file CSV mới ---\n",
    "try:\n",
    "    df_output.to_csv(output_csv_path, index=False, encoding='utf-8-sig') # utf-8-sig để Excel đọc tiếng Việt tốt\n",
    "    print(f\"Hoàn thành! Kết quả đã được ghi vào file: {output_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi ghi file CSV đầu ra: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re  # Để sử dụng regular expression\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Làm sạch chuỗi văn bản bằng cách loại bỏ nội dung trong ngoặc đơn và khoảng trắng thừa.\"\"\"\n",
    "    if pd.isna(text): # Check NaN trước khi làm bất cứ điều gì\n",
    "        return ''\n",
    "    text = str(text)  # Chắc chắn rằng nó là một chuỗi\n",
    "    regex_pattern = r'\\s*\\(.*\\)\\s*'\n",
    "    text = re.sub(regex_pattern, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_unique_rows(evaluate_file, crawl_again_file):\n",
    "    \"\"\"\n",
    "    Tìm các dòng trong evaluate.csv mà không có bản sao tương ứng trong\n",
    "    crawl_again.csv, dựa trên 'title' và 'acronym' đã được làm sạch.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(evaluate_file):\n",
    "        print(f\"Lỗi: Không tìm thấy file '{evaluate_file}'\")\n",
    "        return None\n",
    "    if not os.path.exists(crawl_again_file):\n",
    "        print(f\"Lỗi: Không tìm thấy file '{crawl_again_file}'\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df_evaluate = pd.read_csv(evaluate_file)\n",
    "        df_crawl_again = pd.read_csv(crawl_again_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file: {e}\")\n",
    "        return None\n",
    "\n",
    "    required_columns = ['title', 'acronym']\n",
    "    for df_name, df in [('evaluate.csv', df_evaluate), ('crawl_again.csv', df_crawl_again)]:\n",
    "      if not all(col in df.columns for col in required_columns):\n",
    "        print(f\"Lỗi: File '{df_name}' thiếu cột 'title' hoặc 'acronym'.\")\n",
    "        return None\n",
    "\n",
    "    # Làm sạch các cột 'title' và 'acronym' trong cả hai DataFrame\n",
    "    df_evaluate['cleaned_title'] = df_evaluate['title'].apply(clean_text)\n",
    "    df_evaluate['cleaned_acronym'] = df_evaluate['acronym'].apply(clean_text)\n",
    "\n",
    "    df_crawl_again['cleaned_title'] = df_crawl_again['title'].apply(clean_text)\n",
    "    df_crawl_again['cleaned_acronym'] = df_crawl_again['acronym'].apply(clean_text)\n",
    "\n",
    "    # Tạo khóa duy nhất từ các cột đã làm sạch\n",
    "    df_evaluate['key'] = df_evaluate['cleaned_title'] + '||' + df_evaluate['cleaned_acronym']\n",
    "    df_crawl_again['key'] = df_crawl_again['cleaned_title'] + '||' + df_crawl_again['cleaned_acronym']\n",
    "\n",
    "    # Tìm các khóa chỉ có trong df_evaluate\n",
    "    unique_keys = df_evaluate[~df_evaluate['key'].isin(df_crawl_again['key'])]\n",
    "\n",
    "    # Get the rows BEFORE dropping the 'key' column\n",
    "    result = df_evaluate[df_evaluate['key'].isin(unique_keys['key'])].copy()  # Important: Use .copy()!\n",
    "\n",
    "    # Loại bỏ các cột tạm thời\n",
    "    df_evaluate.drop(columns=['cleaned_title', 'cleaned_acronym', 'key'], inplace=True)\n",
    "    df_crawl_again.drop(columns=['cleaned_title', 'cleaned_acronym', 'key'], inplace=True)\n",
    "    result.drop(columns=['cleaned_title', 'cleaned_acronym', 'key'], inplace=True) # Drop from result as well\n",
    "\n",
    "\n",
    "    return result # Chỉ trả về các cột gốc\n",
    "\n",
    "# --- Main ---\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_file = './CORE_PORTAL_2023.csv'\n",
    "    crawl_again_file = './standard_evaluate.csv'\n",
    "    unique_rows = find_unique_rows(evaluate_file, crawl_again_file)\n",
    "\n",
    "    if unique_rows is not None:\n",
    "        if not unique_rows.empty:\n",
    "            print(\"Các dòng chỉ có trong evaluate.csv (dựa trên title/acronym đã làm sạch):\")\n",
    "            print(unique_rows)  # In ra toàn bộ DataFrame\n",
    "            # Hoặc, in từng dòng một nếu DataFrame quá lớn\n",
    "            # for index, row in unique_rows.iterrows():\n",
    "            #    print(f\"Dòng {index}: {row.to_dict()}\")\n",
    "        else:\n",
    "            print(\"Không tìm thấy dòng nào chỉ có trong evaluate.csv.\")\n",
    "    else:\n",
    "        print(\"Có lỗi xảy ra trong quá trình xử lý.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
