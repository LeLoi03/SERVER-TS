{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " BƯỚC 0: TÍNH TOÁN TỔNG SỐ CONFERENCE DUY NHẤT TRÊN TẤT CẢ CÁC FILE\n",
      "================================================================================\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_2_3_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_8_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_12_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_13_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_16_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_19_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_51_100.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_101_150.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_151_159.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_51_100_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_101_139_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_lan_3.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50_lan_4.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50_lan_4.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_not_crawl_1_50.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_not_crawl_51_end.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_4_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_5_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_9_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_14_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_15_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_18_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_20_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_1_50.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_51_100.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_101_150.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_1_32_lan_2.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_lan_3.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_lan_4.csv\n",
      "Lỗi đọc file evaluate_recrawl_all_tri_1_50_lan_5.csv trong quá trình đếm: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv'\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tri_51_100_lan_5.csv\n",
      "Lỗi đọc file evaluate_recrawl_all_tri_101_120_lan_5.csv trong quá trình đếm: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv'\n",
      "Đã xử lý file để đếm: batch2.csv\n",
      "Đã xử lý file để đếm: batch3.csv\n",
      "Đã xử lý file để đếm: batch8.csv\n",
      "Đã xử lý file để đếm: batch12.csv\n",
      "Đã xử lý file để đếm: batch13.csv\n",
      "Đã xử lý file để đếm: batch16.csv\n",
      "Đã xử lý file để đếm: batch19.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_2_3_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_8_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_12_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_13_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_16_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_19_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_51_100.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_101_150.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_151_159.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_51_100_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_101_139_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_lan_3.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50_lan_4.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_not_crawl_1_50.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_not_crawl_51_end.csv\n",
      "Đã xử lý file để đếm: batch4.csv\n",
      "Đã xử lý file để đếm: batch5.csv\n",
      "Đã xử lý file để đếm: batch9.csv\n",
      "Đã xử lý file để đếm: batch14.csv\n",
      "Đã xử lý file để đếm: batch15.csv\n",
      "Đã xử lý file để đếm: batch18.csv\n",
      "Đã xử lý file để đếm: batch20.csv\n",
      "Đã xử lý file để đếm: recrawl_all_thang_1_50_lan_1_check.csv\n",
      "Đã xử lý file để đếm: recrawl_all_thang_51_58_lan_1_check.csv\n",
      "Lỗi đọc file evaluate_recrawl_all_tri_1_50_lan_5.csv trong quá trình đếm: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv'\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tri_51_100_lan_5.csv\n",
      "Lỗi đọc file evaluate_recrawl_all_tri_101_120_lan_5.csv trong quá trình đếm: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv'\n",
      "Đã xử lý file để đếm: evaluate_crawl_con_lai_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_con_lai.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_con_lai_lan_3.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_missing_info_in_full.csv\n",
      "\n",
      "----------------------------------------\n",
      ">>> TỔNG SỐ CONFERENCE DUY NHẤT (dựa trên title và acronym đã chuẩn hóa) là: 702\n",
      "----------------------------------------\n",
      "\n",
      "================================================================================\n",
      " BẮT ĐẦU KỊCH BẢN: ƯU TIÊN BẤT KỲ PHIÊN BẢN ĐÚNG NÀO\n",
      "================================================================================\n",
      "\n",
      "--- BƯỚC 1: Đọc và gộp tất cả các file ---\n",
      "Lỗi xử lý file ./src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv'\n",
      "Lỗi xử lý file ./src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv'\n",
      "Lỗi xử lý file ./src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv'\n",
      "Lỗi xử lý file ./src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv'\n",
      "Đã gộp thành công. Tổng số dòng để xử lý: 2233\n",
      "\n",
      "--- BƯỚC 2: Xác định danh sách các conference có ít nhất một phiên bản đúng ---\n",
      "Tìm thấy 682 conference có ít nhất một phiên bản đúng.\n",
      "\n",
      "--- BƯỚC 3: Tạo file correct cuối cùng ---\n",
      "\n",
      "--- BƯỚC 4: Tạo file recrawl cuối cùng ---\n",
      "\n",
      "--- BƯỚC 5: Hoàn tất và lưu file tổng hợp cuối cùng ---\n",
      "Đã lưu tổng cộng 682 dòng vào file correct './src/conference/evaluate/tri_tung_all_correct_final.csv'.\n",
      "Đã lưu tổng cộng 19 dòng vào file recrawl './src/conference/evaluate/tri_tung_all_recrawl_final.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ==============================================================================\n",
    "# HÀM CHUẨN HÓA VÀ HÀM TRỢ GIÚP (Giữ nguyên)\n",
    "# ==============================================================================\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    cleaned_text = re.sub(r'\\s*\\(.*?\\)\\s*', ' ', str(text))\n",
    "    final_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "    return final_text\n",
    "\n",
    "def add_normalized_keys(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "    df_copy['key_title'] = df_copy['title'].apply(normalize_text) if 'title' in df_copy.columns else ''\n",
    "    df_copy['key_acronym'] = df_copy['acronym'].apply(normalize_text) if 'acronym' in df_copy.columns else ''\n",
    "    return df_copy\n",
    "\n",
    "# ==============================================================================\n",
    "# HÀM ĐẾM SỐ LƯỢNG DUY NHẤT (Giữ nguyên)\n",
    "# ==============================================================================\n",
    "\n",
    "def calculate_and_print_total_unique_count(all_files: list):\n",
    "    total_unique_conferences = set()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" BƯỚC 0: TÍNH TOÁN TỔNG SỐ CONFERENCE DUY NHẤT TRÊN TẤT CẢ CÁC FILE\")\n",
    "    print(\"=\"*80)\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig', na_values=['', ' '], dtype=str, low_memory=False)\n",
    "            df_normalized = add_normalized_keys(df)\n",
    "            if not df_normalized.empty:\n",
    "                unique_keys_in_file = set(zip(df_normalized['key_title'], df_normalized['key_acronym']))\n",
    "                total_unique_conferences.update(unique_keys_in_file)\n",
    "                print(f\"Đã xử lý file để đếm: {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi đọc file {os.path.basename(file_path)} trong quá trình đếm: {e}\")\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(f\">>> TỔNG SỐ CONFERENCE DUY NHẤT (dựa trên title và acronym đã chuẩn hóa) là: {len(total_unique_conferences)}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "# ==============================================================================\n",
    "# HÀM XỬ LÝ CHÍNH THEO LOGIC MỚI\n",
    "# ==============================================================================\n",
    "def process_by_any_correct_version(\n",
    "    input_files: list,\n",
    "    aggregated_correct_path: str,\n",
    "    aggregated_recrawl_path: str,\n",
    "    recrawled_files: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Phân loại conference: nếu có bất kỳ phiên bản nào không có 'note',\n",
    "    nó sẽ được coi là 'correct' và không nằm trong danh sách 'recrawl'.\n",
    "    Luôn ưu tiên lấy phiên bản mới nhất.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" BẮT ĐẦU KỊCH BẢN: ƯU TIÊN BẤT KỲ PHIÊN BẢN ĐÚNG NÀO\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # --- BƯỚC 1: ĐỌC VÀ GỘP TẤT CẢ CÁC FILE (ƯU TIÊN FILE ĐÃ RECRAWL) ---\n",
    "    print(\"--- BƯỚC 1: Đọc và gộp tất cả các file ---\")\n",
    "    all_dfs = []\n",
    "    all_files_in_order = (recrawled_files or []) + (input_files or [])\n",
    "    \n",
    "    for file_path in all_files_in_order:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig', na_values=[''], dtype=str).fillna('')\n",
    "            df = add_normalized_keys(df)\n",
    "            df = df[df['key_title'] != ''].copy()\n",
    "            if not df.empty:\n",
    "                all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi xử lý file {file_path}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    if not all_dfs:\n",
    "        print(\"Không có dữ liệu để xử lý. Dừng chương trình.\")\n",
    "        return\n",
    "\n",
    "    master_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"Đã gộp thành công. Tổng số dòng để xử lý: {len(master_df)}\")\n",
    "\n",
    "    # --- BƯỚC 2: XÁC ĐỊNH DANH SÁCH CÁC CONFERENCE \"ĐÚNG\" ---\n",
    "    print(\"\\n--- BƯỚC 2: Xác định danh sách các conference có ít nhất một phiên bản đúng ---\")\n",
    "    key_cols = ['key_title', 'key_acronym']\n",
    "    \n",
    "    # Điều kiện một dòng được coi là \"đúng\"\n",
    "    is_correct_condition = ~master_df['note'].notna() | (master_df['note'].astype(str).str.strip() == '')\n",
    "    \n",
    "    # Lấy tất cả các dòng đúng\n",
    "    all_correct_versions = master_df[is_correct_condition]\n",
    "    \n",
    "    # Lấy danh sách các khóa duy nhất của các conference đúng\n",
    "    correct_keys_df = all_correct_versions.drop_duplicates(subset=key_cols, keep='first')\n",
    "    correct_keys_set = set(zip(correct_keys_df['key_title'], correct_keys_df['key_acronym']))\n",
    "    print(f\"Tìm thấy {len(correct_keys_set)} conference có ít nhất một phiên bản đúng.\")\n",
    "\n",
    "    # --- BƯỚC 3: TẠO FILE CORRECT CUỐI CÙNG ---\n",
    "    print(\"\\n--- BƯỚC 3: Tạo file correct cuối cùng ---\")\n",
    "    # Từ tất cả các phiên bản đúng, giữ lại phiên bản mới nhất (xuất hiện đầu tiên) cho mỗi conference\n",
    "    final_correct_df = all_correct_versions.drop_duplicates(subset=key_cols, keep='first')\n",
    "    \n",
    "    # --- BƯỚC 4: TẠO FILE RECRAWL CUỐI CÙNG ---\n",
    "    print(\"\\n--- BƯỚC 4: Tạo file recrawl cuối cùng ---\")\n",
    "    # Lấy tất cả các dòng có note\n",
    "    all_recrawl_versions = master_df[~is_correct_condition].copy()\n",
    "    \n",
    "    if not all_recrawl_versions.empty:\n",
    "        # Loại bỏ những conference đã có trong danh sách đúng\n",
    "        mask_keep = ~all_recrawl_versions.apply(\n",
    "            lambda row: (row['key_title'], row['key_acronym']) in correct_keys_set,\n",
    "            axis=1\n",
    "        )\n",
    "        recrawl_to_keep = all_recrawl_versions[mask_keep]\n",
    "        \n",
    "        # Từ những dòng còn lại, giữ lại phiên bản mới nhất\n",
    "        final_recrawl_df = recrawl_to_keep.drop_duplicates(subset=key_cols, keep='first')\n",
    "    else:\n",
    "        final_recrawl_df = pd.DataFrame(columns=master_df.columns)\n",
    "\n",
    "    # --- BƯỚC 5: LƯU FILE ---\n",
    "    print(\"\\n--- BƯỚC 5: Hoàn tất và lưu file tổng hợp cuối cùng ---\")\n",
    "    \n",
    "    # Dọn dẹp cột khóa trước khi lưu\n",
    "    final_correct_df = final_correct_df.drop(columns=key_cols, errors='ignore')\n",
    "    final_recrawl_df = final_recrawl_df.drop(columns=key_cols, errors='ignore')\n",
    "\n",
    "    # Lưu file CORRECT\n",
    "    final_correct_df.to_csv(aggregated_correct_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Đã lưu tổng cộng {len(final_correct_df)} dòng vào file correct '{aggregated_correct_path}'.\")\n",
    "\n",
    "    # Lưu file RECRAWL\n",
    "    final_recrawl_df.to_csv(aggregated_recrawl_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Đã lưu tổng cộng {len(final_recrawl_df)} dòng vào file recrawl '{aggregated_recrawl_path}'.\")\n",
    "\n",
    "\n",
    "# --- Cách sử dụng ---\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_files_for_note_check = [\n",
    "\n",
    "\n",
    "        './src/conference/evaluate/batch2.csv',\n",
    "        './src/conference/evaluate/batch3.csv',\n",
    "        './src/conference/evaluate/batch8.csv',\n",
    "        './src/conference/evaluate/batch12.csv',\n",
    "        './src/conference/evaluate/batch13.csv',\n",
    "        './src/conference/evaluate/batch16.csv',\n",
    "        './src/conference/evaluate/batch19.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_2_3_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_8_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_12_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_13_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_16_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_19_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_51_100.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_101_150.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_151_159.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_51_100_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_101_139_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_lan_3.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50_lan_4.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_not_crawl_1_50.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_not_crawl_51_end.csv',\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        './src/conference/evaluate/batch4.csv',\n",
    "        './src/conference/evaluate/batch5.csv',\n",
    "        './src/conference/evaluate/batch9.csv',\n",
    "        './src/conference/evaluate/batch14.csv',\n",
    "        './src/conference/evaluate/batch15.csv',\n",
    "        './src/conference/evaluate/batch18.csv',\n",
    "        './src/conference/evaluate/batch20.csv',\n",
    "        './src/conference/evaluate/recrawl_all_thang_1_50_lan_1_check.csv',\n",
    "        './src/conference/evaluate/recrawl_all_thang_51_58_lan_1_check.csv',\n",
    "        \n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_51_100_lan_5.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv',\n",
    "        \n",
    "        './src/conference/evaluate/evaluate_crawl_con_lai_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_con_lai.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_con_lai_lan_3.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_missing_info_in_full.csv',\n",
    "\n",
    "\n",
    "    ]\n",
    "    recrawled_results_files = [\n",
    "\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_2_3_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_8_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_12_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_13_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_16_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_19_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_51_100.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_101_150.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_151_159.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_51_100_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_101_139_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_lan_3.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50_lan_4.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50_lan_4.csv',\n",
    "\n",
    "\n",
    "        './src/conference/evaluate/evaluate_crawl_not_crawl_1_50.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_not_crawl_51_end.csv',\n",
    "\n",
    "\n",
    "        \n",
    "        './src/conference/evaluate/recrawl_batch_4_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_5_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_9_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_14_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_15_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_18_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_20_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_1_50.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_51_100.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_101_150.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_1_32_lan_2.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_lan_3.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_lan_4.csv',\n",
    "\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_51_100_lan_5.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv',\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    ]\n",
    "    aggregated_correct_file_note = './src/conference/evaluate/tri_tung_all_correct_final.csv'\n",
    "    aggregated_recrawl_file_note = './src/conference/evaluate/tri_tung_all_recrawl_final.csv'\n",
    "\n",
    "    all_files_for_counting = recrawled_results_files + input_csv_files_for_note_check\n",
    "    calculate_and_print_total_unique_count(all_files_for_counting)\n",
    "\n",
    "    process_by_any_correct_version(\n",
    "        input_files=input_csv_files_for_note_check,\n",
    "        aggregated_correct_path=aggregated_correct_file_note,\n",
    "        aggregated_recrawl_path=aggregated_recrawl_file_note,\n",
    "        recrawled_files=recrawled_results_files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu quá trình merge các file CSV và lọc các dòng non-link để tạo full.csv...\n",
      "Đã đọc thành công: ./src/conference/evaluate/tri_tung_all_correct_final.csv (có 39 cột)\n",
      "Đã đọc thành công: ./src/conference/evaluate/correctFiles/batch_1_6_7_10_11_17.csv (có 25 cột)\n",
      "\n",
      "Tìm thấy 25 cột chung ở tất cả các file:\n",
      "Thứ tự cột trong file output sẽ dựa trên file cuối cùng: ./src/conference/evaluate/correctFiles/batch_1_6_7_10_11_17.csv\n",
      "Thứ tự các cột: ['requestId', 'originalRequestId', 'title', 'acronym', 'mainLink', 'cfpLink', 'impLink', 'information', 'conferenceDates', 'year', 'location', 'cityStateProvince', 'country', 'continent', 'type', 'submissionDate', 'notificationDate', 'cameraReadyDate', 'registrationDate', 'otherDate', 'topics', 'publisher', 'summary', 'callForPapers', 'Unnamed: 24']\n",
      "DataFrame đã hợp nhất ban đầu có 887 dòng.\n",
      "\n",
      "Đã hợp nhất và lọc thành công các file vào './src/conference/evaluate/full.csv'.\n",
      "File kết quả có 887 dòng và 25 cột.\n",
      "\n",
      "--- Bắt đầu kiểm tra định dạng JSON trong các cột 'Date' ---\n",
      "Các cột sẽ được kiểm tra: ['submissionDate', 'notificationDate', 'cameraReadyDate', 'registrationDate', 'otherDate']\n",
      "\n",
      ">>> KIỂM TRA THÀNH CÔNG: Tất cả các giá trị trong các cột 'Date' đều là JSON hợp lệ (hoặc rỗng).\n",
      "\n",
      "--- Bắt đầu tạo file chứa các dòng thiếu thông tin (conferenceDates/location) ---\n",
      "\n",
      "Đã tạo thành công file './src/conference/evaluate/missing_info.csv' với 48 dòng.\n",
      "\n",
      "--- Bắt đầu tạo file chứa các dòng có 'submissionDate' là \"{}\" (trừ những dòng đã trong missing_info) ---\n",
      "Đã đọc thành công file missing_info: './src/conference/evaluate/missing_info.csv'.\n",
      "Đã loại bỏ 23 dòng trùng lặp với missing_info.csv.\n",
      "\n",
      "Đã tạo thành công file './src/conference/evaluate/empty_submission_date.csv' với 59 dòng.\n",
      "\n",
      "--- Bắt đầu cập nhật './src/conference/evaluate/full.csv' bằng dữ liệu từ './src/conference/evaluate/evaluate_empty_submission_date.csv' ---\n",
      "Đã đọc file full.csv: 887 dòng.\n",
      "Đã đọc file recrawled data: 54 dòng.\n",
      "\n",
      "Đã cập nhật thành công '54' dòng trong './src/conference/evaluate/full.csv'.\n",
      "File './src/conference/evaluate/full.csv' hiện có 887 dòng.\n",
      "Không có dòng nào từ file recrawled không tìm thấy khớp trong full.csv.\n",
      "\n",
      "--- Hoàn thành chương trình ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_non_link_acronyms(filepath):\n",
    "    \"\"\"\n",
    "    Đọc file non_link.txt và trả về một set chứa các acronym.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Đường dẫn đến file non_link.txt.\n",
    "\n",
    "    Returns:\n",
    "        set: Một set chứa các acronym từ file.\n",
    "    \"\"\"\n",
    "    acronyms = set()\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Cảnh báo: File non_link.txt không tồn tại tại '{filepath}'. Không áp dụng lọc non-link.\")\n",
    "        return acronyms\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                acronyms.add(line.strip())\n",
    "        print(f\"Đã tải {len(acronyms)} acronym từ '{filepath}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file non_link.txt '{filepath}': {e}. Không áp dụng lọc non-link.\")\n",
    "    return acronyms\n",
    "\n",
    "def merge_csv_common_columns_ordered(file_paths, output_filename, non_link_acronyms_filepath=None):\n",
    "    \"\"\"\n",
    "    Đọc nhiều file CSV, tìm các cột chung (phân biệt hoa thường),\n",
    "    sau đó hợp nhất các file chỉ với các cột chung đó.\n",
    "    Thứ tự các cột trong file kết quả sẽ theo thứ tự của các cột chung\n",
    "    trong file CSV cuối cùng trong danh sách input.\n",
    "    Sau đó, loại bỏ các dòng có acronym nằm trong danh sách non-link.\n",
    "\n",
    "    Args:\n",
    "        file_paths (list): Danh sách các đường dẫn đầy đủ đến các file CSV.\n",
    "        output_filename (str): Đường dẫn và tên file CSV đầu ra.\n",
    "        non_link_acronyms_filepath (str, optional): Đường dẫn đến file non_link.txt.\n",
    "                                                    Nếu được cung cấp, các dòng có acronym trong file này sẽ bị loại bỏ.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True nếu merge thành công, False nếu có lỗi.\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"Lỗi: Danh sách đường dẫn file CSV trống. Không có gì để xử lý.\")\n",
    "        return False\n",
    "\n",
    "    dataframes = []\n",
    "    # Bước 1: Đọc tất cả các file CSV vào DataFrame\n",
    "    for fp in file_paths:\n",
    "        try:\n",
    "            # Thêm dtype=str để đảm bảo mọi thứ được đọc vào dưới dạng chuỗi,\n",
    "            # tránh việc pandas tự động chuyển đổi kiểu dữ liệu có thể làm hỏng JSON.\n",
    "            df = pd.read_csv(fp, dtype=str)\n",
    "            dataframes.append(df)\n",
    "            print(f\"Đã đọc thành công: {fp} (có {len(df.columns)} cột)\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Lỗi: Không tìm thấy file tại đường dẫn: {fp}. Bỏ qua file này.\")\n",
    "            continue\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Cảnh báo: File trống hoặc không có header: {fp}. Bỏ qua file này.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi đọc file {fp}: {e}. Bỏ qua file này.\")\n",
    "            continue\n",
    "\n",
    "    if not dataframes:\n",
    "        print(\"Không có file CSV nào được đọc thành công. Không thể thực hiện merge.\")\n",
    "        return False\n",
    "\n",
    "    # Bước 2: Tìm danh sách các cột chung có ở tất cả các file\n",
    "    common_columns_set = set(dataframes[0].columns)\n",
    "    for i in range(1, len(dataframes)):\n",
    "        common_columns_set.intersection_update(set(dataframes[i].columns))\n",
    "\n",
    "    if not common_columns_set:\n",
    "        print(\"Không tìm thấy cột chung nào ở TẤT CẢ các file CSV đã đọc. Không tạo file mới.\")\n",
    "        return False\n",
    "\n",
    "    # Bước 3: Lấy thứ tự cột từ file cuối cùng trong danh sách\n",
    "    last_df = dataframes[-1]\n",
    "    ordered_common_columns = [col for col in last_df.columns if col in common_columns_set]\n",
    "    \n",
    "    if len(ordered_common_columns) != len(common_columns_set):\n",
    "        print(\"Cảnh báo: Thứ tự cột từ file cuối cùng không chứa tất cả các cột chung. Sẽ sắp xếp theo alphabet.\")\n",
    "        ordered_common_columns = sorted(list(common_columns_set))\n",
    "\n",
    "    print(f\"\\nTìm thấy {len(ordered_common_columns)} cột chung ở tất cả các file:\")\n",
    "    print(f\"Thứ tự cột trong file output sẽ dựa trên file cuối cùng: {file_paths[-1]}\")\n",
    "    print(f\"Thứ tự các cột: {ordered_common_columns}\")\n",
    "\n",
    "    # Bước 4: Tạo danh sách các DataFrame mới, chỉ chứa các cột chung\n",
    "    filtered_dataframes = [df[ordered_common_columns] for df in dataframes]\n",
    "\n",
    "    # Bước 5: Hợp nhất tất cả các DataFrame đã lọc lại với nhau\n",
    "    try:\n",
    "        merged_df = pd.concat(filtered_dataframes, ignore_index=True)\n",
    "        print(f\"DataFrame đã hợp nhất ban đầu có {merged_df.shape[0]} dòng.\")\n",
    "\n",
    "        # Bước 5.5: Lọc bỏ các dòng có acronym nằm trong non_link.txt\n",
    "        if non_link_acronyms_filepath and 'acronym' in merged_df.columns:\n",
    "            non_link_acronyms = load_non_link_acronyms(non_link_acronyms_filepath)\n",
    "            if non_link_acronyms:\n",
    "                initial_rows = len(merged_df)\n",
    "                merged_df = merged_df[~merged_df['acronym'].isin(non_link_acronyms)]\n",
    "                removed_rows = initial_rows - len(merged_df)\n",
    "                if removed_rows > 0:\n",
    "                    print(f\"Đã loại bỏ {removed_rows} dòng có acronym trong non_link.txt.\")\n",
    "                else:\n",
    "                    print(\"Không có dòng nào được loại bỏ theo non_link.txt.\")\n",
    "            else:\n",
    "                print(\"Không có acronym nào để lọc từ non_link.txt hoặc file không hợp lệ.\")\n",
    "        elif 'acronym' not in merged_df.columns and non_link_acronyms_filepath:\n",
    "            print(\"Cảnh báo: Cột 'acronym' không tồn tại trong DataFrame đã hợp nhất. Không thể lọc non-link.\")\n",
    "\n",
    "        # Bước 6: Lưu DataFrame đã hợp nhất và lọc vào file CSV mới\n",
    "        merged_df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nĐã hợp nhất và lọc thành công các file vào '{output_filename}'.\")\n",
    "        print(f\"File kết quả có {merged_df.shape[0]} dòng và {merged_df.shape[1]} cột.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi hợp nhất các DataFrame hoặc ghi file: {e}\")\n",
    "        return False\n",
    "\n",
    "def validate_json_in_date_columns(csv_filepath):\n",
    "    \"\"\"\n",
    "    Kiểm tra các cột kết thúc bằng \"Date\" trong một file CSV để xem mỗi ô\n",
    "    có chứa một chuỗi JSON hợp lệ hay không.\n",
    "\n",
    "    Args:\n",
    "        csv_filepath (str): Đường dẫn đến file CSV cần kiểm tra.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Bắt đầu kiểm tra định dạng JSON trong các cột 'Date' ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filepath, dtype=str).fillna('') # Đọc mọi thứ dạng chuỗi, và thay NaN bằng chuỗi rỗng\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file '{csv_filepath}' để kiểm tra.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file '{csv_filepath}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Tìm các cột có tên kết thúc bằng \"Date\"\n",
    "    date_columns = [col for col in df.columns if col.endswith(\"Date\")]\n",
    "\n",
    "    if not date_columns:\n",
    "        print(\"Không tìm thấy cột nào kết thúc bằng 'Date' để kiểm tra.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Các cột sẽ được kiểm tra: {date_columns}\")\n",
    "\n",
    "    invalid_entries = []\n",
    "    # Lặp qua từng cột cần kiểm tra\n",
    "    for col_name in date_columns:\n",
    "        # Lặp qua từng dòng (index và value) trong cột đó\n",
    "        for index, value in df[col_name].items():\n",
    "            # Bỏ qua các ô rỗng hoặc chỉ có khoảng trắng\n",
    "            if not value or value.isspace():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Thử phân tích chuỗi thành JSON\n",
    "                json.loads(value)\n",
    "            except json.JSONDecodeError:\n",
    "                # Nếu thất bại, ghi lại thông tin lỗi\n",
    "                invalid_entries.append({\n",
    "                    \"row_index\": index,\n",
    "                    \"column\": col_name,\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "    # In kết quả\n",
    "    if not invalid_entries:\n",
    "        print(\"\\n>>> KIỂM TRA THÀNH CÔNG: Tất cả các giá trị trong các cột 'Date' đều là JSON hợp lệ (hoặc rỗng).\")\n",
    "    else:\n",
    "        print(f\"\\n>>> KIỂM TRA THẤT BẠI: Tìm thấy {len(invalid_entries)} giá trị không phải là JSON hợp lệ:\")\n",
    "        print(\"-\" * 50)\n",
    "        for error in invalid_entries:\n",
    "            print(f\"  - Dòng (chỉ số 0): {error['row_index']}\")\n",
    "            print(f\"    Cột              : '{error['column']}'\")\n",
    "            print(f\"    Giá trị không hợp lệ: {error['value']}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "def create_missing_info_file(full_csv_path, output_missing_info_path):\n",
    "    \"\"\"\n",
    "    Đọc file CSV đã hợp nhất, lọc ra các dòng có 'conferenceDates' hoặc 'location' trống\n",
    "    hoặc chứa các giá trị cụ thể biểu thị thông tin thiếu,\n",
    "    và lưu các cột 'title', 'acronym' của chúng vào một file mới.\n",
    "\n",
    "    Args:\n",
    "        full_csv_path (str): Đường dẫn đến file CSV đầy đủ (đã merge).\n",
    "        output_missing_info_path (str): Đường dẫn file CSV đầu ra cho dữ liệu thiếu.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Bắt đầu tạo file chứa các dòng thiếu thông tin (conferenceDates/location) ---\")\n",
    "    try:\n",
    "        # Đọc file full.csv, thay thế các giá trị NaN bằng chuỗi rỗng để dễ xử lý\n",
    "        df = pd.read_csv(full_csv_path, dtype=str).fillna('')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file '{full_csv_path}' để xử lý.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file '{full_csv_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Kiểm tra xem các cột cần thiết có tồn tại không\n",
    "    required_columns = ['conferenceDates', 'location', 'title', 'acronym']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(f\"Lỗi: File '{full_csv_path}' thiếu một hoặc nhiều cột cần thiết: {required_columns}.\")\n",
    "        return\n",
    "\n",
    "    # Định nghĩa các giá trị được coi là thiếu thông tin\n",
    "    missing_dates_values = ['', 'To be announced', 'TBD', 'To be determined']\n",
    "    missing_location_values = ['', 'No location', 'TBD']\n",
    "\n",
    "    # Lọc các dòng mà 'conferenceDates' hoặc 'location' nằm trong danh sách các giá trị thiếu\n",
    "    missing_info_df = df[(df['conferenceDates'].isin(missing_dates_values)) | \n",
    "                         (df['location'].isin(missing_location_values))]\n",
    "\n",
    "    if missing_info_df.empty:\n",
    "        print(\"Không tìm thấy dòng nào có 'conferenceDates' hoặc 'location' trống/thiếu thông tin cụ thể.\")\n",
    "        return\n",
    "\n",
    "    # Chỉ giữ lại hai cột 'title' và 'acronym'\n",
    "    final_df = missing_info_df[['title', 'acronym']]\n",
    "\n",
    "    # Lưu vào file CSV mới\n",
    "    try:\n",
    "        final_df.to_csv(output_missing_info_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nĐã tạo thành công file '{output_missing_info_path}' với {len(final_df)} dòng.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file '{output_missing_info_path}': {e}\")\n",
    "\n",
    "def create_empty_submission_date_file(full_csv_path, output_empty_submission_path, missing_info_csv_path):\n",
    "    \"\"\"\n",
    "    Đọc file CSV đã hợp nhất, lọc ra các dòng có cột 'submissionDate' là \"{}\",\n",
    "    và loại trừ những dòng đã có trong file missing_info.csv (dựa trên title và acronym),\n",
    "    sau đó lưu toàn bộ các cột của chúng vào một file mới.\n",
    "\n",
    "    Args:\n",
    "        full_csv_path (str): Đường dẫn đến file CSV đầy đủ (đã merge).\n",
    "        output_empty_submission_path (str): Đường dẫn file CSV đầu ra cho dữ liệu có 'submissionDate' là \"{}\".\n",
    "        missing_info_csv_path (str): Đường dẫn đến file CSV chứa các dòng thiếu thông tin.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Bắt đầu tạo file chứa các dòng có 'submissionDate' là \\\"{}\\\" (trừ những dòng đã trong missing_info) ---\")\n",
    "    try:\n",
    "        # Đọc file full.csv\n",
    "        df_full = pd.read_csv(full_csv_path, dtype=str).fillna('')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file '{full_csv_path}' để xử lý.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file '{full_csv_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Kiểm tra xem cột 'submissionDate' có tồn tại không\n",
    "    if 'submissionDate' not in df_full.columns:\n",
    "        print(f\"Lỗi: File '{full_csv_path}' không có cột 'submissionDate'.\")\n",
    "        return\n",
    "    if 'title' not in df_full.columns or 'acronym' not in df_full.columns:\n",
    "        print(f\"Lỗi: File '{full_csv_path}' thiếu cột 'title' hoặc 'acronym', không thể loại trừ các dòng.\")\n",
    "        return\n",
    "\n",
    "    # Lọc các dòng mà 'submissionDate' có giá trị chính xác là \"{}\"\n",
    "    empty_submission_df = df_full[df_full['submissionDate'] == '{}'].copy() # Tạo bản sao để tránh SettingWithCopyWarning\n",
    "\n",
    "    if empty_submission_df.empty:\n",
    "        print(\"Không tìm thấy dòng nào có 'submissionDate' là \\\"{}\\\".\")\n",
    "        return\n",
    "\n",
    "    # Bước loại trừ: Đọc file missing_info.csv\n",
    "    try:\n",
    "        df_missing = pd.read_csv(missing_info_csv_path, dtype=str).fillna('')\n",
    "        print(f\"Đã đọc thành công file missing_info: '{missing_info_csv_path}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Cảnh báo: Không tìm thấy file missing_info tại '{missing_info_csv_path}'. Không thực hiện loại trừ.\")\n",
    "        df_missing = pd.DataFrame(columns=['title', 'acronym']) # Tạo DataFrame rỗng để không ảnh hưởng logic tiếp theo\n",
    "    except Exception as e:\n",
    "        print(f\"Cảnh báo: Lỗi khi đọc file missing_info '{missing_info_csv_path}': {e}. Không thực hiện loại trừ.\")\n",
    "        df_missing = pd.DataFrame(columns=['title', 'acronym'])\n",
    "\n",
    "\n",
    "    # Tạo một cột kết hợp 'title_acronym' để dễ dàng so sánh\n",
    "    if 'title' in df_missing.columns and 'acronym' in df_missing.columns:\n",
    "        empty_submission_df['merge_key'] = empty_submission_df['title'].astype(str) + \"_\" + empty_submission_df['acronym'].astype(str)\n",
    "        df_missing['merge_key'] = df_missing['title'].astype(str) + \"_\" + df_missing['acronym'].astype(str)\n",
    "\n",
    "        # Loại bỏ các dòng trong empty_submission_df nếu merge_key của chúng tồn tại trong df_missing\n",
    "        initial_rows = len(empty_submission_df)\n",
    "        empty_submission_df = empty_submission_df[~empty_submission_df['merge_key'].isin(df_missing['merge_key'])]\n",
    "        removed_rows = initial_rows - len(empty_submission_df)\n",
    "        \n",
    "        if removed_rows > 0:\n",
    "            print(f\"Đã loại bỏ {removed_rows} dòng trùng lặp với missing_info.csv.\")\n",
    "        else:\n",
    "            print(\"Không có dòng nào trong empty_submission_date trùng với missing_info để loại bỏ.\")\n",
    "\n",
    "        # Xóa cột merge_key trước khi lưu\n",
    "        empty_submission_df = empty_submission_df.drop(columns=['merge_key'])\n",
    "    else:\n",
    "        print(\"Cảnh báo: File missing_info.csv không có đủ cột 'title' hoặc 'acronym' để thực hiện loại trừ.\")\n",
    "\n",
    "    if empty_submission_df.empty:\n",
    "        print(\"Sau khi loại trừ, không còn dòng nào thỏa mãn điều kiện để tạo file.\")\n",
    "        return\n",
    "\n",
    "    # Lưu toàn bộ DataFrame đã lọc vào file CSV mới\n",
    "    try:\n",
    "        empty_submission_df.to_csv(output_empty_submission_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nĐã tạo thành công file '{output_empty_submission_path}' với {len(empty_submission_df)} dòng.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file '{output_empty_submission_path}': {e}\")\n",
    "\n",
    "# --- HÀM ĐƯỢC CẬP NHẬT ĐỂ XUẤT CÁC DÒNG KHÔNG ĐƯỢC CẬP NHẬT ---\n",
    "def update_full_csv_with_recrawled_data(full_csv_path, recrawled_empty_submission_path, output_unmatched_recrawl_path):\n",
    "    \"\"\"\n",
    "    Cập nhật các dòng trong full.csv bằng các dòng tương ứng từ recrawled_empty_submission_path\n",
    "    dựa trên cặp 'title' và 'acronym'.\n",
    "    Đồng thời, xuất các dòng từ recrawled_empty_submission_path mà không tìm thấy khớp\n",
    "    trong full.csv vào một file riêng.\n",
    "\n",
    "    Args:\n",
    "        full_csv_path (str): Đường dẫn đến file full.csv cần cập nhật.\n",
    "        recrawled_empty_submission_path (str): Đường dẫn đến file chứa dữ liệu đã được recrawl\n",
    "                                                (ví dụ: empty_submission_date.csv sau khi đã được điền dữ liệu).\n",
    "        output_unmatched_recrawl_path (str): Đường dẫn file CSV đầu ra cho các dòng không khớp.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Bắt đầu cập nhật '{full_csv_path}' bằng dữ liệu từ '{recrawled_empty_submission_path}' ---\")\n",
    "    try:\n",
    "        df_full = pd.read_csv(full_csv_path, dtype=str).fillna('')\n",
    "        print(f\"Đã đọc file full.csv: {len(df_full)} dòng.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file '{full_csv_path}'. Không thể cập nhật.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file '{full_csv_path}': {e}. Không thể cập nhật.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_recrawled = pd.read_csv(recrawled_empty_submission_path, dtype=str).fillna('')\n",
    "        print(f\"Đã đọc file recrawled data: {len(df_recrawled)} dòng.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file '{recrawled_empty_submission_path}'. Không có dữ liệu để cập nhật.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file '{recrawled_empty_submission_path}': {e}. Không có dữ liệu để cập nhật.\")\n",
    "        return\n",
    "\n",
    "    # Kiểm tra các cột cần thiết\n",
    "    required_cols = ['title', 'acronym']\n",
    "    if not all(col in df_full.columns for col in required_cols):\n",
    "        print(f\"Lỗi: File '{full_csv_path}' thiếu một hoặc nhiều cột cần thiết ({required_cols}).\")\n",
    "        return\n",
    "    if not all(col in df_recrawled.columns for col in required_cols):\n",
    "        print(f\"Lỗi: File '{recrawled_empty_submission_path}' thiếu một hoặc nhiều cột cần thiết ({required_cols}).\")\n",
    "        return\n",
    "\n",
    "    # Tạo một cột khóa để merge/cập nhật\n",
    "    df_full['merge_key'] = df_full['title'].astype(str) + \"_\" + df_full['acronym'].astype(str)\n",
    "    df_recrawled['merge_key'] = df_recrawled['title'].astype(str) + \"_\" + df_recrawled['acronym'].astype(str)\n",
    "\n",
    "    updated_rows_count = 0\n",
    "    unmatched_recrawled_rows = []\n",
    "\n",
    "    # Lặp qua từng dòng trong df_recrawled để cập nhật df_full\n",
    "    for index_recrawled, row_recrawled in df_recrawled.iterrows():\n",
    "        merge_key = row_recrawled['merge_key']\n",
    "        \n",
    "        # Tìm vị trí của dòng cần cập nhật trong df_full\n",
    "        matching_rows_indices = df_full[df_full['merge_key'] == merge_key].index\n",
    "        \n",
    "        if not matching_rows_indices.empty:\n",
    "            # Cập nhật tất cả các cột từ row_recrawled vào các dòng tương ứng trong df_full\n",
    "            cols_to_update = [col for col in df_recrawled.columns if col in df_full.columns and col != 'merge_key']\n",
    "            \n",
    "            for col in cols_to_update:\n",
    "                df_full.loc[matching_rows_indices, col] = row_recrawled[col]\n",
    "            \n",
    "            updated_rows_count += len(matching_rows_indices)\n",
    "        else:\n",
    "            # Nếu không tìm thấy khớp, thêm dòng này vào danh sách các dòng không khớp\n",
    "            unmatched_recrawled_rows.append(row_recrawled.drop('merge_key').to_dict())\n",
    "\n",
    "    # Xóa cột merge_key trước khi lưu\n",
    "    df_full = df_full.drop(columns=['merge_key'])\n",
    "\n",
    "    # Lưu DataFrame đã cập nhật vào file full.csv\n",
    "    try:\n",
    "        df_full.to_csv(full_csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nĐã cập nhật thành công '{updated_rows_count}' dòng trong '{full_csv_path}'.\")\n",
    "        print(f\"File '{full_csv_path}' hiện có {len(df_full)} dòng.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file '{full_csv_path}' sau khi cập nhật: {e}\")\n",
    "\n",
    "    # Xử lý các dòng không khớp\n",
    "    if unmatched_recrawled_rows:\n",
    "        df_unmatched = pd.DataFrame(unmatched_recrawled_rows)\n",
    "        try:\n",
    "            df_unmatched.to_csv(output_unmatched_recrawl_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"Đã xuất {len(df_unmatched)} dòng từ '{recrawled_empty_submission_path}' không tìm thấy khớp trong '{full_csv_path}' vào '{output_unmatched_recrawl_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi ghi file các dòng không khớp '{output_unmatched_recrawl_path}': {e}\")\n",
    "    else:\n",
    "        print(\"Không có dòng nào từ file recrawled không tìm thấy khớp trong full.csv.\")\n",
    "\n",
    "\n",
    "# --- Cách sử dụng ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Đặt danh sách các đường dẫn đến file CSV của bạn vào đây.\n",
    "    input_files = [\n",
    "        './src/conference/evaluate/tri_tung_all_correct_final.csv',\n",
    "        './src/conference/evaluate/correctFiles/batch_1_6_7_10_11_17.csv'\n",
    "    ]\n",
    "\n",
    "    # Đặt tên cho các file CSV kết quả đầu ra\n",
    "    output_merged_file = './src/conference/evaluate/full.csv'\n",
    "    output_missing_file = './src/conference/evaluate/missing_info.csv' # File thiếu conferenceDates/location\n",
    "    output_empty_submission_file = './src/conference/evaluate/empty_submission_date.csv' # File thiếu submissionDate\n",
    "    \n",
    "    # Đường dẫn đến file recrawled data (file empty_submission_date.csv sau khi đã được điền dữ liệu)\n",
    "    recrawled_empty_submission_file_path = './src/conference/evaluate/evaluate_empty_submission_date.csv' \n",
    "    # Đường dẫn cho file chứa các dòng không khớp\n",
    "    output_unmatched_recrawl_file = './src/conference/evaluate/unmatched_recrawled_entries.csv'\n",
    "\n",
    "    # --- QUY TRÌNH CHÍNH ---\n",
    "\n",
    "    # Bước 1: Merge các file CSV và lọc non-link để tạo full.csv ban đầu\n",
    "    print(\"Bắt đầu quá trình merge các file CSV và lọc các dòng non-link để tạo full.csv...\")\n",
    "    merge_successful = merge_csv_common_columns_ordered(input_files, output_merged_file)\n",
    "\n",
    "    if merge_successful:\n",
    "        # Bước 2: Kiểm tra định dạng JSON trong file full.csv\n",
    "        validate_json_in_date_columns(output_merged_file)\n",
    "        \n",
    "        # Bước 3: Tạo file chứa các dòng thiếu thông tin (conferenceDates/location)\n",
    "        create_missing_info_file(output_merged_file, output_missing_file)\n",
    "\n",
    "        # Bước 4: Tạo file chứa các dòng có 'submissionDate' là \"{}\" và không nằm trong missing_info\n",
    "        create_empty_submission_date_file(output_merged_file, output_empty_submission_file, output_missing_file)\n",
    "\n",
    "        # Bước 5: Cập nhật full.csv từ file recrawled empty submission date và xuất các dòng không khớp\n",
    "        update_full_csv_with_recrawled_data(output_merged_file, recrawled_empty_submission_file_path, output_unmatched_recrawl_file)\n",
    "\n",
    "    print(\"\\n--- Hoàn thành chương trình ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BƯỚC 1: ĐỌC DỮ LIỆU ---\n",
      "Đang đọc file full.csv từ: ./src/conference/evaluate/full.csv\n",
      "Đã đọc full.csv thành công. 5 dòng đầu tiên:\n",
      "                      requestId originalRequestId                                                                        title acronym                                      mainLink                                                                                 cfpLink                                                       impLink                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           information        conferenceDates    year                                        location  cityStateProvince        country      continent     type                                                                                                                                                                                                                                                                                                                        submissionDate                                                                                                                                        notificationDate                                                                                                                                         cameraReadyDate                           registrationDate                                                                                                                                                      otherDate                                                                                                                                                                                                                                                                                                                                                                                                                                  topics        publisher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 callForPapers Unnamed: 24\n",
      "0  req-conf-1751535138073-e77y3               NaN                Symposium of Asian Association for Algorithms and Computation    AAAC  https://conference.cs.cityu.edu.hk/aaac2025/                            https://conference.cs.cityu.edu.hk/aaac2025/#call-for-papers  https://conference.cs.cityu.edu.hk/aaac2025/#call-for-papers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Conference dates: May 31 - June 1, 2025\\nYear: 2025\\nLocation: City University of Hong Kong, Hong Kong, China\\nCity-State-Province: Hong Kong\\nCountry: China\\nContinent: Asia\\nType: Offline\\nSubmission Date.Submission due: February 28, 2025\\nNotification Date.Notification: March 28, 2025\\nCamera-ready Date.Camera-ready version due: April 18, 2025\\nRegistration Date.Early bird registration by: April 25\\nTopics: Theoretical Computer Science, Algorithms, Complexity Theory  May 31 - June 1, 2025  2025.0  City University of Hong Kong, Hong Kong, China          Hong Kong          China           Asia  Offline                                                                                                                                                                                                                                                                                                {\"Submission due\":\"February 28, 2025\"}                                                                                                                       {\"Notification\":\"March 28, 2025\"}                                                                                                           {\"Camera-ready version due\":\"April 18, 2025\"}  {\"Early bird registration by\":\"April 25\"}                                                                                                                                                             {}                                                                                                                                                                                                                                                                                                                                                                             Theoretical Computer Science, Algorithms, Complexity Theory     No publisher                                                                          The 16th Annual Meeting of the Asian Association for Algorithms and Computation (AAAC 2025) will be held on May 31 and June 1, 2025, at the City University of Hong Kong, Hong Kong, China. The conference invites submissions of one-page abstracts presenting original research or surveys in theoretical computer science, with a focus on algorithm design and complexity theory. The event is face-to-face, and at least one author of each submission is expected to attend and present.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             # AAAC 2025: Call for Papers\\n\\nThe 16th Annual Meeting of the Asian Association for Algorithms and Computation (AAAC 2025) will take place on **May 31 and June 1, 2025**, at the **City University of Hong Kong, Hong Kong, China**.\\n\\nThis meeting is a **face-to-face event**, and at least one author of each submission is expected to register and give the talk at the venue.\\n\\n## Topics\\n\\nAll areas of theoretical computer science are welcome, with a special emphasis on **design and analysis of algorithms** and **complexity theory**.\\n\\n## Submissions\\n\\nAuthors are invited to submit **one-page (A4) abstracts in PDF format**. These abstracts can be based on original results or surveys of existing results.\\n\\nInformal working notes, including the one-page abstracts, will be distributed at the meeting. This distribution does not prevent future publication of the same work in any form.\\n\\nAll submissions must be made electronically through the EasyChair Conference System: [https://easychair.org/conferences?conf=aaac2025](https://easychair.org/conferences?conf=aaac2025).\\n\\n## Important Dates\\n\\n*   **Submission Due:** February 28, 2025 (AoE)\\n*   **Notification:** March 28, 2025\\n*   **Camera-ready Version Due:** April 18, 2025\\n*   **Early Registration Due:** April 25, 2025\\n*   **Conference Dates:** May 31 - June 1, 2025\\n\\n## Best Student Presentation Award\\n\\nThe **Best Student Presentation Award** will be presented to a full-time student author who presents their paper at the meeting. The award will be selected by AAAC board members during the conference.\\n\\n## Registration\\n\\n*   **Normal Registration:** 1800 HKD\\n*   **Student Registration:** 900 HKD\\n*   **Early Bird Registration Deadline:** April 25, 2025\\n*   **Registration Link:** [https://www.hkws.org/registration/aaac2025/reg.html](https://www.hkws.org/registration/aaac2025/reg.html)\\n\\n## Program Committees\\n\\n*   Hee-Kap Ahn (Pohang University of Science and Technology (POSTECH))\\n*   Sang Won Bae (Kyonggi University)\\n*   Ho-Lin Chen (National Taiwan University)\\n*   Siu-Wing Cheng (The Hong Kong University of Science and Technology, Chair)\\n*   Minming Li (City University of Hong Kong)\\n*   Chung-Shou Liao (National Tsing Hua University)\\n*   Pinyan Lu (Shanghai University of Finance and Economics)\\n*   Heejin Park (Hanyang University)\\n*   Kunihiko Sadakane (The University of Tokyo)\\n*   Xiaoming Sun (Chinese Academy of Sciences)\\n*   Takeshi Tokuyama (Tohoku University)\\n*   Ryuhei Uehara (Japan Advanced Institute of Science and Technology (JAIST))\\n*   Guochuan Zhang (Zhejiang University)\\n*   Shengyu Zhang (The Chinese University of Hong Kong)\\n\\n## Accepted Papers\\n\\n*   Instance Optimality in Differential Privacy\\n*   On the Efficiency of Fair and Truthful Trade Mechanisms\\n*   Dynamic Maximum Depth of Geometric Objects\\n*   Confusion Matrix Design for Downstream Decision-making\\n*   Long Arithmetic Progressions in Sumsets and Subset Sums: Constructive Proofs and Efficient Witnesses\\n*   Crossing Minimization in Ortho-Radial Drawings of Column Trees\\n*   Constant Approximation of Fréchet Distance in Strongly Subquadratic Time\\n*   Near-unit Distance Embedding of Points in the Plane and Space\\n*   Simplification of Trajectory Streams\\n*   Shortcutting the Diameter of a Polygon\\n*   Complexity of Reconfiguring Vertex-Disjoint Shortest Paths\\n*   Online Exploration of Grid Graphs with Multiple Searchers\\n*   Space-efficient Representations for a Set of k-mers\\n*   Private-Preserving Encoding and Decoding Using Variable-Length Coding Schemes\\n*   Mechanism Design for Locating a Bridge Between Regions with Prelocated Facilities\\n*   Space-Efficient Data Structure for (k...21)-avoiding Involutions\\n*   Learning-Augmented Algorithms for Online Time-Window TSP\\n*   Yavalath is PSPACE-Complete\\n*   Learning-augmented Algorithms for Density Peaks Clustering\\n*   Guarding Terrains with Guards on a Line\\n*   Protecting the Connectivity of a Graph Under Non-Uniform Edge Failures\\n*   Improved Algorithms for Finding the Smallest Color-Spanning Two Squares\\n*   The Public University Secretary Problem with Predictions\\n*   Fair Allocation of Items in Multiple Regions\\n*   On the Subsidy of Envy-Free Orientations in Graphs.\\n*   Topological Features for Graph Representations\\n*   Interdiction Game on Machine Scheduling under Budgeted Unavailability Patterns\\n*   Monotone Partitions of Simple Polygons         NaN\n",
      "1  req-conf-1751535138073-e77y3               NaN  National Conference of the American Association for Artificial Intelligence    AAAI     https://aaai.org/conference/aaai/aaai-26/                     https://aaai.org/conference/aaai/aaai-26/main-technical-track-call/                                                           NaN                                                                              Conference dates: January 20 - 27, 2026\\nYear: 2026\\nLocation: Singapore EXPO, Singapore\\nCity-State-Province: Singapore\\nCountry: Singapore\\nContinent: Asia\\nType: Offline\\nSubmission Date.Open Review submission site opens for author registration: June 16, 2025\\nSubmission Date.Open Review submission site opens for paper submission: June 25, 2025\\nSubmission Date.Abstracts due: July 25, 2025\\nSubmission Date.Full papers due: August 1, 2025\\nSubmission Date.Supplementary material and code due: August 4, 2025\\nNotification Date.Notification of Phase 1 rejections: September 8, 2025\\nNotification Date.Notification of final acceptance or rejection (Main Technical Track): November 3, 2025\\nCamera-ready Date.Submission of camera-ready files (Main Technical Track): November 13, 2025\\nTopics: Artificial Intelligence (AI), Machine Learning, Natural Language Processing, Computer Vision, Data Mining, Multiagent Systems, Knowledge Representation, Human-in-the-Loop AI, Search, Planning, Reasoning, Robotics and Perception, Ethics\\nPublisher: AAAI  January 20 - 27, 2026  2026.0                       Singapore EXPO, Singapore          Singapore      Singapore           Asia  Offline                                                      {\"Open Review submission site opens for author registration\":\"June 16, 2025\",\"Open Review submission site opens for paper submission\":\"June 25, 2025\",\"Abstracts due\":\"July 25, 2025\",\"Full papers due\":\"August 1, 2025\",\"Supplementary material and code due\":\"August 4, 2025\"}    {\"Notification of Phase 1 rejections\":\"September 8, 2025\",\"Notification of final acceptance or rejection (Main Technical Track)\":\"November 3, 2025\"}                                                                         {\"Submission of camera-ready files (Main Technical Track)\":\"November 13, 2025\"}                                         {}                                                                                                                                                             {}                                                                                                                                                                                             Artificial Intelligence (AI), Machine Learning, Natural Language Processing, Computer Vision, Data Mining, Multiagent Systems, Knowledge Representation, Human-in-the-Loop AI, Search, Planning, Reasoning, Robotics and Perception, Ethics             AAAI  The 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26) will be held in Singapore EXPO, Singapore, from January 20 to January 27, 2026. The conference aims to promote research in Artificial Intelligence (AI) and foster scientific exchange among researchers, practitioners, students, and engineers. AAAI-26 will feature technical paper presentations, special tracks, invited speakers, workshops, tutorials, and more, with a theme of creating collaborative bridges within and beyond AI, emphasizing AI for social impact and responsible AI.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          # AAAI-26: Call for Papers\\n\\nThe 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26) will be held from **January 20 – January 27, 2026**, at the **Singapore EXPO, Singapore**.\\n\\n### Conference Overview\\n\\n*   **Purpose:** To promote research in Artificial Intelligence (AI) and foster scientific exchange between researchers, practitioners, scientists, students, and engineers across the entirety of AI and its affiliated disciplines.\\n*   **Theme:** Creating collaborative bridges within and beyond AI, with emphasis on AI for social impact and responsible AI.\\n*   **Activities:** Technical paper presentations, special tracks, invited speakers, workshops, tutorials, poster sessions, senior member presentations, competitions, exhibition programs, a Bridge Program, and a Lab Program.\\n\\n## Main Technical Track\\n\\n### Submission Details\\n\\n*   **Page Limit:** Submissions may consist of up to **7 pages** of technical content plus additional pages solely for references.\\n*   **Supplementary Material:** Authors may submit supplementary material (technical appendix; multimedia; code and data), but reviewers are not required to review it. The supplementary material deadline is **August 4, 2025**.\\n*   **Reproducibility:** All authors must complete a reproducibility checklist.\\n*   **Reviewer Availability:** All authors are expected to be available to review (light load).\\n*   **Format:** AAAI-26 is an **in-person conference**.\\n\\n### Appropriate Use of Generative AI by Authors\\n\\n*   Authors may judiciously use generative AI tools, but remain fully responsible for all submitted material.\\n*   Plagiarism, non-existent references, or other violations of the AAAI Code of Professional Ethics and Conduct will be subject to sanctions.\\n*   AI systems, including Generative Models such as ChatGPT, are prohibited from authorship or being used as a citable source.\\n\\n### Timetable for Authors (all deadlines Anywhere on Earth - UTC-12)\\n\\n*   **Open Review submission site opens for author registration:** June 18, 2025\\n*   **Open Review submission site opens for paper submission:** June 25, 2025\\n*   **Abstracts due:** July 25, 2025\\n*   **Full papers due:** August 1, 2025\\n*   **Supplementary material and code due:** August 4, 2025\\n*   **Notification of Phase 1 rejections:** September 8, 2025\\n*   **Author feedback window:** October 2-8, 2025\\n*   **Notification of final acceptance or rejection (Main Technical Track):** November 3, 2025\\n*   **Submission of camera-ready files (Main Technical Track):** November 13, 2025\\n\\n## Special Tracks\\n\\nAAAI-26 features two special tracks: AI for Social Impact and AI Alignment. The same reviewing schedule will be followed for all papers.\\n\\n### Special Track on AI for Social Impact (AISI)\\n\\n*   **Focus:** Emphasizes the fit between AI techniques and problems of social importance, the significance of the addressed problem, engagement with previous literature, novelty and justification of the AI-based approach, quality of evaluation, facilitation of follow-up work, and overall scope for social impact.\\n*   **Review Rubric:** Different from the main track.\\n\\n### Special Track on AI Alignment (AIA)\\n\\n*   **Focus:** Scalable oversight, mechanistic interpretability, empirical robustness evaluation, red-teaming, human cognitive and psychological factors, and safe-by-design engineering. Also welcomes work on transparent governance frameworks, economic incentives, institutional accountability, human-centered modeling and evaluation, and pluralistic coordination methods.\\n*   **Encouraged:** Submissions that release open datasets, reproducible code, or practical evaluation tools.\\n*   **Review Emphasis:** Technical correctness, appropriate coverage of related work, and relevance to the track.\\n\\n## Topics of Interest\\n\\nAAAI-26 welcomes submissions reporting research that advances artificial intelligence broadly conceived. Key areas include:\\n\\n*   Machine learning, natural language processing, computer vision, data mining, multiagent systems, knowledge representation, human-in-the-loop AI, search, planning, reasoning, robotics and perception, and ethics.\\n*   The conference expressly encourages work that cuts across technical areas (e.g., machine learning and computer vision), bridges between AI and related research areas (e.g., neuroscience, cognitive science), or develops AI techniques in the context of important application domains (e.g., healthcare, sustainability, transportation, and commerce).\\n\\n## Review Criteria\\n\\n*   All submissions will be rigorously evaluated for the significance and novelty of contributions, theoretical and/or empirical soundness, relevance to the AAAI community, and clarity of exposition.\\n*   Additional considerations include adherence to responsible research practices and steps to ensure reproducibility.\\n*   Preference is given to solid technical papers that explore new territory or point out new directions for research over those that only incrementally advance the state of the art within a narrow sub-area.\\n\\n## AI-Assisted Peer-Review Process (Pilot Program)\\n\\n*   AAAI-26 will implement a pilot program for an AI-Assisted Peer Review System to explore the use of AI in the review process.\\n*   AI-generated reviews and summaries will supplement human reviews but will not contain ratings or recommendations, and will not play a formal role in paper decisions.\\n*   The two-phase review process will include an AI-generated supplementary review in Phase 1, and authors will be able to respond to all reviews (including the AI-generated one) in Phase 2.\\n*   Privacy controls will be implemented to ensure confidentiality and compliance with data protection regulations.\\n\\n## Contact Information\\n\\n*   **Registration Inquiries:** aaai26@aaai.org\\n*   **Submission Process & Conference Inquiries:** workflowchairs26@aaai.zendesk.com         NaN\n",
      "2  req-conf-1751535138073-e77y3               NaN              Conference on Algorithmic Aspects in Information and Management    AAIM                     https://www.aaim2025.org/                                                     https://www.aaim2025.org/submission                                                           NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Conference dates: June 23 - 25, 2025\\nYear: 2025\\nLocation: Ulaanbaatar, Mongolia\\nCity-State-Province: Ulaanbaatar\\nCountry: Mongolia\\nContinent: Asia\\nType: Hybrid\\nSubmission Date.Abstract Submission Deadline: May 30, 2025\\nSubmission Date.Full Paper Submission Deadline: July 15, 2025\\nNotification Date.Author Notification: August 15, 2025\\nCamera-ready Date.Camera-Ready Version Due: September 1, 2025\\nTopics: Algorithmic Aspects in Information and Management\\nPublisher: Springer-Verlag     June 23 - 25, 2025  2025.0                           Ulaanbaatar, Mongolia        Ulaanbaatar       Mongolia           Asia   Hybrid                                                                                                                                                                                                                                      {\"Abstract Submission Deadline\":\"May 30, 2025\",\"Full Paper Submission Deadline\":\"July 15, 2025\"}                                                                                                               {\"Author Notification\":\"August 15, 2025\"}                                                                                                        {\"Camera-Ready Version Due\":\"September 1, 2025\"}                                         {}                                                                                                                                                             {}                                                                                                                                                                                                                                                                                                                                                                                       Algorithmic Aspects in Information and Management  Springer-Verlag                                                                                                                                                                                                                                                                                                                        The 19th International Conference on Algorithmic Aspects in Information and Management (AAIM 2025) will be held in Ulaanbaatar, Mongolia from June 23-25, 2025. The conference will offer both in-person and online (hybrid mode) participation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              # AAIM 2025: Call for Papers\\n\\nThe 19th International Conference on Algorithmic Aspects in Information and Management (AAIM 2025) will be held during **June 23-25, 2025**, in **Ulaanbaatar, Mongolia**. The conference will be organized in **In-person and Online (Hybrid mode)**.\\n\\n## Important Dates:\\n\\n*   **Abstract Submission Deadline:** May 30, 2025\\n*   **Full Paper Submission Deadline:** July 15, 2025\\n*   **Author Notification:** August 15, 2025\\n*   **Camera-Ready Version Due:** September 1, 2025\\n*   **Conference Dates:** June 23-25, 2025\\n\\nIf you wish to submit a late abstract, please contact altannar@num.edu.mn.\\n\\n## Paper Submission:\\n\\n*   AAIM 2025 will only accept **PDF format submissions** via **Springer Online Conference Service**. Authors need to register their account and submit their paper at the [Springer OCS AAIM2025 Homepage](https://ocs.springer.com/).\\n*   Only previously unpublished new results will be considered for publication. Papers that have already been published or simultaneously submitted to another journal or conference (with published proceedings) will not be considered.\\n*   A submission should start with the title of the paper and a one-paragraph summary of the results, followed by a scholarly exposition of the ideas, techniques, and a full description of the results achieved. A clear indication of the motivation and comparison with prior or related work should be presented.\\n*   The paper should not exceed **12 pages**, including bibliography, formatted for letter-size paper using 10 point font, with at least one inch margins around.\\n*   Authors are required to submit their papers in the **Springer LNCS Format**. For the complete author guideline, please access Springer's official [Springer_Instructions_for_Authors_of_Proceedings_CS.pdf](https://www.springer.com/gp/authors-editors/book-authors-editors/manuscript-submission/policy-and-procedures/instructions-for-authors-of-proceedings/17805620).\\n*   Springer’s proceedings LaTeX templates are available in [ZIP](https://resource-cms.springer.com/springer_prod/collection_generic/115/978-3-031-47525-7/LaTeX.zip) and [Overleaf](https://www.overleaf.com/latex/templates/springer-lecture-notes-in-computer-science-lncs-template/tvfgz7wmvjqs).\\n*   **Double-blind review process:** Please make sure to hide all author-related information.\\n*   Additional details can be included in a clearly marked appendix, to be consulted at the discretion of program committee members. The appendix is not included in the page limit, and will not be published in the conference proceedings.\\n*   Submissions that deviate significantly from these guidelines or are unprintable risk rejection without consideration of their merit.\\n*   At least one author of an accepted paper is expected to present the paper at the conference as a registered participant.\\n\\n## Publication and Special Issue:\\n\\n*   The conference proceedings will be published by **Springer-Verlag in the Lecture Notes in Computer Science (LNCS) series**.\\n*   After the conference, selected papers will be invited to a special issue of **Journal of Combinatorial Optimization**. The invited papers will go through the normal reviewing process.\\n\\n**Contact:** aaim2025conference@gmail.com         NaN\n",
      "3  req-conf-1752303287310-m9cvg               NaN   International Joint Conference on Autonomous Agents and Multiagent Systems   AAMAS                        https://aamas2025.org/  https://aamas2025.org/index.php/conference/calls/call-for-papers-main-technical-track/                                        https://aamas2025.org/                                  Conference dates: May 19 - 23, 2025\\nYear: 2025\\nLocation: Detroit, Michigan, USA\\nCity-State-Province: Detroit, Michigan\\nCountry: United States\\nContinent: North America\\nType: Offline\\nSubmission Date.(Main Technical Track) Abstract: October 9, 2024\\nSubmission Date.(Main Technical Track) Paper: October 16, 2024\\nNotification Date.(Main Technical Track) Author notification: December 23, 2024\\nCamera-ready Date.(Main Technical Track) Camera-ready paper: February 21, 2025\\nOther Date.Tutorials & Workshops: May 19 - 20, 2025\\nOther Date.Main Conference: May 21 - 23, 2025\\nOther Date.(Main Technical Track) Rebuttal period: November 27 - December 4, 2024\\nTopics: Autonomous Agents, Multiagent Systems, Learning and Adaptation, Game Theory and Economic Paradigms, Coordination, Organizations, Institutions, Norms, and Ethics, Search, Optimization, Planning, and Scheduling, Representation, Perception, and Reasoning, Engineering and Analysis of Multiagent Systems, Modeling and Simulation of (Artificial) Societies, Human-Agent Interaction, Robotics and Control, Innovative Applications      May 19 - 23, 2025  2025.0                          Detroit, Michigan, USA  Detroit, Michigan  United States  North America  Offline                                                                                                                                                                                                                               {\"(Main Technical Track) Abstract\":\"October 9, 2024\",\"(Main Technical Track) Paper\":\"October 16, 2024\"}                                                                                      {\"(Main Technical Track) Author notification\":\"December 23, 2024\"}                                                                                       {\"(Main Technical Track) Camera-ready paper\":\"February 21, 2025\"}                                         {}  {\"Tutorials & Workshops\":\"May 19 - 20, 2025\",\"Main Conference\":\"May 21 - 23, 2025\",\"(Main Technical Track) Rebuttal period\":\"November 27 - December 4, 2024\"}  Autonomous Agents, Multiagent Systems, Learning and Adaptation, Game Theory and Economic Paradigms, Coordination, Organizations, Institutions, Norms, and Ethics, Search, Optimization, Planning, and Scheduling, Representation, Perception, and Reasoning, Engineering and Analysis of Multiagent Systems, Modeling and Simulation of (Artificial) Societies, Human-Agent Interaction, Robotics and Control, Innovative Applications     No publisher                                                                                                                                                                                                                                   The 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025) was held from May 19–23, 2025 in Detroit, Michigan, USA. It is the largest and most influential conference in the area of agents and multiagent systems, bringing together researchers and practitioners in all areas of agent technology.  # AAMAS 2025: Call for Papers (Main Technical Track)\\n\\n**NOTE:** This call is officially closed.\\n\\nThe 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025) took place **May 19-23, 2025**, in **Detroit, Michigan, USA**.\\n\\nAll submissions were rigorously peer-reviewed and evaluated on the basis of the overall quality of their technical contribution, taking into account criteria such as originality, significance, soundness, reproducibility, clarity, relevance to the conference, quality of presentation, as well as understanding and appropriate referencing of the state of the art.\\n\\nThe papers were published under CC BY license.\\n\\n## Areas of Interest\\n\\nWe welcomed the submission of technical papers describing significant and original research on all aspects of the theory and practice of autonomous agents and multiagent systems. At the time of submission, authors were asked to associate their paper with one of the following areas of interest:\\n\\n*   **LEARN:** Learning and Adaptation\\n*   **GTEP:** Game Theory and Economic Paradigms\\n*   **COINE:** Coordination, Organizations, Institutions, Norms, and Ethics\\n*   **SOPS:** Search, Optimization, Planning, and Scheduling\\n*   **RPR:** Representation, Perception, and Reasoning\\n*   **EMAS:** Engineering and Analysis of Multiagent Systems\\n*   **SIM:** Modeling and Simulation of Societies\\n*   **HAI:** Human-Agent Interaction\\n*   **ROBOT:** Robotics and Control\\n*   **IA:** Innovative Applications\\n\\n### Learning and Adaptation (LEARN)\\n\\nArea Chairs: Long Tran-Thanh, Bo An, Marc Lanctot, Chongjie Zhang, Jianye Hao, Haifeng Xu, Jakob Foerster\\n\\nTopics:\\n\\n*   Reasoning and learning under uncertainty\\n*   Supervised learning\\n*   Unsupervised and representation learning\\n*   Reinforcement learning\\n*   Multiagent learning\\n*   Evolutionary algorithms\\n*   Learning agent capabilities\\n*   Learning agent-to-agent interactions\\n*   Human-in-the-loop learning\\n*   Agency and learning in large language models (LLMs)\\n*   Learning for value alignment and RLHF\\n*   Modeling and analysis of Generative AI agents\\n*   Few-shot learning\\n*   Distributionally-robust learning\\n*   Adversarial learning\\n\\n### Game Theory and Economic Paradigms (GTEP)\\n\\nArea Chairs: Reshef Meir, Nisarg Shah, Georgios Piliouras, Vasilis Gkatzelis, Rica Gonen\\n\\nTopics:\\n\\n*   Auctions and Mechanism Design\\n*   Bargaining and Negotiation\\n*   Behavioral Game Theory\\n*   Evolutionary Game Theory\\n*   Non-Cooperative Games: Equilibrium Concepts\\n*   Non-Cooperative Games: Computational Issues\\n*   Non-Cooperative Games: Theory and Applications\\n*   Voting and Preference Aggregation\\n*   Social Choice\\n*   Preference Aggregation and Value Alignment\\n*   Matching and Allocation\\n*   Coalition Formation\\n*   Cooperative Games\\n\\n### Coordination, Organizations, Institutions, Norms, and Ethics (COINE)\\n\\nArea Chairs: Felipe Meneguzzi, Pradeep Murukannaiah\\n\\nTopics:\\n\\n*   Coordination and teamwork\\n*   Social network analysis\\n*   Norms, normative systems\\n*   Organizations and institutions\\n*   Non-strategic coalition/team formation\\n*   Communication, including using natural language\\n*   Policy, regulation, and accountability\\n*   Trust and reputation\\n*   Ethical considerations, including privacy, safety, security, transparency\\n*   Agreement Technologies: Negotiation and Argumentations\\n*   Responsible socio-technical systems\\n\\n### Search, Optimization, Planning, and Scheduling (SOPS)\\n\\nArea Chairs: William Yeoh, Sven Koenig\\n\\nTopics:\\n\\n*   Single-agent planning and scheduling\\n*   Multiagent planning and scheduling\\n*   Decentralized planning and scheduling\\n*   Planning under uncertainty\\n*   Combinatorial optimization\\n*   Constraint programming\\n*   Distributed constraint reasoning\\n*   Resource and task allocation\\n*   Non-strategic coalition formation\\n\\n### Representation, Perception, and Reasoning (RPR)\\n\\nArea Chairs: Natasha Alechina, Aparna Taneja, Alessio Lomuscio\\n\\nTopics:\\n\\n*   Computer vision\\n*   Representation learning and generative AI\\n*   Neurosymbolic approaches\\n*   Argumentation\\n*   Agent theories and models\\n*   Explainability\\n*   Logics for agent reasoning\\n*   Ontologies for agents\\n*   Reasoning about knowledge, beliefs, goals, actions, plans, and change in multiagent systems\\n*   Reasoning and problem solving in agent-based systems\\n*   Verification of agents and multiagent systems\\n\\n### Engineering and Analysis of Multiagent Systems (EMAS)\\n\\nArea Chairs: Viviana Mascardi, Daniela Briola\\n\\nTopics:\\n\\n*   Requirements and formal specification\\n*   Architecture and modeling\\n*   Formal verification and validation\\n*   Programming models and languages\\n*   Testing, maintenance, and evolution\\n*   Concurrency, fault tolerance, robustness, reliability, performance, and scalability\\n*   Sociotechnical systems, norms, and governance\\n*   Responsibility and accountability\\n*   Interoperability, business agreements, and interaction protocols\\n*   Declarative, Logic-based, and BDI-based agents\\n*   Engineering ethical agents\\n*   Engineering MAS-based simulations\\n*   Tools and testbeds\\n*   Technological paradigms, including microservices, the Web, the IoT, Cloud computing, distributed Ledgers, and Robotics\\n*   Middleware and platforms for MAS\\n*   Engineering learning agents\\n*   Usability\\n*   Applications, including Finance, Health, Agriculture, Autonomous Vehicles and Smart-*\\n\\n### Modeling and Simulation of (Artificial) Societies (SIM)\\n\\nArea Chairs: Ana Bazzan, Samarth Swarup\\n\\nTopics:\\n\\n*   Analysis of agent-based simulations\\n*   Calibration methods for socio-demographic data\\n*   Agent-based models & Social Networks\\n*   Applications of agent-based simulations in social phenomena (polarization, inequality, etc.)\\n*   Emergent behavior\\n*   Engineering agent-based simulations\\n*   Interactive simulation\\n*   Modeling for agent-based simulation\\n*   Simulation of complex systems\\n*   Simulation techniques, tools and platforms\\n*   Social simulation\\n*   Validation of social simulation systems\\n\\n### Human-Agent Interaction (HAI)\\n\\nArea Chairs: Michael Goodrich, Birgit Lugrin\\n\\nTopics:\\n\\n*   Human-agent interaction\\n*   Agent-based analysis of human interactions\\n*   Socially interactive agents\\n*   Trust and explainability in human-agent interactions\\n*   Human-robot interaction and collaboration\\n*   Social robotics and social interactions\\n*   Mixed-initiative and shared autonomy in human-agent interactions\\n*   Groups of humans and agents\\n*   Agents models and architectures for interaction with humans\\n*   Designing for human-agent interaction\\n*   Virtual humans\\n\\n### Robotics and Control (ROBOT)\\n\\nArea Chairs: Noa Agmon, Christopher Amato\\n\\nTopics:\\n\\n*   Multi-robot coordination and collaboration\\n*   Robot planning\\n*   Robot learning\\n*   Explainability, trust and ethics for robots\\n*   Knowledge representation and reasoning in robotic systems\\n*   Long-term (or lifelong) autonomy for robotic systems\\n*   Mapping, localization and exploration\\n*   Robot Modeling & Simulation\\n*   Manipulation and navigation\\n*   Networked systems and distributed robotics\\n*   Robot control\\n*   Robot perception and vision\\n*   Robots in adversarial settings\\n*   Swarm and collective behavior\\n*   Execution monitoring and failure recovery for robots\\n\\n### Innovative Applications (IA)\\n\\nArea Chairs: Thanh Nguyen, Pradeep Varakantham\\n\\nTopics:\\n\\n*   Deployed or emerging applications of agent-based systems\\n*   Realistic agent-based models of human organizations\\n*   Evaluation of the cognitive capabilities of agent-based systems\\n*   Integrated applications of agent-based and other technologies\\n*   Challenges and best practices of real-world deployments of agent-based technologies\\n\\n## Special Tracks\\n\\nIn addition to the main track, AAMAS 2025 featured four special tracks:\\n\\n*   [AAAI Resubmissions Track](https://aamas2025.org/index.php/conference/calls/aaai-track/)\\n*   [Blue Sky Ideas Track](https://aamas2025.org/index.php/conference/calls/call-for-blue-sky-ideas/)\\n*   [JAAMAS Track](https://aamas2025.org/index.php/conference/calls/jaamas-track/)\\n*   [Demo Track](https://aamas2025.org/index.php/conference/calls/call-for-demos/)\\n\\n## Important Dates\\n\\n*   Abstract submission: October 9, 2024\\n*   Paper submission: October 16, 2024\\n*   Rebuttal period: Nov 27 – Dec 4, 2024\\n*   Author notification: Dec 23, 2024\\n*   Camera-ready paper: Feb 7, 2025, Feb 21, 2025\\n*   Conference: May 19-23, 2025\\n\\n**NOTE:** All deadlines are at the end of the specified day, Anywhere on Earth (UTC-12).\\n\\n## Organizing Committee\\n\\n*   **AAMAS 2025 General Chairs:**\\n    *   Ann Nowé (Vrije Universiteit Brussel, Belgium)\\n    *   Sanmay Das (George Mason University, USA)\\n*   **AAMAS 2025 Program Chairs:**\\n    *   Amal El Fallah Seghrouchni (Sorbonne University, France)\\n    *   Yevgeniy Vorobeychik (Washington University in Saint Louis, USA)\\n*   **AAMAS 2025 Workflow Chairs:**\\n    *   Tao Zhang (Washington University in Saint Louis, USA)\\n    *   Ayan Mukhopahdyay (Vanderbilt University, USA)\\n*   **AAMAS 2025 Local Chair:**\\n    *   Michael Wellman (University of Michigan, Ann Arbor, USA)\\n\\nIf you have additional questions, please contact the program and workflow chairs using [aamas2025pcchairs@googlegroups.com](mailto:aamas2025pcchairs@googlegroups.com).         NaN\n",
      "4  req-conf-1751535138073-e77y3               NaN  International Conference Abstract State Machines, Alloy, B, TLA, VDM, and Z     ABZ               https://abz-conf.org/site/2025/                                                    https://abz-conf.org/site/2025/calls                 https://abz-conf.org/site/2025/importantdates  Conference dates: June 11 - 13, 2025\\nYear: 2025\\nLocation: Düsseldorf, Germany\\nCity-State-Province: Düsseldorf\\nCountry: Germany\\nContinent: Europe\\nType: Offline\\nSubmission Date.(Case Study Track) Abstract submission: March 1, 2025\\nSubmission Date.(Case Study Track) Paper submission: March 2, 2025\\nSubmission Date.(Main Track) Abstract submission: March 1, 2025\\nSubmission Date.(Main Track) Paper submission (including research/short/industry/journal-first papers): March 2, 2025\\nSubmission Date.(Doctoral Symposium) Paper submission: March 2, 2025\\nNotification Date.(Case Study Track) Notification: March 29, 2025\\nNotification Date.(Main Track) Notification: March 29, 2025\\nNotification Date.(Doctoral Symposium) Notification: March 29, 2025\\nCamera-ready Date.(Case Study Track) Final version: April 9, 2025\\nCamera-ready Date.(Main Track) Final version: April 9, 2025\\nCamera-ready Date.(Doctoral Symposium) Final version: April 9, 2025\\nOther Date.Workshops and Tutorials Düsseldorf, Germany: June 10, 2025\\nTopics: Rigorous State Based Methods, Abstract State Machines, Alloy, B, TLA, VDM, Z, Formal Methods     June 11 - 13, 2025  2025.0                             Düsseldorf, Germany         Düsseldorf        Germany         Europe  Offline  {\"(Case Study Track) Abstract submission\":\"March 1, 2025\",\"(Case Study Track) Paper submission\":\"March 2, 2025\",\"(Main Track) Abstract submission\":\"March 1, 2025\",\"(Main Track) Paper submission (including research/short/industry/journal-first papers)\":\"March 2, 2025\",\"(Doctoral Symposium) Paper submission\":\"March 2, 2025\"}  {\"(Case Study Track) Notification\":\"March 29, 2025\",\"(Main Track) Notification\":\"March 29, 2025\",\"(Doctoral Symposium) Notification\":\"March 29, 2025\"}  {\"(Case Study Track) Final version\":\"April 9, 2025\",\"(Main Track) Final version\":\"April 9, 2025\",\"(Doctoral Symposium) Final version\":\"April 9, 2025\"}                                         {}                                                                                                {\"Workshops and Tutorials Düsseldorf, Germany\":\"June 10, 2025\"}                                                                                                                                                                                                                                                                                                                                            Rigorous State Based Methods, Abstract State Machines, Alloy, B, TLA, VDM, Z, Formal Methods     No publisher                                                                                                                                                                                                                                                    The 11th International Conference on Rigorous State Based Methods (ABZ 2025) will be held in Düsseldorf, Germany, from June 10 to June 13, 2025. The conference focuses on the cross-fertilization of state-based and machine-based formal methods such as Abstract State Machines (ASM), Alloy, B, TLA, VDM, and Z.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # ABZ 2025: Calls for Contributions\\n\\nThe ABZ conference is dedicated to the cross-fertilization of state-based and machine-based formal methods, like Abstract State Machines (ASM), Alloy, B, TLA, VDM and Z. The aim is to foster a vital exchange of knowledge and experience among the research communities around different formal methods.\\n\\n## Submission Channels\\n\\nContributions can be submitted via EasyChair: [https://easychair.org/conferences/?conf=abz2025](https://easychair.org/conferences/?conf=abz2025)\\n\\n## Tracks and Deadlines\\n\\n*   **Workshops/Tutorials:**\\n    *   Please contact the organizers directly as soon as possible for workshop or tutorial proposals. There is no special call for workshops or tutorials this year.\\n\\n*   **Case Study Track:**\\n    *   Abstract submission: **March 1, 2025 AOE**\\n    *   Paper submission: **March 2, 2025 AOE**\\n    *   Notification: **March 29, 2025**\\n    *   Final version: **April 9, 2025**\\n\\n*   **Main Track** (including research/short/industry/journal-first papers):\\n    *   Abstract submission: **March 1, 2025 AOE**\\n    *   Paper submission: **March 2, 2025 AOE**\\n    *   Notification: **March 29, 2025**\\n    *   Final version: **April 9, 2025**\\n\\n*   **Doctoral Symposium:**\\n    *   Paper submission: **March 2, 2025 AOE**\\n    *   Notification: **March 29, 2025**\\n    *   Final version: **April 9, 2025**\\n\\n## Conference Dates\\n\\n*   Workshops and Tutorials: **June 10, 2025** (Düsseldorf, Germany)\\n*   ABZ 2025 Conference: **June 11-13, 2025** (Düsseldorf, Germany)\\n\\n## Open Access / Open Choice\\n\\nAuthors interested in contributing to ABZ 2025 in Open Access or Open Choice should refer to the corresponding Springer webpage.\\n\\n## Contact Us\\n\\n[abz2025@hhu.de](mailto:abz2025@hhu.de)         NaN\n",
      "Tổng số dòng trong full.csv: 864\n",
      "\n",
      "Đang đọc file CORE_2023.csv từ: ./src/conference/csv/CORE_2023.csv\n",
      "Đã đọc CORE_2023.csv thành công. 5 dòng đầu tiên (cột 1 là title, cột 2 là acronym):\n",
      "      0                                                                                                                                                               title acronym         3   4    5     6     7       8\n",
      "0  1772                                                                                                       Symposium of Asian Association for Algorithms and Computation    AAAC  CORE2023   C   No  4613   NaN     NaN\n",
      "1  1629                                                                                         National Conference of the American Association for Artificial Intelligence    AAAI  CORE2023  A*  Yes  4602  4603  4611.0\n",
      "2   356                                                                                                     Conference on Algorithmic Aspects in Information and Management    AAIM  CORE2023   C   No  4613   NaN     NaN\n",
      "3   922  International Joint Conference on Autonomous Agents and Multiagent Systems (previously the International Conference on Multiagent Systems, ICMAS, changed in 2000)   AAMAS  CORE2023  A*  Yes  4602   NaN     NaN\n",
      "4   882             International Conference Abstract State Machines, Alloy, B, TLA, VDM, and Z (Previously International Conference of B and Z Users, ZB, changed in 2008)     ABZ  CORE2023   C   No  4612  4613     NaN\n",
      "Tổng số dòng trong CORE_2023.csv: 955\n",
      "\n",
      "--- BƯỚC 2: CHUẨN HÓA DỮ LIỆU ---\n",
      "Áp dụng hàm normalize_text cho cột 'title' và 'acronym' của cả hai DataFrame.\n",
      "\n",
      "5 dòng đầu tiên của full.csv sau khi chuẩn hóa:\n",
      "                                                                         title                                                             normalized_title acronym normalized_acronym\n",
      "0                Symposium of Asian Association for Algorithms and Computation                Symposium of Asian Association for Algorithms and Computation    AAAC               AAAC\n",
      "1  National Conference of the American Association for Artificial Intelligence  National Conference of the American Association for Artificial Intelligence    AAAI               AAAI\n",
      "2              Conference on Algorithmic Aspects in Information and Management              Conference on Algorithmic Aspects in Information and Management    AAIM               AAIM\n",
      "3   International Joint Conference on Autonomous Agents and Multiagent Systems   International Joint Conference on Autonomous Agents and Multiagent Systems   AAMAS              AAMAS\n",
      "4  International Conference Abstract State Machines, Alloy, B, TLA, VDM, and Z  International Conference Abstract State Machines, Alloy, B, TLA, VDM, and Z     ABZ                ABZ\n",
      "\n",
      "5 dòng đầu tiên của CORE_2023.csv sau khi chuẩn hóa:\n",
      "                                                                                                                                                                title                                                             normalized_title acronym normalized_acronym\n",
      "0                                                                                                       Symposium of Asian Association for Algorithms and Computation                Symposium of Asian Association for Algorithms and Computation    AAAC               AAAC\n",
      "1                                                                                         National Conference of the American Association for Artificial Intelligence  National Conference of the American Association for Artificial Intelligence    AAAI               AAAI\n",
      "2                                                                                                     Conference on Algorithmic Aspects in Information and Management              Conference on Algorithmic Aspects in Information and Management    AAIM               AAIM\n",
      "3  International Joint Conference on Autonomous Agents and Multiagent Systems (previously the International Conference on Multiagent Systems, ICMAS, changed in 2000)   International Joint Conference on Autonomous Agents and Multiagent Systems   AAMAS              AAMAS\n",
      "4             International Conference Abstract State Machines, Alloy, B, TLA, VDM, and Z (Previously International Conference of B and Z Users, ZB, changed in 2008)  International Conference Abstract State Machines, Alloy, B, TLA, VDM, and Z     ABZ                ABZ\n",
      "\n",
      "--- BƯỚC 3: TẠO KHÓA SO SÁNH (KEY) ---\n",
      "Tạo cột 'key' từ cặp (normalized_title, normalized_acronym) cho cả hai DataFrame.\n",
      "\n",
      "5 dòng đầu tiên của full.csv với cột 'key':\n",
      "                                                                         title acronym                                                                                  key\n",
      "0                Symposium of Asian Association for Algorithms and Computation    AAAC                (Symposium of Asian Association for Algorithms and Computation, AAAC)\n",
      "1  National Conference of the American Association for Artificial Intelligence    AAAI  (National Conference of the American Association for Artificial Intelligence, AAAI)\n",
      "2              Conference on Algorithmic Aspects in Information and Management    AAIM              (Conference on Algorithmic Aspects in Information and Management, AAIM)\n",
      "3   International Joint Conference on Autonomous Agents and Multiagent Systems   AAMAS  (International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS)\n",
      "4  International Conference Abstract State Machines, Alloy, B, TLA, VDM, and Z     ABZ   (International Conference Abstract State Machines, Alloy, B, TLA, VDM, and Z, ABZ)\n",
      "\n",
      "5 dòng đầu tiên của CORE_2023.csv với cột 'key':\n",
      "                                                                                                                                                                title acronym                                                                                  key\n",
      "0                                                                                                       Symposium of Asian Association for Algorithms and Computation    AAAC                (Symposium of Asian Association for Algorithms and Computation, AAAC)\n",
      "1                                                                                         National Conference of the American Association for Artificial Intelligence    AAAI  (National Conference of the American Association for Artificial Intelligence, AAAI)\n",
      "2                                                                                                     Conference on Algorithmic Aspects in Information and Management    AAIM              (Conference on Algorithmic Aspects in Information and Management, AAIM)\n",
      "3  International Joint Conference on Autonomous Agents and Multiagent Systems (previously the International Conference on Multiagent Systems, ICMAS, changed in 2000)   AAMAS  (International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS)\n",
      "4             International Conference Abstract State Machines, Alloy, B, TLA, VDM, and Z (Previously International Conference of B and Z Users, ZB, changed in 2008)     ABZ   (International Conference Abstract State Machines, Alloy, B, TLA, VDM, and Z, ABZ)\n",
      "\n",
      "--- BƯỚC 4: ĐỌC VÀ XỬ LÝ DANH SÁCH NON-LINK VÀ SKIP ---\n",
      "Đã đọc 69 key non-link từ ./non_link.txt.\n",
      "Đã đọc 9 key skip từ ./skip.txt.\n",
      "\n",
      "--- BƯỚC 5: LỌC BỎ CÁC CONFERENCE TRONG full.csv CÓ TRONG NON-LINK VÀ SKIP ---\n",
      "Số dòng ban đầu trong full.csv: 864\n",
      "Số dòng trong full.csv sau khi loại bỏ non-link: 863 (Đã loại bỏ 1 dòng).\n",
      "Số dòng trong full.csv sau khi loại bỏ skip: 860 (Đã loại bỏ 4 dòng tổng cộng).\n",
      "\n",
      "--- BƯỚC 6: TẠO TẬP HỢP CÁC KEY TỪ full.csv ĐÃ LỌC ---\n",
      "Tổng số key duy nhất trong full.csv (sau khi lọc non-link và skip): 860\n",
      "Một vài key mẫu từ full.csv (đã chuẩn hóa và lọc):\n",
      "  - ('International Symposium on Applied Computational Intelligence and Informatics', 'SACI')\n",
      "  - ('ICT in Education, Research, and Industrial Applications', 'ICTERI')\n",
      "  - ('European Association for Computational Linguistics', 'EACL')\n",
      "  - ('IFIP International Symposium on Computing Performance, Modelling, Measurement and Evaluation', 'PERFORMANCE')\n",
      "  - ('Conference on Visualization and Data Analysis', 'VDA')\n",
      "\n",
      "--- BƯỚC 7: LỌC CÁC DÒNG CHỈ CÓ TRONG CORE_2023.csv ---\n",
      "Lọc df_core để tìm các dòng mà 'key' của chúng KHÔNG có trong tập hợp key của full.csv đã lọc.\n",
      "\n",
      "Tìm thấy 95 dòng chỉ có trong CORE_2023.csv.\n",
      "5 dòng đầu tiên của kết quả (các dòng chỉ có trong CORE_2023.csv):\n",
      "       0                                                                                                                                                 title acronym         3               4    5     6     7   8                                                     normalized_title normalized_acronym                                                                          key\n",
      "7   2295                                                                                   International Workshop on Algebraic and Combinatorial Coding Theory    ACCT  CORE2023        National  Yes  4613  4604 NaN  International Workshop on Algebraic and Combinatorial Coding Theory               ACCT  (International Workshop on Algebraic and Combinatorial Coding Theory, ACCT)\n",
      "8    167                                                                                                                   Asian Conference on Computer Vision    ACCV  CORE2023               B   No  4603   NaN NaN                                  Asian Conference on Computer Vision               ACCV                                  (Asian Conference on Computer Vision, ACCV)\n",
      "9     23  ACM International Conference on Advances in Computer Entertainment (merged with DIMEA, Digital Interactive Media in Entertainment and Arts, in 2009)     ACE  CORE2023               C   No  4607  4608 NaN   ACM International Conference on Advances in Computer Entertainment                ACE    (ACM International Conference on Advances in Computer Entertainment, ACE)\n",
      "28  1954                                                                                                             Australasian Document Computing Symposium    ADCS  CORE2023  Australasian B  Yes  4605   NaN NaN                            Australasian Document Computing Symposium               ADCS                            (Australasian Document Computing Symposium, ADCS)\n",
      "33   261                                                                                                    Australian Institute of Computer Ethics Conference    AICE  CORE2023  Australasian C  Yes  4608   NaN NaN                   Australian Institute of Computer Ethics Conference               AICE                   (Australian Institute of Computer Ethics Conference, AICE)\n",
      "\n",
      "--- BƯỚC 8: DỌN DẸP CÁC CỘT TẠM THỜI ---\n",
      "Đã xóa các cột 'normalized_title', 'normalized_acronym', 'key' khỏi DataFrame kết quả.\n",
      "Quá trình so sánh hoàn tất.\n",
      "\n",
      "--- KẾT QUẢ CUỐI CÙNG ---\n",
      "Tổng số dòng duy nhất trong CORE_2023.csv: 95\n",
      "Kết quả đã được lưu vào file: ./src/conference/evaluate/recrawl_all_core.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Chuẩn hóa văn bản bằng cách loại bỏ nội dung bên trong cặp dấu ngoặc đơn\n",
    "    và chính cặp dấu ngoặc đơn, đồng thời đảm bảo chỉ có một khoảng trắng\n",
    "    giữa các từ.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    cleaned_text = re.sub(r'\\s*\\(.*?\\)\\s*', ' ', str(text))\n",
    "    final_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return final_text\n",
    "\n",
    "def find_unique_in_core_detailed(full_csv_path, core_csv_path, non_link_acronyms_path, skip_acronyms_path):\n",
    "    \"\"\"\n",
    "    Tìm các dòng chỉ có trong file CORE_2023.csv dựa trên cặp (title, acronym)\n",
    "    sau khi chuẩn hóa, và in ra các bước chi tiết.\n",
    "    Đồng thời, lọc bỏ các conference trong full.csv có trong non_link.txt và skip.txt.\n",
    "\n",
    "    Args:\n",
    "        full_csv_path (str): Đường dẫn đến file full.csv.\n",
    "        core_csv_path (str): Đường dẫn đến file CORE_2023.csv.\n",
    "        non_link_acronyms_path (str): Đường dẫn đến file TXT chứa các key non-link.\n",
    "        skip_acronyms_path (str): Đường dẫn đến file TXT chứa các key cần bỏ qua (skip).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame chứa các dòng chỉ có trong CORE_2023.csv\n",
    "                          sau khi đã lọc bỏ các conference trong non_link và skip.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- BƯỚC 1: ĐỌC DỮ LIỆU ---\")\n",
    "    print(f\"Đang đọc file full.csv từ: {full_csv_path}\")\n",
    "    try:\n",
    "        df_full = pd.read_csv(full_csv_path)\n",
    "        if 'title' not in df_full.columns or 'acronym' not in df_full.columns:\n",
    "            raise ValueError(\"File full.csv phải có các cột 'title' và 'acronym'.\")\n",
    "        print(\"Đã đọc full.csv thành công. 5 dòng đầu tiên:\")\n",
    "        print(df_full.head().to_string())\n",
    "        print(f\"Tổng số dòng trong full.csv: {len(df_full)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file full.csv tại đường dẫn: {full_csv_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc full.csv: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nĐang đọc file CORE_2023.csv từ: {core_csv_path}\")\n",
    "    try:\n",
    "        df_core = pd.read_csv(core_csv_path, header=None)\n",
    "        if df_core.shape[1] < 3:\n",
    "            raise ValueError(\"File CORE_2023.csv phải có ít nhất 3 cột để lấy title và acronym.\")\n",
    "        df_core.rename(columns={1: 'title', 2: 'acronym'}, inplace=True)\n",
    "        print(\"Đã đọc CORE_2023.csv thành công. 5 dòng đầu tiên (cột 1 là title, cột 2 là acronym):\")\n",
    "        print(df_core.head().to_string())\n",
    "        print(f\"Tổng số dòng trong CORE_2023.csv: {len(df_core)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file CORE_2023.csv tại đường dẫn: {core_csv_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc CORE_2023.csv: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"\\n--- BƯỚC 2: CHUẨN HÓA DỮ LIỆU ---\")\n",
    "    print(\"Áp dụng hàm normalize_text cho cột 'title' và 'acronym' của cả hai DataFrame.\")\n",
    "\n",
    "    df_full['normalized_title'] = df_full['title'].apply(normalize_text)\n",
    "    df_full['normalized_acronym'] = df_full['acronym'].apply(normalize_text)\n",
    "    print(\"\\n5 dòng đầu tiên của full.csv sau khi chuẩn hóa:\")\n",
    "    print(df_full[['title', 'normalized_title', 'acronym', 'normalized_acronym']].head().to_string())\n",
    "\n",
    "    df_core['normalized_title'] = df_core['title'].apply(normalize_text)\n",
    "    df_core['normalized_acronym'] = df_core['acronym'].apply(normalize_text)\n",
    "    print(\"\\n5 dòng đầu tiên của CORE_2023.csv sau khi chuẩn hóa:\")\n",
    "    print(df_core[['title', 'normalized_title', 'acronym', 'normalized_acronym']].head().to_string())\n",
    "\n",
    "    print(\"\\n--- BƯỚC 3: TẠO KHÓA SO SÁNH (KEY) ---\")\n",
    "    print(\"Tạo cột 'key' từ cặp (normalized_title, normalized_acronym) cho cả hai DataFrame.\")\n",
    "    df_full['key'] = list(zip(df_full['normalized_title'], df_full['normalized_acronym']))\n",
    "    df_core['key'] = list(zip(df_core['normalized_title'], df_core['normalized_acronym']))\n",
    "\n",
    "    print(\"\\n5 dòng đầu tiên của full.csv với cột 'key':\")\n",
    "    print(df_full[['title', 'acronym', 'key']].head().to_string())\n",
    "    print(\"\\n5 dòng đầu tiên của CORE_2023.csv với cột 'key':\")\n",
    "    print(df_core[['title', 'acronym', 'key']].head().to_string())\n",
    "\n",
    "    print(\"\\n--- BƯỚC 4: ĐỌC VÀ XỬ LÝ DANH SÁCH NON-LINK VÀ SKIP ---\")\n",
    "    non_link_keys = set()\n",
    "    try:\n",
    "        with open(non_link_acronyms_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 3:\n",
    "                    title = normalize_text(parts[1])\n",
    "                    acronym = normalize_text(parts[2])\n",
    "                    if title and acronym:\n",
    "                        non_link_keys.add((title, acronym))\n",
    "        print(f\"Đã đọc {len(non_link_keys)} key non-link từ {non_link_acronyms_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file non-link acronyms tại đường dẫn: {non_link_acronyms_path}. Bỏ qua bước lọc này.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file non-link acronyms: {e}. Bỏ qua bước lọc này.\")\n",
    "\n",
    "    skip_keys = set()\n",
    "    try:\n",
    "        with open(skip_acronyms_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 3:\n",
    "                    title = normalize_text(parts[1])\n",
    "                    acronym = normalize_text(parts[2])\n",
    "                    if title and acronym:\n",
    "                        skip_keys.add((title, acronym))\n",
    "        print(f\"Đã đọc {len(skip_keys)} key skip từ {skip_acronyms_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file skip acronyms tại đường dẫn: {skip_acronyms_path}. Bỏ qua bước lọc này.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file skip acronyms: {e}. Bỏ qua bước lọc này.\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 5: LỌC BỎ CÁC CONFERENCE TRONG full.csv CÓ TRONG NON-LINK VÀ SKIP ---\")\n",
    "    initial_full_count = len(df_full)\n",
    "    print(f\"Số dòng ban đầu trong full.csv: {initial_full_count}\")\n",
    "\n",
    "    # Lọc bỏ các conference có key trong danh sách non-link\n",
    "    df_full_filtered = df_full[~df_full['key'].isin(non_link_keys)].copy()\n",
    "    print(f\"Số dòng trong full.csv sau khi loại bỏ non-link: {len(df_full_filtered)} (Đã loại bỏ {initial_full_count - len(df_full_filtered)} dòng).\")\n",
    "\n",
    "    # Lọc bỏ các conference có key trong danh sách skip\n",
    "    df_full_filtered = df_full_filtered[~df_full_filtered['key'].isin(skip_keys)].copy()\n",
    "    print(f\"Số dòng trong full.csv sau khi loại bỏ skip: {len(df_full_filtered)} (Đã loại bỏ {initial_full_count - len(df_full_filtered)} dòng tổng cộng).\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 6: TẠO TẬP HỢP CÁC KEY TỪ full.csv ĐÃ LỌC ---\")\n",
    "    keys_in_full_filtered = set(df_full_filtered['key'])\n",
    "    print(f\"Tổng số key duy nhất trong full.csv (sau khi lọc non-link và skip): {len(keys_in_full_filtered)}\")\n",
    "    # In ra một vài key mẫu để kiểm tra\n",
    "    print(\"Một vài key mẫu từ full.csv (đã chuẩn hóa và lọc):\")\n",
    "    for i, key in enumerate(list(keys_in_full_filtered)[:5]):\n",
    "        print(f\"  - {key}\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 7: LỌC CÁC DÒNG CHỈ CÓ TRONG CORE_2023.csv ---\")\n",
    "    print(\"Lọc df_core để tìm các dòng mà 'key' của chúng KHÔNG có trong tập hợp key của full.csv đã lọc.\")\n",
    "    unique_in_core_df = df_core[~df_core['key'].isin(keys_in_full_filtered)].copy()\n",
    "\n",
    "    print(f\"\\nTìm thấy {len(unique_in_core_df)} dòng chỉ có trong CORE_2023.csv.\")\n",
    "    if not unique_in_core_df.empty:\n",
    "        print(\"5 dòng đầu tiên của kết quả (các dòng chỉ có trong CORE_2023.csv):\")\n",
    "        print(unique_in_core_df.head().to_string())\n",
    "    else:\n",
    "        print(\"Không tìm thấy dòng nào chỉ có trong CORE_2023.csv.\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 8: DỌN DẸP CÁC CỘT TẠM THỜI ---\")\n",
    "    # Xóa các cột tạm thời đã tạo\n",
    "    unique_in_core_df.drop(columns=['normalized_title', 'normalized_acronym', 'key'], inplace=True, errors='ignore')\n",
    "\n",
    "    print(\"Đã xóa các cột 'normalized_title', 'normalized_acronym', 'key' khỏi DataFrame kết quả.\")\n",
    "    print(\"Quá trình so sánh hoàn tất.\")\n",
    "\n",
    "    return unique_in_core_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Cấu hình đường dẫn file của bạn ---\n",
    "    FULL_CSV_FILE = './src/conference/evaluate/full.csv'\n",
    "    CORE_CSV_FILE = './src/conference/csv/CORE_2023.csv'\n",
    "    NON_LINK_ACRONYMS_FILE = './non_link.txt'\n",
    "    SKIP_ACRONYMS_FILE = './skip.txt'\n",
    "    OUTPUT_CSV_FILE = './src/conference/evaluate/recrawl_all_core.csv'\n",
    "\n",
    "    # Gọi hàm để tìm các dòng duy nhất với chi tiết\n",
    "    result_df = find_unique_in_core_detailed(FULL_CSV_FILE, CORE_CSV_FILE, NON_LINK_ACRONYMS_FILE, SKIP_ACRONYMS_FILE)\n",
    "\n",
    "    if not result_df.empty:\n",
    "        print(f\"\\n--- KẾT QUẢ CUỐI CÙNG ---\")\n",
    "        print(f\"Tổng số dòng duy nhất trong CORE_2023.csv: {len(result_df)}\")\n",
    "        print(f\"Kết quả đã được lưu vào file: {OUTPUT_CSV_FILE}\")\n",
    "        result_df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "    else:\n",
    "        print(\"\\n--- KẾT QUẢ CUỐI CÙNG ---\")\n",
    "        print(\"Không tìm thấy dòng nào chỉ có trong CORE_2023.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BƯỚC 1: ĐỌC DANH SÁCH CẦN RECRAWL TỪ recrawl_all_core.csv ---\n",
      "Đã đọc 95 dòng từ ./src/conference/evaluate/recrawl_all_core.csv.\n",
      "\n",
      "--- BƯỚC 2: CHUẨN HÓA VÀ TẠO KEY CHO DANH SÁCH RECRAWL ---\n",
      "Tổng số key duy nhất trong recrawl_all_core: 95\n",
      "\n",
      "--- BƯỚC 3: ĐỌC VÀ TỔNG HỢP CÁC CONFERENCE ĐANG ĐƯỢC CRAWL ---\n",
      "\n",
      "Tổng số key duy nhất từ tất cả các file đang crawl: 0\n",
      "\n",
      "--- BƯỚC 4: ĐỌC DANH SÁCH ACRONYM NON-LINK VÀ SKIP (Đã điều chỉnh cho định dạng TXT mới) ---\n",
      "Đã đọc 69 key non-link từ ./non_link.txt.\n",
      "Đã đọc 9 key skip từ ./skip.txt.\n",
      "\n",
      "--- BƯỚC 5: LỌC DANH SÁCH RECRAWL CUỐI CÙNG ---\n",
      "Tìm các conference trong recrawl_all_core mà KHÔNG có trong danh sách đang crawl,\n",
      "KHÔNG có key trong danh sách non-link, VÀ KHÔNG có key trong danh sách skip.\n",
      "Số dòng ban đầu trong danh sách recrawl: 95\n",
      "Số dòng sau khi loại bỏ các conference đang crawl: 95 (Đã loại bỏ 0 dòng).\n",
      "Số dòng sau khi loại bỏ các conference non-link: 32 (Đã loại bỏ 63 dòng).\n",
      "Số dòng sau khi loại bỏ các conference skip: 23 (Đã loại bỏ 9 dòng).\n",
      "\n",
      "Tìm thấy 23 conference cần recrawl cuối cùng.\n",
      "\n",
      "--- BƯỚC 6: DỌN DẸP VÀ LƯU KẾT QUẢ ---\n",
      "5 dòng đầu tiên của danh sách recrawl cuối cùng:\n",
      "       0                                                                                                                                                 title acronym         3         4    5     6       7       8\n",
      "2     23  ACM International Conference on Advances in Computer Entertainment (merged with DIMEA, Digital Interactive Media in Entertainment and Arts, in 2009)     ACE  CORE2023         C   No  4607  4608.0     NaN\n",
      "5    175                                                                                                                Asia-Pacific Bioinformatics Conference    APBC  CORE2023         B   No  4601     NaN     NaN\n",
      "14  2308                                                                                                                          Conference on Robot Learning    CoRL  CORE2023  Unranked  Yes  4602  4611.0  4603.0\n",
      "15    13                                                                                                 ACM Conference on Computer Supported Cooperative Work    CSCW  CORE2023         A   No  4608     NaN     NaN\n",
      "19   801                                                                   IFIP International Conference on Distributed Applications and Interoperable Systems    DAIS  CORE2023         C  Yes  4606     NaN     NaN\n",
      "\n",
      "Kết quả đã được lưu vào file: ./src/conference/evaluate/final_recrawl_list.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sử dụng lại hàm normalize_text đã được cải tiến\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Chuẩn hóa văn bản bằng cách loại bỏ nội dung bên trong cặp dấu ngoặc đơn\n",
    "    và chính cặp dấu ngoặc đơn, đồng thời đảm bảo chỉ có một khoảng trắng\n",
    "    giữa các từ.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    cleaned_text = re.sub(r'\\s*\\(.*?\\)\\s*', ' ', str(text))\n",
    "    final_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return final_text\n",
    "\n",
    "def filter_recrawl_list_with_non_links(recrawl_all_core_path, crawling_files_paths, non_link_acronyms_path, skip_acronyms_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Lọc danh sách recrawl_all_core.csv bằng cách loại bỏ các conference\n",
    "    đang được crawl (dựa trên 3 file CSV khác), các conference\n",
    "    có acronym trong danh sách non-link từ file TXT, và các conference\n",
    "    có acronym trong danh sách skip từ file TXT.\n",
    "\n",
    "    Args:\n",
    "        recrawl_all_core_path (str): Đường dẫn đến file recrawl_all_core.csv.\n",
    "        crawling_files_paths (list): Danh sách các đường dẫn đến các file CSV\n",
    "                                      chứa danh sách conference đang được crawl.\n",
    "        non_link_acronyms_path (str): Đường dẫn đến file TXT chứa các acronym non-link.\n",
    "        skip_acronyms_path (str): Đường dẫn đến file TXT chứa các acronym cần bỏ qua (skip).\n",
    "        output_csv_path (str): Đường dẫn để lưu file CSV kết quả cuối cùng.\n",
    "    \"\"\"\n",
    "    print(\"--- BƯỚC 1: ĐỌC DANH SÁCH CẦN RECRAWL TỪ recrawl_all_core.csv ---\")\n",
    "    try:\n",
    "        # Đọc recrawl_all_core.csv - giả định nó đã có cột 'title' và 'acronym'\n",
    "        df_recrawl = pd.read_csv(recrawl_all_core_path)\n",
    "        if 'title' not in df_recrawl.columns or 'acronym' not in df_recrawl.columns:\n",
    "            raise ValueError(f\"File {recrawl_all_core_path} phải có các cột 'title' và 'acronym'.\")\n",
    "        print(f\"Đã đọc {len(df_recrawl)} dòng từ {recrawl_all_core_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file: {recrawl_all_core_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc {recrawl_all_core_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- BƯỚC 2: CHUẨN HÓA VÀ TẠO KEY CHO DANH SÁCH RECRAWL ---\")\n",
    "    df_recrawl['normalized_title'] = df_recrawl['title'].apply(normalize_text)\n",
    "    df_recrawl['normalized_acronym'] = df_recrawl['acronym'].apply(normalize_text)\n",
    "    df_recrawl['key'] = list(zip(df_recrawl['normalized_title'], df_recrawl['normalized_acronym']))\n",
    "    recrawl_keys = set(df_recrawl['key'])\n",
    "    print(f\"Tổng số key duy nhất trong recrawl_all_core: {len(recrawl_keys)}\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 3: ĐỌC VÀ TỔNG HỢP CÁC CONFERENCE ĐANG ĐƯỢC CRAWL ---\")\n",
    "    all_crawling_keys = set()\n",
    "    for i, file_path in enumerate(crawling_files_paths):\n",
    "        print(f\"Đang đọc file crawling {i+1}/{len(crawling_files_paths)}: {file_path}\")\n",
    "        try:\n",
    "            # Đối với các file crawling, nếu chúng cũng là CSV với cột 'title' và 'acronym'\n",
    "            df_crawling = pd.read_csv(file_path)\n",
    "            if 'title' not in df_crawling.columns or 'acronym' not in df_crawling.columns:\n",
    "                print(f\"Cảnh báo: File {file_path} không có cột 'title' hoặc 'acronym'. Bỏ qua file này.\")\n",
    "                continue\n",
    "\n",
    "            df_crawling['normalized_title'] = df_crawling['title'].apply(normalize_text)\n",
    "            df_crawling['normalized_acronym'] = df_crawling['acronym'].apply(normalize_text)\n",
    "            df_crawling['key'] = list(zip(df_crawling['normalized_title'], df_crawling['normalized_acronym']))\n",
    "\n",
    "            all_crawling_keys.update(set(df_crawling['key']))\n",
    "            print(f\"  Đã thêm {len(set(df_crawling['key']))} key từ {file_path}. Tổng số key đang crawl: {len(all_crawling_keys)}\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Lỗi: Không tìm thấy file crawling tại đường dẫn: {file_path}. Bỏ qua file này.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi đọc file crawling {file_path}: {e}. Bỏ qua file này.\")\n",
    "\n",
    "    print(f\"\\nTổng số key duy nhất từ tất cả các file đang crawl: {len(all_crawling_keys)}\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 4: ĐỌC DANH SÁCH ACRONYM NON-LINK VÀ SKIP (Đã điều chỉnh cho định dạng TXT mới) ---\")\n",
    "    non_link_keys = set() # Thay đổi từ set acronyms thành set keys\n",
    "    try:\n",
    "        with open(non_link_acronyms_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',') # Tách dòng bằng dấu phẩy\n",
    "                if len(parts) >= 3: # Đảm bảo có đủ các cột\n",
    "                    title = normalize_text(parts[1]) # Cột 2 là title\n",
    "                    acronym = normalize_text(parts[2]) # Cột 3 là acronym\n",
    "                    if title and acronym: # Đảm bảo không thêm key trống\n",
    "                        non_link_keys.add((title, acronym))\n",
    "        print(f\"Đã đọc {len(non_link_keys)} key non-link từ {non_link_acronyms_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file non-link acronyms tại đường dẫn: {non_link_acronyms_path}. Bỏ qua bước lọc này.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file non-link acronyms: {e}. Bỏ qua bước lọc này.\")\n",
    "\n",
    "    skip_keys = set() # Thay đổi từ set acronyms thành set keys\n",
    "    try:\n",
    "        with open(skip_acronyms_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',') # Tách dòng bằng dấu phẩy\n",
    "                if len(parts) >= 3: # Đảm bảo có đủ các cột\n",
    "                    title = normalize_text(parts[1]) # Cột 2 là title\n",
    "                    acronym = normalize_text(parts[2]) # Cột 3 là acronym\n",
    "                    if title and acronym: # Đảm bảo không thêm key trống\n",
    "                        skip_keys.add((title, acronym))\n",
    "        print(f\"Đã đọc {len(skip_keys)} key skip từ {skip_acronyms_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file skip acronyms tại đường dẫn: {skip_acronyms_path}. Bỏ qua bước lọc này.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file skip acronyms: {e}. Bỏ qua bước lọc này.\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 5: LỌC DANH SÁCH RECRAWL CUỐI CÙNG ---\")\n",
    "    print(\"Tìm các conference trong recrawl_all_core mà KHÔNG có trong danh sách đang crawl,\")\n",
    "    print(\"KHÔNG có key trong danh sách non-link, VÀ KHÔNG có key trong danh sách skip.\")\n",
    "\n",
    "    # Bắt đầu với danh sách recrawl_all_core\n",
    "    current_recrawl_df = df_recrawl.copy()\n",
    "    initial_count = len(current_recrawl_df)\n",
    "    print(f\"Số dòng ban đầu trong danh sách recrawl: {initial_count}\")\n",
    "\n",
    "    # Lọc bỏ các conference đang được crawl\n",
    "    filtered_by_crawling = current_recrawl_df[~current_recrawl_df['key'].isin(all_crawling_keys)].copy()\n",
    "    print(f\"Số dòng sau khi loại bỏ các conference đang crawl: {len(filtered_by_crawling)} (Đã loại bỏ {initial_count - len(filtered_by_crawling)} dòng).\")\n",
    "\n",
    "    # Lọc bỏ các conference có key trong danh sách non-link\n",
    "    # Bây giờ chúng ta lọc dựa trên 'key' thay vì 'normalized_acronym'\n",
    "    filtered_by_non_link = filtered_by_crawling[~filtered_by_crawling['key'].isin(non_link_keys)].copy()\n",
    "    print(f\"Số dòng sau khi loại bỏ các conference non-link: {len(filtered_by_non_link)} (Đã loại bỏ {len(filtered_by_crawling) - len(filtered_by_non_link)} dòng).\")\n",
    "\n",
    "    # Lọc bỏ các conference có key trong danh sách skip\n",
    "    # Tương tự, lọc dựa trên 'key'\n",
    "    final_recrawl_list_df = filtered_by_non_link[~filtered_by_non_link['key'].isin(skip_keys)].copy()\n",
    "    print(f\"Số dòng sau khi loại bỏ các conference skip: {len(final_recrawl_list_df)} (Đã loại bỏ {len(filtered_by_non_link) - len(final_recrawl_list_df)} dòng).\")\n",
    "\n",
    "    print(f\"\\nTìm thấy {len(final_recrawl_list_df)} conference cần recrawl cuối cùng.\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 6: DỌN DẸP VÀ LƯU KẾT QUẢ ---\")\n",
    "    # Xóa các cột tạm thời đã tạo\n",
    "    final_recrawl_list_df.drop(columns=['normalized_title', 'normalized_acronym', 'key'], inplace=True, errors='ignore')\n",
    "\n",
    "    if not final_recrawl_list_df.empty:\n",
    "        print(f\"5 dòng đầu tiên của danh sách recrawl cuối cùng:\")\n",
    "        print(final_recrawl_list_df.head().to_string())\n",
    "        print(f\"\\nKết quả đã được lưu vào file: {output_csv_path}\")\n",
    "        final_recrawl_list_df.to_csv(output_csv_path, index=False)\n",
    "    else:\n",
    "        print(\"Không tìm thấy conference nào cần recrawl cuối cùng sau khi lọc.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Cấu hình đường dẫn file của bạn ---\n",
    "    RECRAWL_ALL_CORE_FILE = './src/conference/evaluate/recrawl_all_core.csv'\n",
    "\n",
    "    # Danh sách các file CSV chứa conference đang được crawl\n",
    "    # Đảm bảo các file này có cột 'title' và 'acronym'\n",
    "    CRAWLING_FILES = [\n",
    "    #    './src/conference/evaluate/tri_tung_all_recrawl_final.csv'\n",
    "    ]\n",
    "\n",
    "    # Đường dẫn đến file TXT chứa các acronym non-link\n",
    "    # File này bây giờ được mong đợi có định dạng: id,title,acronym,...\n",
    "    NON_LINK_ACRONYMS_FILE = './non_link.txt'\n",
    "\n",
    "    # Đường dẫn đến file TXT chứa các acronym cần bỏ qua (skip)\n",
    "    # File này cũng được mong đợi có định dạng: id,title,acronym,...\n",
    "    SKIP_ACRONYMS_FILE = './skip.txt'\n",
    "\n",
    "    OUTPUT_FINAL_RECRAWL_FILE = './src/conference/evaluate/final_recrawl_list.csv'\n",
    "\n",
    "    # Gọi hàm để lọc danh sách recrawl\n",
    "    filter_recrawl_list_with_non_links(RECRAWL_ALL_CORE_FILE, CRAWLING_FILES, NON_LINK_ACRONYMS_FILE, SKIP_ACRONYMS_FILE, OUTPUT_FINAL_RECRAWL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đọc file full.csv từ: ./src/conference/evaluate/full.csv\n",
      "Đã đọc full.csv thành công.\n",
      "Đang đọc file unique_in_CORE_2023.csv từ: ./src/conference/evaluate/recrawl_all_core.csv\n",
      "Đã đọc unique_in_CORE_2023.csv thành công.\n",
      "Đang chọn các cột 'title' và 'acronym' từ cả hai DataFrame...\n",
      "Đang gộp hai DataFrame...\n",
      "Tổng số dòng sau khi gộp: 959\n",
      "Đang loại bỏ các dòng trùng lặp (nếu có)...\n",
      "Số dòng sau khi loại bỏ trùng lặp: 955 (Đã loại bỏ 4 dòng trùng lặp).\n",
      "Đang lưu kết quả vào file: ./src/conference/evaluate/merged_full_and_unique.csv\n",
      "Đã lưu file thành công.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_and_select_columns(full_csv_path, unique_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Gộp hai file CSV (full.csv và unique_in_CORE_2023.csv) và chỉ giữ lại\n",
    "    các cột 'title' và 'acronym', sau đó lưu vào một file CSV mới.\n",
    "\n",
    "    Args:\n",
    "        full_csv_path (str): Đường dẫn đến file full.csv.\n",
    "        unique_csv_path (str): Đường dẫn đến file unique_in_CORE_2023.csv.\n",
    "        output_csv_path (str): Đường dẫn để lưu file CSV kết quả.\n",
    "    \"\"\"\n",
    "    print(f\"Đang đọc file full.csv từ: {full_csv_path}\")\n",
    "    try:\n",
    "        df_full = pd.read_csv(full_csv_path)\n",
    "        # Đảm bảo các cột 'title' và 'acronym' tồn tại\n",
    "        if 'title' not in df_full.columns or 'acronym' not in df_full.columns:\n",
    "            raise ValueError(f\"File {full_csv_path} phải có các cột 'title' và 'acronym'.\")\n",
    "        print(\"Đã đọc full.csv thành công.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file full.csv tại đường dẫn: {full_csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc full.csv: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Đang đọc file unique_in_CORE_2023.csv từ: {unique_csv_path}\")\n",
    "    try:\n",
    "        df_unique = pd.read_csv(unique_csv_path)\n",
    "        # Đảm bảo các cột 'title' và 'acronym' tồn tại\n",
    "        if 'title' not in df_unique.columns or 'acronym' not in df_unique.columns:\n",
    "            raise ValueError(f\"File {unique_csv_path} phải có các cột 'title' và 'acronym'.\")\n",
    "        print(\"Đã đọc unique_in_CORE_2023.csv thành công.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file unique_in_CORE_2023.csv tại đường dẫn: {unique_csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc unique_in_CORE_2023.csv: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Đang chọn các cột 'title' và 'acronym' từ cả hai DataFrame...\")\n",
    "    # Chọn chỉ các cột 'title' và 'acronym' từ mỗi DataFrame\n",
    "    df_full_selected = df_full[['title', 'acronym']]\n",
    "    df_unique_selected = df_unique[['title', 'acronym']]\n",
    "\n",
    "    print(\"Đang gộp hai DataFrame...\")\n",
    "    # Gộp hai DataFrame theo chiều dọc (thêm hàng)\n",
    "    # ignore_index=True để reset index của DataFrame kết quả\n",
    "    merged_df = pd.concat([df_full_selected, df_unique_selected], ignore_index=True)\n",
    "\n",
    "    print(f\"Tổng số dòng sau khi gộp: {len(merged_df)}\")\n",
    "\n",
    "    # Tùy chọn: Xóa các dòng trùng lặp nếu bạn muốn một danh sách duy nhất\n",
    "    # Dòng trùng lặp ở đây có nghĩa là cả title và acronym đều giống hệt nhau\n",
    "    # Nếu bạn muốn giữ lại tất cả các dòng, kể cả trùng lặp, hãy bỏ qua bước này\n",
    "    print(\"Đang loại bỏ các dòng trùng lặp (nếu có)...\")\n",
    "    initial_rows = len(merged_df)\n",
    "    merged_df.drop_duplicates(inplace=True)\n",
    "    rows_after_dedup = len(merged_df)\n",
    "    print(f\"Số dòng sau khi loại bỏ trùng lặp: {rows_after_dedup} (Đã loại bỏ {initial_rows - rows_after_dedup} dòng trùng lặp).\")\n",
    "\n",
    "\n",
    "    print(f\"Đang lưu kết quả vào file: {output_csv_path}\")\n",
    "    try:\n",
    "        merged_df.to_csv(output_csv_path, index=False)\n",
    "        print(\"Đã lưu file thành công.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi lưu file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FULL_CSV_FILE = './src/conference/evaluate/full.csv'\n",
    "    UNIQUE_CSV_FILE = './src/conference/evaluate/recrawl_all_core.csv'\n",
    "    OUTPUT_MERGED_FILE = './src/conference/evaluate/merged_full_and_unique.csv'\n",
    "\n",
    "    # Gọi hàm để gộp các file\n",
    "    merge_and_select_columns(FULL_CSV_FILE, UNIQUE_CSV_FILE, OUTPUT_MERGED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đọc file CSV từ: ./src/conference/evaluate/full.csv\n",
      "Đã đọc 864 dòng từ ./src/conference/evaluate/full.csv.\n",
      "Đang tìm kiếm các dòng trùng lặp...\n",
      "Không tìm thấy dòng trùng lặp nào.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_and_export_duplicates(input_csv_path, output_csv_path, subset_columns=None):\n",
    "    \"\"\"\n",
    "    Tìm và xuất các dòng trùng lặp trong một file CSV.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): Đường dẫn đến file CSV đầu vào (ví dụ: full.csv).\n",
    "        output_csv_path (str): Đường dẫn để lưu file CSV chứa các dòng trùng lặp.\n",
    "        subset_columns (list, optional): Danh sách các tên cột để kiểm tra trùng lặp.\n",
    "                                         Nếu None, tất cả các cột sẽ được sử dụng.\n",
    "                                         Ví dụ: ['title', 'acronym']\n",
    "    \"\"\"\n",
    "    print(f\"Đang đọc file CSV từ: {input_csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        print(f\"Đã đọc {len(df)} dòng từ {input_csv_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file CSV tại đường dẫn: {input_csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Đang tìm kiếm các dòng trùng lặp...\")\n",
    "\n",
    "    # Tìm các dòng trùng lặp\n",
    "    # keep=False: Đánh dấu TẤT CẢ các lần xuất hiện của một dòng trùng lặp là True\n",
    "    #             (bao gồm cả lần xuất hiện đầu tiên).\n",
    "    #             Nếu muốn chỉ đánh dấu các bản sao (không bao gồm bản gốc đầu tiên),\n",
    "    #             sử dụng keep='first' hoặc keep='last'.\n",
    "    # subset: Các cột để kiểm tra trùng lặp. Nếu None, kiểm tra tất cả các cột.\n",
    "    duplicate_rows = df[df.duplicated(subset=subset_columns, keep=False)]\n",
    "\n",
    "    if not duplicate_rows.empty:\n",
    "        print(f\"Tìm thấy {len(duplicate_rows)} dòng trùng lặp.\")\n",
    "        print(f\"Đang lưu các dòng trùng lặp vào file: {output_csv_path}\")\n",
    "        try:\n",
    "            duplicate_rows.to_csv(output_csv_path, index=False)\n",
    "            print(\"Đã lưu file trùng lặp thành công.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi lưu file trùng lặp: {e}\")\n",
    "    else:\n",
    "        print(\"Không tìm thấy dòng trùng lặp nào.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Cấu hình đường dẫn file của bạn ---\n",
    "    INPUT_CSV_FILE = './src/conference/evaluate/full.csv'\n",
    "    OUTPUT_DUPLICATES_FILE = 'full_duplicates.csv'\n",
    "\n",
    "    # --- Cấu hình các cột để kiểm tra trùng lặp ---\n",
    "    # Nếu bạn muốn kiểm tra trùng lặp dựa trên TẤT CẢ các cột, hãy để là None\n",
    "    # COLUMNS_TO_CHECK = None\n",
    "\n",
    "    # Nếu bạn muốn kiểm tra trùng lặp chỉ dựa trên 'title' và 'acronym', hãy sử dụng:\n",
    "    COLUMNS_TO_CHECK = ['title', 'acronym']\n",
    "\n",
    "    # Gọi hàm để tìm và xuất các dòng trùng lặp\n",
    "    find_and_export_duplicates(INPUT_CSV_FILE, OUTPUT_DUPLICATES_FILE, COLUMNS_TO_CHECK)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
