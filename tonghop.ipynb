{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " BƯỚC 0: TÍNH TOÁN TỔNG SỐ CONFERENCE DUY NHẤT TRÊN TẤT CẢ CÁC FILE\n",
      "================================================================================\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_2_3_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_8_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_12_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_13_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_16_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_19_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_51_100.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_101_150.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_151_159.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_51_100_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_101_139_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_lan_3.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50_lan_4.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50_lan_4.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_not_crawl_1_50.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_not_crawl_51_end.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_4_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_5_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_9_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_14_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_15_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_18_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_batch_20_check_lan_1.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_1_50.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_51_100.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_101_150.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_1_32_lan_2.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_lan_3.csv\n",
      "Đã xử lý file để đếm: recrawl_all_tri_lan_4.csv\n",
      "Lỗi đọc file evaluate_recrawl_all_tri_1_50_lan_5.csv trong quá trình đếm: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv'\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tri_51_100_lan_5.csv\n",
      "Lỗi đọc file evaluate_recrawl_all_tri_101_120_lan_5.csv trong quá trình đếm: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv'\n",
      "Đã xử lý file để đếm: batch2.csv\n",
      "Đã xử lý file để đếm: batch3.csv\n",
      "Đã xử lý file để đếm: batch8.csv\n",
      "Đã xử lý file để đếm: batch12.csv\n",
      "Đã xử lý file để đếm: batch13.csv\n",
      "Đã xử lý file để đếm: batch16.csv\n",
      "Đã xử lý file để đếm: batch19.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_2_3_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_8_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_12_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_13_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_16_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_batch_19_lan_1.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_51_100.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_101_150.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_151_159.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_51_100_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_101_139_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_lan_3.csv\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tung_1_50_lan_4.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_not_crawl_1_50.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_not_crawl_51_end.csv\n",
      "Đã xử lý file để đếm: batch4.csv\n",
      "Đã xử lý file để đếm: batch5.csv\n",
      "Đã xử lý file để đếm: batch9.csv\n",
      "Đã xử lý file để đếm: batch14.csv\n",
      "Đã xử lý file để đếm: batch15.csv\n",
      "Đã xử lý file để đếm: batch18.csv\n",
      "Đã xử lý file để đếm: batch20.csv\n",
      "Đã xử lý file để đếm: recrawl_all_thang_1_50_lan_1_check.csv\n",
      "Đã xử lý file để đếm: recrawl_all_thang_51_58_lan_1_check.csv\n",
      "Lỗi đọc file evaluate_recrawl_all_tri_1_50_lan_5.csv trong quá trình đếm: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv'\n",
      "Đã xử lý file để đếm: evaluate_recrawl_all_tri_51_100_lan_5.csv\n",
      "Lỗi đọc file evaluate_recrawl_all_tri_101_120_lan_5.csv trong quá trình đếm: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv'\n",
      "Đã xử lý file để đếm: evaluate_crawl_con_lai_lan_2.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_con_lai.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_con_lai_lan_3.csv\n",
      "Đã xử lý file để đếm: evaluate_crawl_missing_info_in_full.csv\n",
      "\n",
      "----------------------------------------\n",
      ">>> TỔNG SỐ CONFERENCE DUY NHẤT (dựa trên title và acronym đã chuẩn hóa) là: 702\n",
      "----------------------------------------\n",
      "\n",
      "================================================================================\n",
      " BẮT ĐẦU KỊCH BẢN: ƯU TIÊN BẤT KỲ PHIÊN BẢN ĐÚNG NÀO\n",
      "================================================================================\n",
      "\n",
      "--- BƯỚC 1: Đọc và gộp tất cả các file ---\n",
      "Lỗi xử lý file ./src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv'\n",
      "Lỗi xử lý file ./src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv'\n",
      "Lỗi xử lý file ./src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv'\n",
      "Lỗi xử lý file ./src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv: [Errno 2] No such file or directory: './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv'\n",
      "Đã gộp thành công. Tổng số dòng để xử lý: 2233\n",
      "\n",
      "--- BƯỚC 2: Xác định danh sách các conference có ít nhất một phiên bản đúng ---\n",
      "Tìm thấy 682 conference có ít nhất một phiên bản đúng.\n",
      "\n",
      "--- BƯỚC 3: Tạo file correct cuối cùng ---\n",
      "\n",
      "--- BƯỚC 4: Tạo file recrawl cuối cùng ---\n",
      "\n",
      "--- BƯỚC 5: Hoàn tất và lưu file tổng hợp cuối cùng ---\n",
      "Đã lưu tổng cộng 682 dòng vào file correct './src/conference/evaluate/tri_tung_all_correct_final.csv'.\n",
      "Đã lưu tổng cộng 19 dòng vào file recrawl './src/conference/evaluate/tri_tung_all_recrawl_final.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ==============================================================================\n",
    "# HÀM CHUẨN HÓA VÀ HÀM TRỢ GIÚP (Giữ nguyên)\n",
    "# ==============================================================================\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    cleaned_text = re.sub(r'\\s*\\(.*?\\)\\s*', ' ', str(text))\n",
    "    final_text = re.sub(r'\\s+', ' ', cleaned_text).strip().lower()\n",
    "    return final_text\n",
    "\n",
    "def add_normalized_keys(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "    df_copy['key_title'] = df_copy['title'].apply(normalize_text) if 'title' in df_copy.columns else ''\n",
    "    df_copy['key_acronym'] = df_copy['acronym'].apply(normalize_text) if 'acronym' in df_copy.columns else ''\n",
    "    return df_copy\n",
    "\n",
    "# ==============================================================================\n",
    "# HÀM ĐẾM SỐ LƯỢNG DUY NHẤT (Giữ nguyên)\n",
    "# ==============================================================================\n",
    "\n",
    "def calculate_and_print_total_unique_count(all_files: list):\n",
    "    total_unique_conferences = set()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" BƯỚC 0: TÍNH TOÁN TỔNG SỐ CONFERENCE DUY NHẤT TRÊN TẤT CẢ CÁC FILE\")\n",
    "    print(\"=\"*80)\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig', na_values=['', ' '], dtype=str, low_memory=False)\n",
    "            df_normalized = add_normalized_keys(df)\n",
    "            if not df_normalized.empty:\n",
    "                unique_keys_in_file = set(zip(df_normalized['key_title'], df_normalized['key_acronym']))\n",
    "                total_unique_conferences.update(unique_keys_in_file)\n",
    "                print(f\"Đã xử lý file để đếm: {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi đọc file {os.path.basename(file_path)} trong quá trình đếm: {e}\")\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(f\">>> TỔNG SỐ CONFERENCE DUY NHẤT (dựa trên title và acronym đã chuẩn hóa) là: {len(total_unique_conferences)}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "# ==============================================================================\n",
    "# HÀM XỬ LÝ CHÍNH THEO LOGIC MỚI\n",
    "# ==============================================================================\n",
    "def process_by_any_correct_version(\n",
    "    input_files: list,\n",
    "    aggregated_correct_path: str,\n",
    "    aggregated_recrawl_path: str,\n",
    "    recrawled_files: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Phân loại conference: nếu có bất kỳ phiên bản nào không có 'note',\n",
    "    nó sẽ được coi là 'correct' và không nằm trong danh sách 'recrawl'.\n",
    "    Luôn ưu tiên lấy phiên bản mới nhất.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" BẮT ĐẦU KỊCH BẢN: ƯU TIÊN BẤT KỲ PHIÊN BẢN ĐÚNG NÀO\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # --- BƯỚC 1: ĐỌC VÀ GỘP TẤT CẢ CÁC FILE (ƯU TIÊN FILE ĐÃ RECRAWL) ---\n",
    "    print(\"--- BƯỚC 1: Đọc và gộp tất cả các file ---\")\n",
    "    all_dfs = []\n",
    "    all_files_in_order = (recrawled_files or []) + (input_files or [])\n",
    "    \n",
    "    for file_path in all_files_in_order:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig', na_values=[''], dtype=str).fillna('')\n",
    "            df = add_normalized_keys(df)\n",
    "            df = df[df['key_title'] != ''].copy()\n",
    "            if not df.empty:\n",
    "                all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi xử lý file {file_path}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    if not all_dfs:\n",
    "        print(\"Không có dữ liệu để xử lý. Dừng chương trình.\")\n",
    "        return\n",
    "\n",
    "    master_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"Đã gộp thành công. Tổng số dòng để xử lý: {len(master_df)}\")\n",
    "\n",
    "    # --- BƯỚC 2: XÁC ĐỊNH DANH SÁCH CÁC CONFERENCE \"ĐÚNG\" ---\n",
    "    print(\"\\n--- BƯỚC 2: Xác định danh sách các conference có ít nhất một phiên bản đúng ---\")\n",
    "    key_cols = ['key_title', 'key_acronym']\n",
    "    \n",
    "    # Điều kiện một dòng được coi là \"đúng\"\n",
    "    is_correct_condition = ~master_df['note'].notna() | (master_df['note'].astype(str).str.strip() == '')\n",
    "    \n",
    "    # Lấy tất cả các dòng đúng\n",
    "    all_correct_versions = master_df[is_correct_condition]\n",
    "    \n",
    "    # Lấy danh sách các khóa duy nhất của các conference đúng\n",
    "    correct_keys_df = all_correct_versions.drop_duplicates(subset=key_cols, keep='first')\n",
    "    correct_keys_set = set(zip(correct_keys_df['key_title'], correct_keys_df['key_acronym']))\n",
    "    print(f\"Tìm thấy {len(correct_keys_set)} conference có ít nhất một phiên bản đúng.\")\n",
    "\n",
    "    # --- BƯỚC 3: TẠO FILE CORRECT CUỐI CÙNG ---\n",
    "    print(\"\\n--- BƯỚC 3: Tạo file correct cuối cùng ---\")\n",
    "    # Từ tất cả các phiên bản đúng, giữ lại phiên bản mới nhất (xuất hiện đầu tiên) cho mỗi conference\n",
    "    final_correct_df = all_correct_versions.drop_duplicates(subset=key_cols, keep='first')\n",
    "    \n",
    "    # --- BƯỚC 4: TẠO FILE RECRAWL CUỐI CÙNG ---\n",
    "    print(\"\\n--- BƯỚC 4: Tạo file recrawl cuối cùng ---\")\n",
    "    # Lấy tất cả các dòng có note\n",
    "    all_recrawl_versions = master_df[~is_correct_condition].copy()\n",
    "    \n",
    "    if not all_recrawl_versions.empty:\n",
    "        # Loại bỏ những conference đã có trong danh sách đúng\n",
    "        mask_keep = ~all_recrawl_versions.apply(\n",
    "            lambda row: (row['key_title'], row['key_acronym']) in correct_keys_set,\n",
    "            axis=1\n",
    "        )\n",
    "        recrawl_to_keep = all_recrawl_versions[mask_keep]\n",
    "        \n",
    "        # Từ những dòng còn lại, giữ lại phiên bản mới nhất\n",
    "        final_recrawl_df = recrawl_to_keep.drop_duplicates(subset=key_cols, keep='first')\n",
    "    else:\n",
    "        final_recrawl_df = pd.DataFrame(columns=master_df.columns)\n",
    "\n",
    "    # --- BƯỚC 5: LƯU FILE ---\n",
    "    print(\"\\n--- BƯỚC 5: Hoàn tất và lưu file tổng hợp cuối cùng ---\")\n",
    "    \n",
    "    # Dọn dẹp cột khóa trước khi lưu\n",
    "    final_correct_df = final_correct_df.drop(columns=key_cols, errors='ignore')\n",
    "    final_recrawl_df = final_recrawl_df.drop(columns=key_cols, errors='ignore')\n",
    "\n",
    "    # Lưu file CORRECT\n",
    "    final_correct_df.to_csv(aggregated_correct_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Đã lưu tổng cộng {len(final_correct_df)} dòng vào file correct '{aggregated_correct_path}'.\")\n",
    "\n",
    "    # Lưu file RECRAWL\n",
    "    final_recrawl_df.to_csv(aggregated_recrawl_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Đã lưu tổng cộng {len(final_recrawl_df)} dòng vào file recrawl '{aggregated_recrawl_path}'.\")\n",
    "\n",
    "\n",
    "# --- Cách sử dụng ---\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_files_for_note_check = [\n",
    "\n",
    "\n",
    "        './src/conference/evaluate/batch2.csv',\n",
    "        './src/conference/evaluate/batch3.csv',\n",
    "        './src/conference/evaluate/batch8.csv',\n",
    "        './src/conference/evaluate/batch12.csv',\n",
    "        './src/conference/evaluate/batch13.csv',\n",
    "        './src/conference/evaluate/batch16.csv',\n",
    "        './src/conference/evaluate/batch19.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_2_3_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_8_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_12_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_13_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_16_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_19_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_51_100.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_101_150.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_151_159.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_51_100_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_101_139_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_lan_3.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50_lan_4.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_not_crawl_1_50.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_not_crawl_51_end.csv',\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        './src/conference/evaluate/batch4.csv',\n",
    "        './src/conference/evaluate/batch5.csv',\n",
    "        './src/conference/evaluate/batch9.csv',\n",
    "        './src/conference/evaluate/batch14.csv',\n",
    "        './src/conference/evaluate/batch15.csv',\n",
    "        './src/conference/evaluate/batch18.csv',\n",
    "        './src/conference/evaluate/batch20.csv',\n",
    "        './src/conference/evaluate/recrawl_all_thang_1_50_lan_1_check.csv',\n",
    "        './src/conference/evaluate/recrawl_all_thang_51_58_lan_1_check.csv',\n",
    "        \n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_51_100_lan_5.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv',\n",
    "        \n",
    "        './src/conference/evaluate/evaluate_crawl_con_lai_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_con_lai.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_con_lai_lan_3.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_missing_info_in_full.csv',\n",
    "\n",
    "\n",
    "    ]\n",
    "    recrawled_results_files = [\n",
    "\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_2_3_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_8_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_12_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_13_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_16_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_batch_19_lan_1.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_51_100.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_101_150.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_151_159.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_51_100_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_101_139_lan_2.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_lan_3.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50_lan_4.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tung_1_50_lan_4.csv',\n",
    "\n",
    "\n",
    "        './src/conference/evaluate/evaluate_crawl_not_crawl_1_50.csv',\n",
    "        './src/conference/evaluate/evaluate_crawl_not_crawl_51_end.csv',\n",
    "\n",
    "\n",
    "        \n",
    "        './src/conference/evaluate/recrawl_batch_4_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_5_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_9_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_14_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_15_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_18_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_batch_20_check_lan_1.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_1_50.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_51_100.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_101_150.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_1_32_lan_2.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_lan_3.csv',\n",
    "        './src/conference/evaluate/recrawl_all_tri_lan_4.csv',\n",
    "\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_1_50_lan_5.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_51_100_lan_5.csv',\n",
    "        './src/conference/evaluate/evaluate_recrawl_all_tri_101_120_lan_5.csv',\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    ]\n",
    "    aggregated_correct_file_note = './src/conference/evaluate/tri_tung_all_correct_final.csv'\n",
    "    aggregated_recrawl_file_note = './src/conference/evaluate/tri_tung_all_recrawl_final.csv'\n",
    "\n",
    "    all_files_for_counting = recrawled_results_files + input_csv_files_for_note_check\n",
    "    calculate_and_print_total_unique_count(all_files_for_counting)\n",
    "\n",
    "    process_by_any_correct_version(\n",
    "        input_files=input_csv_files_for_note_check,\n",
    "        aggregated_correct_path=aggregated_correct_file_note,\n",
    "        aggregated_recrawl_path=aggregated_recrawl_file_note,\n",
    "        recrawled_files=recrawled_results_files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu quá trình merge các file CSV và lọc các dòng non-link để tạo full.csv...\n",
      "Đã đọc thành công: ./src/conference/evaluate/tri_tung_all_correct_final.csv (có 39 cột)\n",
      "Đã đọc thành công: ./src/conference/evaluate/correctFiles/batch_1_6_7_10_11_17.csv (có 25 cột)\n",
      "\n",
      "Tìm thấy 25 cột chung ở tất cả các file:\n",
      "Thứ tự cột trong file output sẽ dựa trên file cuối cùng: ./src/conference/evaluate/correctFiles/batch_1_6_7_10_11_17.csv\n",
      "Thứ tự các cột: ['requestId', 'originalRequestId', 'title', 'acronym', 'mainLink', 'cfpLink', 'impLink', 'information', 'conferenceDates', 'year', 'location', 'cityStateProvince', 'country', 'continent', 'type', 'submissionDate', 'notificationDate', 'cameraReadyDate', 'registrationDate', 'otherDate', 'topics', 'publisher', 'summary', 'callForPapers', 'Unnamed: 24']\n",
      "DataFrame đã hợp nhất ban đầu có 887 dòng.\n",
      "\n",
      "Đã hợp nhất và lọc thành công các file vào './src/conference/evaluate/full.csv'.\n",
      "File kết quả có 887 dòng và 25 cột.\n",
      "\n",
      "--- Bắt đầu kiểm tra định dạng JSON trong các cột 'Date' ---\n",
      "Các cột sẽ được kiểm tra: ['submissionDate', 'notificationDate', 'cameraReadyDate', 'registrationDate', 'otherDate']\n",
      "\n",
      ">>> KIỂM TRA THÀNH CÔNG: Tất cả các giá trị trong các cột 'Date' đều là JSON hợp lệ (hoặc rỗng).\n",
      "\n",
      "--- Bắt đầu tạo file chứa các dòng thiếu thông tin (conferenceDates/location) ---\n",
      "\n",
      "Đã tạo thành công file './src/conference/evaluate/missing_info.csv' với 48 dòng.\n",
      "\n",
      "--- Bắt đầu tạo file chứa các dòng có 'submissionDate' là \"{}\" (trừ những dòng đã trong missing_info) ---\n",
      "Đã đọc thành công file missing_info: './src/conference/evaluate/missing_info.csv'.\n",
      "Đã loại bỏ 23 dòng trùng lặp với missing_info.csv.\n",
      "\n",
      "Đã tạo thành công file './src/conference/evaluate/empty_submission_date.csv' với 59 dòng.\n",
      "\n",
      "--- Bắt đầu cập nhật './src/conference/evaluate/full.csv' bằng dữ liệu từ './src/conference/evaluate/evaluate_empty_submission_date.csv' ---\n",
      "Đã đọc file full.csv: 887 dòng.\n",
      "Đã đọc file recrawled data: 54 dòng.\n",
      "\n",
      "Đã cập nhật thành công '54' dòng trong './src/conference/evaluate/full.csv'.\n",
      "File './src/conference/evaluate/full.csv' hiện có 887 dòng.\n",
      "Không có dòng nào từ file recrawled không tìm thấy khớp trong full.csv.\n",
      "\n",
      "--- Hoàn thành chương trình ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_non_link_acronyms(filepath):\n",
    "    \"\"\"\n",
    "    Đọc file non_link.txt và trả về một set chứa các acronym.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Đường dẫn đến file non_link.txt.\n",
    "\n",
    "    Returns:\n",
    "        set: Một set chứa các acronym từ file.\n",
    "    \"\"\"\n",
    "    acronyms = set()\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Cảnh báo: File non_link.txt không tồn tại tại '{filepath}'. Không áp dụng lọc non-link.\")\n",
    "        return acronyms\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                acronyms.add(line.strip())\n",
    "        print(f\"Đã tải {len(acronyms)} acronym từ '{filepath}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file non_link.txt '{filepath}': {e}. Không áp dụng lọc non-link.\")\n",
    "    return acronyms\n",
    "\n",
    "def merge_csv_common_columns_ordered(file_paths, output_filename, non_link_acronyms_filepath=None):\n",
    "    \"\"\"\n",
    "    Đọc nhiều file CSV, tìm các cột chung (phân biệt hoa thường),\n",
    "    sau đó hợp nhất các file chỉ với các cột chung đó.\n",
    "    Thứ tự các cột trong file kết quả sẽ theo thứ tự của các cột chung\n",
    "    trong file CSV cuối cùng trong danh sách input.\n",
    "    Sau đó, loại bỏ các dòng có acronym nằm trong danh sách non-link.\n",
    "\n",
    "    Args:\n",
    "        file_paths (list): Danh sách các đường dẫn đầy đủ đến các file CSV.\n",
    "        output_filename (str): Đường dẫn và tên file CSV đầu ra.\n",
    "        non_link_acronyms_filepath (str, optional): Đường dẫn đến file non_link.txt.\n",
    "                                                    Nếu được cung cấp, các dòng có acronym trong file này sẽ bị loại bỏ.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True nếu merge thành công, False nếu có lỗi.\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"Lỗi: Danh sách đường dẫn file CSV trống. Không có gì để xử lý.\")\n",
    "        return False\n",
    "\n",
    "    dataframes = []\n",
    "    # Bước 1: Đọc tất cả các file CSV vào DataFrame\n",
    "    for fp in file_paths:\n",
    "        try:\n",
    "            # Thêm dtype=str để đảm bảo mọi thứ được đọc vào dưới dạng chuỗi,\n",
    "            # tránh việc pandas tự động chuyển đổi kiểu dữ liệu có thể làm hỏng JSON.\n",
    "            df = pd.read_csv(fp, dtype=str)\n",
    "            dataframes.append(df)\n",
    "            print(f\"Đã đọc thành công: {fp} (có {len(df.columns)} cột)\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Lỗi: Không tìm thấy file tại đường dẫn: {fp}. Bỏ qua file này.\")\n",
    "            continue\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Cảnh báo: File trống hoặc không có header: {fp}. Bỏ qua file này.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi đọc file {fp}: {e}. Bỏ qua file này.\")\n",
    "            continue\n",
    "\n",
    "    if not dataframes:\n",
    "        print(\"Không có file CSV nào được đọc thành công. Không thể thực hiện merge.\")\n",
    "        return False\n",
    "\n",
    "    # Bước 2: Tìm danh sách các cột chung có ở tất cả các file\n",
    "    common_columns_set = set(dataframes[0].columns)\n",
    "    for i in range(1, len(dataframes)):\n",
    "        common_columns_set.intersection_update(set(dataframes[i].columns))\n",
    "\n",
    "    if not common_columns_set:\n",
    "        print(\"Không tìm thấy cột chung nào ở TẤT CẢ các file CSV đã đọc. Không tạo file mới.\")\n",
    "        return False\n",
    "\n",
    "    # Bước 3: Lấy thứ tự cột từ file cuối cùng trong danh sách\n",
    "    last_df = dataframes[-1]\n",
    "    ordered_common_columns = [col for col in last_df.columns if col in common_columns_set]\n",
    "    \n",
    "    if len(ordered_common_columns) != len(common_columns_set):\n",
    "        print(\"Cảnh báo: Thứ tự cột từ file cuối cùng không chứa tất cả các cột chung. Sẽ sắp xếp theo alphabet.\")\n",
    "        ordered_common_columns = sorted(list(common_columns_set))\n",
    "\n",
    "    print(f\"\\nTìm thấy {len(ordered_common_columns)} cột chung ở tất cả các file:\")\n",
    "    print(f\"Thứ tự cột trong file output sẽ dựa trên file cuối cùng: {file_paths[-1]}\")\n",
    "    print(f\"Thứ tự các cột: {ordered_common_columns}\")\n",
    "\n",
    "    # Bước 4: Tạo danh sách các DataFrame mới, chỉ chứa các cột chung\n",
    "    filtered_dataframes = [df[ordered_common_columns] for df in dataframes]\n",
    "\n",
    "    # Bước 5: Hợp nhất tất cả các DataFrame đã lọc lại với nhau\n",
    "    try:\n",
    "        merged_df = pd.concat(filtered_dataframes, ignore_index=True)\n",
    "        print(f\"DataFrame đã hợp nhất ban đầu có {merged_df.shape[0]} dòng.\")\n",
    "\n",
    "        # Bước 5.5: Lọc bỏ các dòng có acronym nằm trong non_link.txt\n",
    "        if non_link_acronyms_filepath and 'acronym' in merged_df.columns:\n",
    "            non_link_acronyms = load_non_link_acronyms(non_link_acronyms_filepath)\n",
    "            if non_link_acronyms:\n",
    "                initial_rows = len(merged_df)\n",
    "                merged_df = merged_df[~merged_df['acronym'].isin(non_link_acronyms)]\n",
    "                removed_rows = initial_rows - len(merged_df)\n",
    "                if removed_rows > 0:\n",
    "                    print(f\"Đã loại bỏ {removed_rows} dòng có acronym trong non_link.txt.\")\n",
    "                else:\n",
    "                    print(\"Không có dòng nào được loại bỏ theo non_link.txt.\")\n",
    "            else:\n",
    "                print(\"Không có acronym nào để lọc từ non_link.txt hoặc file không hợp lệ.\")\n",
    "        elif 'acronym' not in merged_df.columns and non_link_acronyms_filepath:\n",
    "            print(\"Cảnh báo: Cột 'acronym' không tồn tại trong DataFrame đã hợp nhất. Không thể lọc non-link.\")\n",
    "\n",
    "        # Bước 6: Lưu DataFrame đã hợp nhất và lọc vào file CSV mới\n",
    "        merged_df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nĐã hợp nhất và lọc thành công các file vào '{output_filename}'.\")\n",
    "        print(f\"File kết quả có {merged_df.shape[0]} dòng và {merged_df.shape[1]} cột.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi hợp nhất các DataFrame hoặc ghi file: {e}\")\n",
    "        return False\n",
    "\n",
    "def validate_json_in_date_columns(csv_filepath):\n",
    "    \"\"\"\n",
    "    Kiểm tra các cột kết thúc bằng \"Date\" trong một file CSV để xem mỗi ô\n",
    "    có chứa một chuỗi JSON hợp lệ hay không.\n",
    "\n",
    "    Args:\n",
    "        csv_filepath (str): Đường dẫn đến file CSV cần kiểm tra.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Bắt đầu kiểm tra định dạng JSON trong các cột 'Date' ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filepath, dtype=str).fillna('') # Đọc mọi thứ dạng chuỗi, và thay NaN bằng chuỗi rỗng\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file '{csv_filepath}' để kiểm tra.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file '{csv_filepath}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Tìm các cột có tên kết thúc bằng \"Date\"\n",
    "    date_columns = [col for col in df.columns if col.endswith(\"Date\")]\n",
    "\n",
    "    if not date_columns:\n",
    "        print(\"Không tìm thấy cột nào kết thúc bằng 'Date' để kiểm tra.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Các cột sẽ được kiểm tra: {date_columns}\")\n",
    "\n",
    "    invalid_entries = []\n",
    "    # Lặp qua từng cột cần kiểm tra\n",
    "    for col_name in date_columns:\n",
    "        # Lặp qua từng dòng (index và value) trong cột đó\n",
    "        for index, value in df[col_name].items():\n",
    "            # Bỏ qua các ô rỗng hoặc chỉ có khoảng trắng\n",
    "            if not value or value.isspace():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Thử phân tích chuỗi thành JSON\n",
    "                json.loads(value)\n",
    "            except json.JSONDecodeError:\n",
    "                # Nếu thất bại, ghi lại thông tin lỗi\n",
    "                invalid_entries.append({\n",
    "                    \"row_index\": index,\n",
    "                    \"column\": col_name,\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "    # In kết quả\n",
    "    if not invalid_entries:\n",
    "        print(\"\\n>>> KIỂM TRA THÀNH CÔNG: Tất cả các giá trị trong các cột 'Date' đều là JSON hợp lệ (hoặc rỗng).\")\n",
    "    else:\n",
    "        print(f\"\\n>>> KIỂM TRA THẤT BẠI: Tìm thấy {len(invalid_entries)} giá trị không phải là JSON hợp lệ:\")\n",
    "        print(\"-\" * 50)\n",
    "        for error in invalid_entries:\n",
    "            print(f\"  - Dòng (chỉ số 0): {error['row_index']}\")\n",
    "            print(f\"    Cột              : '{error['column']}'\")\n",
    "            print(f\"    Giá trị không hợp lệ: {error['value']}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "def create_missing_info_file(full_csv_path, output_missing_info_path):\n",
    "    \"\"\"\n",
    "    Đọc file CSV đã hợp nhất, lọc ra các dòng có 'conferenceDates' hoặc 'location' trống\n",
    "    hoặc chứa các giá trị cụ thể biểu thị thông tin thiếu,\n",
    "    và lưu các cột 'title', 'acronym' của chúng vào một file mới.\n",
    "\n",
    "    Args:\n",
    "        full_csv_path (str): Đường dẫn đến file CSV đầy đủ (đã merge).\n",
    "        output_missing_info_path (str): Đường dẫn file CSV đầu ra cho dữ liệu thiếu.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Bắt đầu tạo file chứa các dòng thiếu thông tin (conferenceDates/location) ---\")\n",
    "    try:\n",
    "        # Đọc file full.csv, thay thế các giá trị NaN bằng chuỗi rỗng để dễ xử lý\n",
    "        df = pd.read_csv(full_csv_path, dtype=str).fillna('')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file '{full_csv_path}' để xử lý.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file '{full_csv_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Kiểm tra xem các cột cần thiết có tồn tại không\n",
    "    required_columns = ['conferenceDates', 'location', 'title', 'acronym']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(f\"Lỗi: File '{full_csv_path}' thiếu một hoặc nhiều cột cần thiết: {required_columns}.\")\n",
    "        return\n",
    "\n",
    "    # Định nghĩa các giá trị được coi là thiếu thông tin\n",
    "    missing_dates_values = ['', 'To be announced', 'TBD', 'To be determined']\n",
    "    missing_location_values = ['', 'No location', 'TBD']\n",
    "\n",
    "    # Lọc các dòng mà 'conferenceDates' hoặc 'location' nằm trong danh sách các giá trị thiếu\n",
    "    missing_info_df = df[(df['conferenceDates'].isin(missing_dates_values)) | \n",
    "                         (df['location'].isin(missing_location_values))]\n",
    "\n",
    "    if missing_info_df.empty:\n",
    "        print(\"Không tìm thấy dòng nào có 'conferenceDates' hoặc 'location' trống/thiếu thông tin cụ thể.\")\n",
    "        return\n",
    "\n",
    "    # Chỉ giữ lại hai cột 'title' và 'acronym'\n",
    "    final_df = missing_info_df[['title', 'acronym']]\n",
    "\n",
    "    # Lưu vào file CSV mới\n",
    "    try:\n",
    "        final_df.to_csv(output_missing_info_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nĐã tạo thành công file '{output_missing_info_path}' với {len(final_df)} dòng.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file '{output_missing_info_path}': {e}\")\n",
    "\n",
    "def create_empty_submission_date_file(full_csv_path, output_empty_submission_path, missing_info_csv_path):\n",
    "    \"\"\"\n",
    "    Đọc file CSV đã hợp nhất, lọc ra các dòng có cột 'submissionDate' là \"{}\",\n",
    "    và loại trừ những dòng đã có trong file missing_info.csv (dựa trên title và acronym),\n",
    "    sau đó lưu toàn bộ các cột của chúng vào một file mới.\n",
    "\n",
    "    Args:\n",
    "        full_csv_path (str): Đường dẫn đến file CSV đầy đủ (đã merge).\n",
    "        output_empty_submission_path (str): Đường dẫn file CSV đầu ra cho dữ liệu có 'submissionDate' là \"{}\".\n",
    "        missing_info_csv_path (str): Đường dẫn đến file CSV chứa các dòng thiếu thông tin.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Bắt đầu tạo file chứa các dòng có 'submissionDate' là \\\"{}\\\" (trừ những dòng đã trong missing_info) ---\")\n",
    "    try:\n",
    "        # Đọc file full.csv\n",
    "        df_full = pd.read_csv(full_csv_path, dtype=str).fillna('')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file '{full_csv_path}' để xử lý.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file '{full_csv_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Kiểm tra xem cột 'submissionDate' có tồn tại không\n",
    "    if 'submissionDate' not in df_full.columns:\n",
    "        print(f\"Lỗi: File '{full_csv_path}' không có cột 'submissionDate'.\")\n",
    "        return\n",
    "    if 'title' not in df_full.columns or 'acronym' not in df_full.columns:\n",
    "        print(f\"Lỗi: File '{full_csv_path}' thiếu cột 'title' hoặc 'acronym', không thể loại trừ các dòng.\")\n",
    "        return\n",
    "\n",
    "    # Lọc các dòng mà 'submissionDate' có giá trị chính xác là \"{}\"\n",
    "    empty_submission_df = df_full[df_full['submissionDate'] == '{}'].copy() # Tạo bản sao để tránh SettingWithCopyWarning\n",
    "\n",
    "    if empty_submission_df.empty:\n",
    "        print(\"Không tìm thấy dòng nào có 'submissionDate' là \\\"{}\\\".\")\n",
    "        return\n",
    "\n",
    "    # Bước loại trừ: Đọc file missing_info.csv\n",
    "    try:\n",
    "        df_missing = pd.read_csv(missing_info_csv_path, dtype=str).fillna('')\n",
    "        print(f\"Đã đọc thành công file missing_info: '{missing_info_csv_path}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Cảnh báo: Không tìm thấy file missing_info tại '{missing_info_csv_path}'. Không thực hiện loại trừ.\")\n",
    "        df_missing = pd.DataFrame(columns=['title', 'acronym']) # Tạo DataFrame rỗng để không ảnh hưởng logic tiếp theo\n",
    "    except Exception as e:\n",
    "        print(f\"Cảnh báo: Lỗi khi đọc file missing_info '{missing_info_csv_path}': {e}. Không thực hiện loại trừ.\")\n",
    "        df_missing = pd.DataFrame(columns=['title', 'acronym'])\n",
    "\n",
    "\n",
    "    # Tạo một cột kết hợp 'title_acronym' để dễ dàng so sánh\n",
    "    if 'title' in df_missing.columns and 'acronym' in df_missing.columns:\n",
    "        empty_submission_df['merge_key'] = empty_submission_df['title'].astype(str) + \"_\" + empty_submission_df['acronym'].astype(str)\n",
    "        df_missing['merge_key'] = df_missing['title'].astype(str) + \"_\" + df_missing['acronym'].astype(str)\n",
    "\n",
    "        # Loại bỏ các dòng trong empty_submission_df nếu merge_key của chúng tồn tại trong df_missing\n",
    "        initial_rows = len(empty_submission_df)\n",
    "        empty_submission_df = empty_submission_df[~empty_submission_df['merge_key'].isin(df_missing['merge_key'])]\n",
    "        removed_rows = initial_rows - len(empty_submission_df)\n",
    "        \n",
    "        if removed_rows > 0:\n",
    "            print(f\"Đã loại bỏ {removed_rows} dòng trùng lặp với missing_info.csv.\")\n",
    "        else:\n",
    "            print(\"Không có dòng nào trong empty_submission_date trùng với missing_info để loại bỏ.\")\n",
    "\n",
    "        # Xóa cột merge_key trước khi lưu\n",
    "        empty_submission_df = empty_submission_df.drop(columns=['merge_key'])\n",
    "    else:\n",
    "        print(\"Cảnh báo: File missing_info.csv không có đủ cột 'title' hoặc 'acronym' để thực hiện loại trừ.\")\n",
    "\n",
    "    if empty_submission_df.empty:\n",
    "        print(\"Sau khi loại trừ, không còn dòng nào thỏa mãn điều kiện để tạo file.\")\n",
    "        return\n",
    "\n",
    "    # Lưu toàn bộ DataFrame đã lọc vào file CSV mới\n",
    "    try:\n",
    "        empty_submission_df.to_csv(output_empty_submission_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nĐã tạo thành công file '{output_empty_submission_path}' với {len(empty_submission_df)} dòng.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file '{output_empty_submission_path}': {e}\")\n",
    "\n",
    "# --- HÀM ĐƯỢC CẬP NHẬT ĐỂ XUẤT CÁC DÒNG KHÔNG ĐƯỢC CẬP NHẬT ---\n",
    "def update_full_csv_with_recrawled_data(full_csv_path, recrawled_empty_submission_path, output_unmatched_recrawl_path):\n",
    "    \"\"\"\n",
    "    Cập nhật các dòng trong full.csv bằng các dòng tương ứng từ recrawled_empty_submission_path\n",
    "    dựa trên cặp 'title' và 'acronym'.\n",
    "    Đồng thời, xuất các dòng từ recrawled_empty_submission_path mà không tìm thấy khớp\n",
    "    trong full.csv vào một file riêng.\n",
    "\n",
    "    Args:\n",
    "        full_csv_path (str): Đường dẫn đến file full.csv cần cập nhật.\n",
    "        recrawled_empty_submission_path (str): Đường dẫn đến file chứa dữ liệu đã được recrawl\n",
    "                                                (ví dụ: empty_submission_date.csv sau khi đã được điền dữ liệu).\n",
    "        output_unmatched_recrawl_path (str): Đường dẫn file CSV đầu ra cho các dòng không khớp.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Bắt đầu cập nhật '{full_csv_path}' bằng dữ liệu từ '{recrawled_empty_submission_path}' ---\")\n",
    "    try:\n",
    "        df_full = pd.read_csv(full_csv_path, dtype=str).fillna('')\n",
    "        print(f\"Đã đọc file full.csv: {len(df_full)} dòng.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file '{full_csv_path}'. Không thể cập nhật.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file '{full_csv_path}': {e}. Không thể cập nhật.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_recrawled = pd.read_csv(recrawled_empty_submission_path, dtype=str).fillna('')\n",
    "        print(f\"Đã đọc file recrawled data: {len(df_recrawled)} dòng.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file '{recrawled_empty_submission_path}'. Không có dữ liệu để cập nhật.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file '{recrawled_empty_submission_path}': {e}. Không có dữ liệu để cập nhật.\")\n",
    "        return\n",
    "\n",
    "    # Kiểm tra các cột cần thiết\n",
    "    required_cols = ['title', 'acronym']\n",
    "    if not all(col in df_full.columns for col in required_cols):\n",
    "        print(f\"Lỗi: File '{full_csv_path}' thiếu một hoặc nhiều cột cần thiết ({required_cols}).\")\n",
    "        return\n",
    "    if not all(col in df_recrawled.columns for col in required_cols):\n",
    "        print(f\"Lỗi: File '{recrawled_empty_submission_path}' thiếu một hoặc nhiều cột cần thiết ({required_cols}).\")\n",
    "        return\n",
    "\n",
    "    # Tạo một cột khóa để merge/cập nhật\n",
    "    df_full['merge_key'] = df_full['title'].astype(str) + \"_\" + df_full['acronym'].astype(str)\n",
    "    df_recrawled['merge_key'] = df_recrawled['title'].astype(str) + \"_\" + df_recrawled['acronym'].astype(str)\n",
    "\n",
    "    updated_rows_count = 0\n",
    "    unmatched_recrawled_rows = []\n",
    "\n",
    "    # Lặp qua từng dòng trong df_recrawled để cập nhật df_full\n",
    "    for index_recrawled, row_recrawled in df_recrawled.iterrows():\n",
    "        merge_key = row_recrawled['merge_key']\n",
    "        \n",
    "        # Tìm vị trí của dòng cần cập nhật trong df_full\n",
    "        matching_rows_indices = df_full[df_full['merge_key'] == merge_key].index\n",
    "        \n",
    "        if not matching_rows_indices.empty:\n",
    "            # Cập nhật tất cả các cột từ row_recrawled vào các dòng tương ứng trong df_full\n",
    "            cols_to_update = [col for col in df_recrawled.columns if col in df_full.columns and col != 'merge_key']\n",
    "            \n",
    "            for col in cols_to_update:\n",
    "                df_full.loc[matching_rows_indices, col] = row_recrawled[col]\n",
    "            \n",
    "            updated_rows_count += len(matching_rows_indices)\n",
    "        else:\n",
    "            # Nếu không tìm thấy khớp, thêm dòng này vào danh sách các dòng không khớp\n",
    "            unmatched_recrawled_rows.append(row_recrawled.drop('merge_key').to_dict())\n",
    "\n",
    "    # Xóa cột merge_key trước khi lưu\n",
    "    df_full = df_full.drop(columns=['merge_key'])\n",
    "\n",
    "    # Lưu DataFrame đã cập nhật vào file full.csv\n",
    "    try:\n",
    "        df_full.to_csv(full_csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nĐã cập nhật thành công '{updated_rows_count}' dòng trong '{full_csv_path}'.\")\n",
    "        print(f\"File '{full_csv_path}' hiện có {len(df_full)} dòng.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi ghi file '{full_csv_path}' sau khi cập nhật: {e}\")\n",
    "\n",
    "    # Xử lý các dòng không khớp\n",
    "    if unmatched_recrawled_rows:\n",
    "        df_unmatched = pd.DataFrame(unmatched_recrawled_rows)\n",
    "        try:\n",
    "            df_unmatched.to_csv(output_unmatched_recrawl_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"Đã xuất {len(df_unmatched)} dòng từ '{recrawled_empty_submission_path}' không tìm thấy khớp trong '{full_csv_path}' vào '{output_unmatched_recrawl_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi ghi file các dòng không khớp '{output_unmatched_recrawl_path}': {e}\")\n",
    "    else:\n",
    "        print(\"Không có dòng nào từ file recrawled không tìm thấy khớp trong full.csv.\")\n",
    "\n",
    "\n",
    "# --- Cách sử dụng ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Đặt danh sách các đường dẫn đến file CSV của bạn vào đây.\n",
    "    input_files = [\n",
    "        './src/conference/evaluate/tri_tung_all_correct_final.csv',\n",
    "        './src/conference/evaluate/correctFiles/batch_1_6_7_10_11_17.csv'\n",
    "    ]\n",
    "\n",
    "    # Đặt tên cho các file CSV kết quả đầu ra\n",
    "    output_merged_file = './src/conference/evaluate/full.csv'\n",
    "    output_missing_file = './src/conference/evaluate/missing_info.csv' # File thiếu conferenceDates/location\n",
    "    output_empty_submission_file = './src/conference/evaluate/empty_submission_date.csv' # File thiếu submissionDate\n",
    "    \n",
    "    # Đường dẫn đến file recrawled data (file empty_submission_date.csv sau khi đã được điền dữ liệu)\n",
    "    recrawled_empty_submission_file_path = './src/conference/evaluate/evaluate_empty_submission_date.csv' \n",
    "    # Đường dẫn cho file chứa các dòng không khớp\n",
    "    output_unmatched_recrawl_file = './src/conference/evaluate/unmatched_recrawled_entries.csv'\n",
    "\n",
    "    # --- QUY TRÌNH CHÍNH ---\n",
    "\n",
    "    # Bước 1: Merge các file CSV và lọc non-link để tạo full.csv ban đầu\n",
    "    print(\"Bắt đầu quá trình merge các file CSV và lọc các dòng non-link để tạo full.csv...\")\n",
    "    merge_successful = merge_csv_common_columns_ordered(input_files, output_merged_file)\n",
    "\n",
    "    if merge_successful:\n",
    "        # Bước 2: Kiểm tra định dạng JSON trong file full.csv\n",
    "        validate_json_in_date_columns(output_merged_file)\n",
    "        \n",
    "        # Bước 3: Tạo file chứa các dòng thiếu thông tin (conferenceDates/location)\n",
    "        create_missing_info_file(output_merged_file, output_missing_file)\n",
    "\n",
    "        # Bước 4: Tạo file chứa các dòng có 'submissionDate' là \"{}\" và không nằm trong missing_info\n",
    "        create_empty_submission_date_file(output_merged_file, output_empty_submission_file, output_missing_file)\n",
    "\n",
    "        # Bước 5: Cập nhật full.csv từ file recrawled empty submission date và xuất các dòng không khớp\n",
    "        update_full_csv_with_recrawled_data(output_merged_file, recrawled_empty_submission_file_path, output_unmatched_recrawl_file)\n",
    "\n",
    "    print(\"\\n--- Hoàn thành chương trình ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Chuẩn hóa văn bản bằng cách loại bỏ nội dung bên trong cặp dấu ngoặc đơn\n",
    "    và chính cặp dấu ngoặc đơn, đồng thời đảm bảo chỉ có một khoảng trắng\n",
    "    giữa các từ.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    cleaned_text = re.sub(r'\\s*\\(.*?\\)\\s*', ' ', str(text))\n",
    "    final_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return final_text\n",
    "\n",
    "def find_unique_in_core_detailed(full_csv_path, core_csv_path, non_link_acronyms_path, skip_acronyms_path):\n",
    "    \"\"\"\n",
    "    Tìm các dòng chỉ có trong file CORE_2023.csv dựa trên cặp (title, acronym)\n",
    "    sau khi chuẩn hóa, và in ra các bước chi tiết.\n",
    "    Đồng thời, lọc bỏ các conference trong full.csv có trong non_link.txt và skip.txt.\n",
    "\n",
    "    Args:\n",
    "        full_csv_path (str): Đường dẫn đến file full.csv.\n",
    "        core_csv_path (str): Đường dẫn đến file CORE_2023.csv.\n",
    "        non_link_acronyms_path (str): Đường dẫn đến file TXT chứa các key non-link.\n",
    "        skip_acronyms_path (str): Đường dẫn đến file TXT chứa các key cần bỏ qua (skip).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame chứa các dòng chỉ có trong CORE_2023.csv\n",
    "                          sau khi đã lọc bỏ các conference trong non_link và skip.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- BƯỚC 1: ĐỌC DỮ LIỆU ---\")\n",
    "    print(f\"Đang đọc file full.csv từ: {full_csv_path}\")\n",
    "    try:\n",
    "        df_full = pd.read_csv(full_csv_path)\n",
    "        if 'title' not in df_full.columns or 'acronym' not in df_full.columns:\n",
    "            raise ValueError(\"File full.csv phải có các cột 'title' và 'acronym'.\")\n",
    "        print(\"Đã đọc full.csv thành công. 5 dòng đầu tiên:\")\n",
    "        print(df_full.head().to_string())\n",
    "        print(f\"Tổng số dòng trong full.csv: {len(df_full)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file full.csv tại đường dẫn: {full_csv_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc full.csv: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nĐang đọc file CORE_2023.csv từ: {core_csv_path}\")\n",
    "    try:\n",
    "        df_core = pd.read_csv(core_csv_path, header=None)\n",
    "        if df_core.shape[1] < 3:\n",
    "            raise ValueError(\"File CORE_2023.csv phải có ít nhất 3 cột để lấy title và acronym.\")\n",
    "        df_core.rename(columns={1: 'title', 2: 'acronym'}, inplace=True)\n",
    "        print(\"Đã đọc CORE_2023.csv thành công. 5 dòng đầu tiên (cột 1 là title, cột 2 là acronym):\")\n",
    "        print(df_core.head().to_string())\n",
    "        print(f\"Tổng số dòng trong CORE_2023.csv: {len(df_core)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file CORE_2023.csv tại đường dẫn: {core_csv_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc CORE_2023.csv: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"\\n--- BƯỚC 2: CHUẨN HÓA DỮ LIỆU ---\")\n",
    "    print(\"Áp dụng hàm normalize_text cho cột 'title' và 'acronym' của cả hai DataFrame.\")\n",
    "\n",
    "    df_full['normalized_title'] = df_full['title'].apply(normalize_text)\n",
    "    df_full['normalized_acronym'] = df_full['acronym'].apply(normalize_text)\n",
    "    print(\"\\n5 dòng đầu tiên của full.csv sau khi chuẩn hóa:\")\n",
    "    print(df_full[['title', 'normalized_title', 'acronym', 'normalized_acronym']].head().to_string())\n",
    "\n",
    "    df_core['normalized_title'] = df_core['title'].apply(normalize_text)\n",
    "    df_core['normalized_acronym'] = df_core['acronym'].apply(normalize_text)\n",
    "    print(\"\\n5 dòng đầu tiên của CORE_2023.csv sau khi chuẩn hóa:\")\n",
    "    print(df_core[['title', 'normalized_title', 'acronym', 'normalized_acronym']].head().to_string())\n",
    "\n",
    "    print(\"\\n--- BƯỚC 3: TẠO KHÓA SO SÁNH (KEY) ---\")\n",
    "    print(\"Tạo cột 'key' từ cặp (normalized_title, normalized_acronym) cho cả hai DataFrame.\")\n",
    "    df_full['key'] = list(zip(df_full['normalized_title'], df_full['normalized_acronym']))\n",
    "    df_core['key'] = list(zip(df_core['normalized_title'], df_core['normalized_acronym']))\n",
    "\n",
    "    print(\"\\n5 dòng đầu tiên của full.csv với cột 'key':\")\n",
    "    print(df_full[['title', 'acronym', 'key']].head().to_string())\n",
    "    print(\"\\n5 dòng đầu tiên của CORE_2023.csv với cột 'key':\")\n",
    "    print(df_core[['title', 'acronym', 'key']].head().to_string())\n",
    "\n",
    "    print(\"\\n--- BƯỚC 4: ĐỌC VÀ XỬ LÝ DANH SÁCH NON-LINK VÀ SKIP ---\")\n",
    "    non_link_keys = set()\n",
    "    try:\n",
    "        with open(non_link_acronyms_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 3:\n",
    "                    title = normalize_text(parts[1])\n",
    "                    acronym = normalize_text(parts[2])\n",
    "                    if title and acronym:\n",
    "                        non_link_keys.add((title, acronym))\n",
    "        print(f\"Đã đọc {len(non_link_keys)} key non-link từ {non_link_acronyms_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file non-link acronyms tại đường dẫn: {non_link_acronyms_path}. Bỏ qua bước lọc này.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file non-link acronyms: {e}. Bỏ qua bước lọc này.\")\n",
    "\n",
    "    skip_keys = set()\n",
    "    try:\n",
    "        with open(skip_acronyms_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 3:\n",
    "                    title = normalize_text(parts[1])\n",
    "                    acronym = normalize_text(parts[2])\n",
    "                    if title and acronym:\n",
    "                        skip_keys.add((title, acronym))\n",
    "        print(f\"Đã đọc {len(skip_keys)} key skip từ {skip_acronyms_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file skip acronyms tại đường dẫn: {skip_acronyms_path}. Bỏ qua bước lọc này.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file skip acronyms: {e}. Bỏ qua bước lọc này.\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 5: LỌC BỎ CÁC CONFERENCE TRONG full.csv CÓ TRONG NON-LINK VÀ SKIP ---\")\n",
    "    initial_full_count = len(df_full)\n",
    "    print(f\"Số dòng ban đầu trong full.csv: {initial_full_count}\")\n",
    "\n",
    "    # Lọc bỏ các conference có key trong danh sách non-link\n",
    "    df_full_filtered = df_full[~df_full['key'].isin(non_link_keys)].copy()\n",
    "    print(f\"Số dòng trong full.csv sau khi loại bỏ non-link: {len(df_full_filtered)} (Đã loại bỏ {initial_full_count - len(df_full_filtered)} dòng).\")\n",
    "\n",
    "    # Lọc bỏ các conference có key trong danh sách skip\n",
    "    df_full_filtered = df_full_filtered[~df_full_filtered['key'].isin(skip_keys)].copy()\n",
    "    print(f\"Số dòng trong full.csv sau khi loại bỏ skip: {len(df_full_filtered)} (Đã loại bỏ {initial_full_count - len(df_full_filtered)} dòng tổng cộng).\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 6: TẠO TẬP HỢP CÁC KEY TỪ full.csv ĐÃ LỌC ---\")\n",
    "    keys_in_full_filtered = set(df_full_filtered['key'])\n",
    "    print(f\"Tổng số key duy nhất trong full.csv (sau khi lọc non-link và skip): {len(keys_in_full_filtered)}\")\n",
    "    # In ra một vài key mẫu để kiểm tra\n",
    "    print(\"Một vài key mẫu từ full.csv (đã chuẩn hóa và lọc):\")\n",
    "    for i, key in enumerate(list(keys_in_full_filtered)[:5]):\n",
    "        print(f\"  - {key}\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 7: LỌC CÁC DÒNG CHỈ CÓ TRONG CORE_2023.csv ---\")\n",
    "    print(\"Lọc df_core để tìm các dòng mà 'key' của chúng KHÔNG có trong tập hợp key của full.csv đã lọc.\")\n",
    "    unique_in_core_df = df_core[~df_core['key'].isin(keys_in_full_filtered)].copy()\n",
    "\n",
    "    print(f\"\\nTìm thấy {len(unique_in_core_df)} dòng chỉ có trong CORE_2023.csv.\")\n",
    "    if not unique_in_core_df.empty:\n",
    "        print(\"5 dòng đầu tiên của kết quả (các dòng chỉ có trong CORE_2023.csv):\")\n",
    "        print(unique_in_core_df.head().to_string())\n",
    "    else:\n",
    "        print(\"Không tìm thấy dòng nào chỉ có trong CORE_2023.csv.\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 8: DỌN DẸP CÁC CỘT TẠM THỜI ---\")\n",
    "    # Xóa các cột tạm thời đã tạo\n",
    "    unique_in_core_df.drop(columns=['normalized_title', 'normalized_acronym', 'key'], inplace=True, errors='ignore')\n",
    "\n",
    "    print(\"Đã xóa các cột 'normalized_title', 'normalized_acronym', 'key' khỏi DataFrame kết quả.\")\n",
    "    print(\"Quá trình so sánh hoàn tất.\")\n",
    "\n",
    "    return unique_in_core_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Cấu hình đường dẫn file của bạn ---\n",
    "    FULL_CSV_FILE = './src/conference/evaluate/corrected_full.csv'\n",
    "    CORE_CSV_FILE = './src/conference/csv/CORE_2023.csv'\n",
    "    NON_LINK_ACRONYMS_FILE = './non_link.txt'\n",
    "    SKIP_ACRONYMS_FILE = './skip.txt'\n",
    "    OUTPUT_CSV_FILE = './src/conference/evaluate/recrawl_all_core.csv'\n",
    "\n",
    "    # Gọi hàm để tìm các dòng duy nhất với chi tiết\n",
    "    result_df = find_unique_in_core_detailed(FULL_CSV_FILE, CORE_CSV_FILE, NON_LINK_ACRONYMS_FILE, SKIP_ACRONYMS_FILE)\n",
    "\n",
    "    if not result_df.empty:\n",
    "        print(f\"\\n--- KẾT QUẢ CUỐI CÙNG ---\")\n",
    "        print(f\"Tổng số dòng duy nhất trong CORE_2023.csv: {len(result_df)}\")\n",
    "        print(f\"Kết quả đã được lưu vào file: {OUTPUT_CSV_FILE}\")\n",
    "        result_df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "    else:\n",
    "        print(\"\\n--- KẾT QUẢ CUỐI CÙNG ---\")\n",
    "        print(\"Không tìm thấy dòng nào chỉ có trong CORE_2023.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sử dụng lại hàm normalize_text đã được cải tiến\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Chuẩn hóa văn bản bằng cách loại bỏ nội dung bên trong cặp dấu ngoặc đơn\n",
    "    và chính cặp dấu ngoặc đơn, đồng thời đảm bảo chỉ có một khoảng trắng\n",
    "    giữa các từ.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    cleaned_text = re.sub(r'\\s*\\(.*?\\)\\s*', ' ', str(text))\n",
    "    final_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return final_text\n",
    "\n",
    "def filter_recrawl_list_with_non_links(recrawl_all_core_path, crawling_files_paths, non_link_acronyms_path, skip_acronyms_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Lọc danh sách recrawl_all_core.csv bằng cách loại bỏ các conference\n",
    "    đang được crawl (dựa trên 3 file CSV khác), các conference\n",
    "    có acronym trong danh sách non-link từ file TXT, và các conference\n",
    "    có acronym trong danh sách skip từ file TXT.\n",
    "\n",
    "    Args:\n",
    "        recrawl_all_core_path (str): Đường dẫn đến file recrawl_all_core.csv.\n",
    "        crawling_files_paths (list): Danh sách các đường dẫn đến các file CSV\n",
    "                                      chứa danh sách conference đang được crawl.\n",
    "        non_link_acronyms_path (str): Đường dẫn đến file TXT chứa các acronym non-link.\n",
    "        skip_acronyms_path (str): Đường dẫn đến file TXT chứa các acronym cần bỏ qua (skip).\n",
    "        output_csv_path (str): Đường dẫn để lưu file CSV kết quả cuối cùng.\n",
    "    \"\"\"\n",
    "    print(\"--- BƯỚC 1: ĐỌC DANH SÁCH CẦN RECRAWL TỪ recrawl_all_core.csv ---\")\n",
    "    try:\n",
    "        # Đọc recrawl_all_core.csv - giả định nó đã có cột 'title' và 'acronym'\n",
    "        df_recrawl = pd.read_csv(recrawl_all_core_path)\n",
    "        if 'title' not in df_recrawl.columns or 'acronym' not in df_recrawl.columns:\n",
    "            raise ValueError(f\"File {recrawl_all_core_path} phải có các cột 'title' và 'acronym'.\")\n",
    "        print(f\"Đã đọc {len(df_recrawl)} dòng từ {recrawl_all_core_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file: {recrawl_all_core_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc {recrawl_all_core_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- BƯỚC 2: CHUẨN HÓA VÀ TẠO KEY CHO DANH SÁCH RECRAWL ---\")\n",
    "    df_recrawl['normalized_title'] = df_recrawl['title'].apply(normalize_text)\n",
    "    df_recrawl['normalized_acronym'] = df_recrawl['acronym'].apply(normalize_text)\n",
    "    df_recrawl['key'] = list(zip(df_recrawl['normalized_title'], df_recrawl['normalized_acronym']))\n",
    "    recrawl_keys = set(df_recrawl['key'])\n",
    "    print(f\"Tổng số key duy nhất trong recrawl_all_core: {len(recrawl_keys)}\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 3: ĐỌC VÀ TỔNG HỢP CÁC CONFERENCE ĐANG ĐƯỢC CRAWL ---\")\n",
    "    all_crawling_keys = set()\n",
    "    for i, file_path in enumerate(crawling_files_paths):\n",
    "        print(f\"Đang đọc file crawling {i+1}/{len(crawling_files_paths)}: {file_path}\")\n",
    "        try:\n",
    "            # Đối với các file crawling, nếu chúng cũng là CSV với cột 'title' và 'acronym'\n",
    "            df_crawling = pd.read_csv(file_path)\n",
    "            if 'title' not in df_crawling.columns or 'acronym' not in df_crawling.columns:\n",
    "                print(f\"Cảnh báo: File {file_path} không có cột 'title' hoặc 'acronym'. Bỏ qua file này.\")\n",
    "                continue\n",
    "\n",
    "            df_crawling['normalized_title'] = df_crawling['title'].apply(normalize_text)\n",
    "            df_crawling['normalized_acronym'] = df_crawling['acronym'].apply(normalize_text)\n",
    "            df_crawling['key'] = list(zip(df_crawling['normalized_title'], df_crawling['normalized_acronym']))\n",
    "\n",
    "            all_crawling_keys.update(set(df_crawling['key']))\n",
    "            print(f\"  Đã thêm {len(set(df_crawling['key']))} key từ {file_path}. Tổng số key đang crawl: {len(all_crawling_keys)}\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Lỗi: Không tìm thấy file crawling tại đường dẫn: {file_path}. Bỏ qua file này.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi đọc file crawling {file_path}: {e}. Bỏ qua file này.\")\n",
    "\n",
    "    print(f\"\\nTổng số key duy nhất từ tất cả các file đang crawl: {len(all_crawling_keys)}\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 4: ĐỌC DANH SÁCH ACRONYM NON-LINK VÀ SKIP (Đã điều chỉnh cho định dạng TXT mới) ---\")\n",
    "    non_link_keys = set() # Thay đổi từ set acronyms thành set keys\n",
    "    try:\n",
    "        with open(non_link_acronyms_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',') # Tách dòng bằng dấu phẩy\n",
    "                if len(parts) >= 3: # Đảm bảo có đủ các cột\n",
    "                    title = normalize_text(parts[1]) # Cột 2 là title\n",
    "                    acronym = normalize_text(parts[2]) # Cột 3 là acronym\n",
    "                    if title and acronym: # Đảm bảo không thêm key trống\n",
    "                        non_link_keys.add((title, acronym))\n",
    "        print(f\"Đã đọc {len(non_link_keys)} key non-link từ {non_link_acronyms_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file non-link acronyms tại đường dẫn: {non_link_acronyms_path}. Bỏ qua bước lọc này.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file non-link acronyms: {e}. Bỏ qua bước lọc này.\")\n",
    "\n",
    "    skip_keys = set() # Thay đổi từ set acronyms thành set keys\n",
    "    try:\n",
    "        with open(skip_acronyms_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',') # Tách dòng bằng dấu phẩy\n",
    "                if len(parts) >= 3: # Đảm bảo có đủ các cột\n",
    "                    title = normalize_text(parts[1]) # Cột 2 là title\n",
    "                    acronym = normalize_text(parts[2]) # Cột 3 là acronym\n",
    "                    if title and acronym: # Đảm bảo không thêm key trống\n",
    "                        skip_keys.add((title, acronym))\n",
    "        print(f\"Đã đọc {len(skip_keys)} key skip từ {skip_acronyms_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file skip acronyms tại đường dẫn: {skip_acronyms_path}. Bỏ qua bước lọc này.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file skip acronyms: {e}. Bỏ qua bước lọc này.\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 5: LỌC DANH SÁCH RECRAWL CUỐI CÙNG ---\")\n",
    "    print(\"Tìm các conference trong recrawl_all_core mà KHÔNG có trong danh sách đang crawl,\")\n",
    "    print(\"KHÔNG có key trong danh sách non-link, VÀ KHÔNG có key trong danh sách skip.\")\n",
    "\n",
    "    # Bắt đầu với danh sách recrawl_all_core\n",
    "    current_recrawl_df = df_recrawl.copy()\n",
    "    initial_count = len(current_recrawl_df)\n",
    "    print(f\"Số dòng ban đầu trong danh sách recrawl: {initial_count}\")\n",
    "\n",
    "    # Lọc bỏ các conference đang được crawl\n",
    "    filtered_by_crawling = current_recrawl_df[~current_recrawl_df['key'].isin(all_crawling_keys)].copy()\n",
    "    print(f\"Số dòng sau khi loại bỏ các conference đang crawl: {len(filtered_by_crawling)} (Đã loại bỏ {initial_count - len(filtered_by_crawling)} dòng).\")\n",
    "\n",
    "    # Lọc bỏ các conference có key trong danh sách non-link\n",
    "    # Bây giờ chúng ta lọc dựa trên 'key' thay vì 'normalized_acronym'\n",
    "    filtered_by_non_link = filtered_by_crawling[~filtered_by_crawling['key'].isin(non_link_keys)].copy()\n",
    "    print(f\"Số dòng sau khi loại bỏ các conference non-link: {len(filtered_by_non_link)} (Đã loại bỏ {len(filtered_by_crawling) - len(filtered_by_non_link)} dòng).\")\n",
    "\n",
    "    # Lọc bỏ các conference có key trong danh sách skip\n",
    "    # Tương tự, lọc dựa trên 'key'\n",
    "    final_recrawl_list_df = filtered_by_non_link[~filtered_by_non_link['key'].isin(skip_keys)].copy()\n",
    "    print(f\"Số dòng sau khi loại bỏ các conference skip: {len(final_recrawl_list_df)} (Đã loại bỏ {len(filtered_by_non_link) - len(final_recrawl_list_df)} dòng).\")\n",
    "\n",
    "    print(f\"\\nTìm thấy {len(final_recrawl_list_df)} conference cần recrawl cuối cùng.\")\n",
    "\n",
    "    print(\"\\n--- BƯỚC 6: DỌN DẸP VÀ LƯU KẾT QUẢ ---\")\n",
    "    # Xóa các cột tạm thời đã tạo\n",
    "    final_recrawl_list_df.drop(columns=['normalized_title', 'normalized_acronym', 'key'], inplace=True, errors='ignore')\n",
    "\n",
    "    if not final_recrawl_list_df.empty:\n",
    "        print(f\"5 dòng đầu tiên của danh sách recrawl cuối cùng:\")\n",
    "        print(final_recrawl_list_df.head().to_string())\n",
    "        print(f\"\\nKết quả đã được lưu vào file: {output_csv_path}\")\n",
    "        final_recrawl_list_df.to_csv(output_csv_path, index=False)\n",
    "    else:\n",
    "        print(\"Không tìm thấy conference nào cần recrawl cuối cùng sau khi lọc.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Cấu hình đường dẫn file của bạn ---\n",
    "    RECRAWL_ALL_CORE_FILE = './src/conference/evaluate/recrawl_all_core.csv'\n",
    "\n",
    "    # Danh sách các file CSV chứa conference đang được crawl\n",
    "    # Đảm bảo các file này có cột 'title' và 'acronym'\n",
    "    CRAWLING_FILES = [\n",
    "    #    './src/conference/evaluate/tri_tung_all_recrawl_final.csv'\n",
    "    ]\n",
    "\n",
    "    # Đường dẫn đến file TXT chứa các acronym non-link\n",
    "    # File này bây giờ được mong đợi có định dạng: id,title,acronym,...\n",
    "    NON_LINK_ACRONYMS_FILE = './non_link.txt'\n",
    "\n",
    "    # Đường dẫn đến file TXT chứa các acronym cần bỏ qua (skip)\n",
    "    # File này cũng được mong đợi có định dạng: id,title,acronym,...\n",
    "    SKIP_ACRONYMS_FILE = './skip.txt'\n",
    "\n",
    "    OUTPUT_FINAL_RECRAWL_FILE = './src/conference/evaluate/final_recrawl_list.csv'\n",
    "\n",
    "    # Gọi hàm để lọc danh sách recrawl\n",
    "    filter_recrawl_list_with_non_links(RECRAWL_ALL_CORE_FILE, CRAWLING_FILES, NON_LINK_ACRONYMS_FILE, SKIP_ACRONYMS_FILE, OUTPUT_FINAL_RECRAWL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_and_select_columns(full_csv_path, unique_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Gộp hai file CSV (full.csv và unique_in_CORE_2023.csv) và chỉ giữ lại\n",
    "    các cột 'title' và 'acronym', sau đó lưu vào một file CSV mới.\n",
    "\n",
    "    Args:\n",
    "        full_csv_path (str): Đường dẫn đến file full.csv.\n",
    "        unique_csv_path (str): Đường dẫn đến file unique_in_CORE_2023.csv.\n",
    "        output_csv_path (str): Đường dẫn để lưu file CSV kết quả.\n",
    "    \"\"\"\n",
    "    print(f\"Đang đọc file full.csv từ: {full_csv_path}\")\n",
    "    try:\n",
    "        df_full = pd.read_csv(full_csv_path)\n",
    "        # Đảm bảo các cột 'title' và 'acronym' tồn tại\n",
    "        if 'title' not in df_full.columns or 'acronym' not in df_full.columns:\n",
    "            raise ValueError(f\"File {full_csv_path} phải có các cột 'title' và 'acronym'.\")\n",
    "        print(\"Đã đọc full.csv thành công.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file full.csv tại đường dẫn: {full_csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc full.csv: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Đang đọc file unique_in_CORE_2023.csv từ: {unique_csv_path}\")\n",
    "    try:\n",
    "        df_unique = pd.read_csv(unique_csv_path)\n",
    "        # Đảm bảo các cột 'title' và 'acronym' tồn tại\n",
    "        if 'title' not in df_unique.columns or 'acronym' not in df_unique.columns:\n",
    "            raise ValueError(f\"File {unique_csv_path} phải có các cột 'title' và 'acronym'.\")\n",
    "        print(\"Đã đọc unique_in_CORE_2023.csv thành công.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file unique_in_CORE_2023.csv tại đường dẫn: {unique_csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc unique_in_CORE_2023.csv: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Đang chọn các cột 'title' và 'acronym' từ cả hai DataFrame...\")\n",
    "    # Chọn chỉ các cột 'title' và 'acronym' từ mỗi DataFrame\n",
    "    df_full_selected = df_full[['title', 'acronym']]\n",
    "    df_unique_selected = df_unique[['title', 'acronym']]\n",
    "\n",
    "    print(\"Đang gộp hai DataFrame...\")\n",
    "    # Gộp hai DataFrame theo chiều dọc (thêm hàng)\n",
    "    # ignore_index=True để reset index của DataFrame kết quả\n",
    "    merged_df = pd.concat([df_full_selected, df_unique_selected], ignore_index=True)\n",
    "\n",
    "    print(f\"Tổng số dòng sau khi gộp: {len(merged_df)}\")\n",
    "\n",
    "    # Tùy chọn: Xóa các dòng trùng lặp nếu bạn muốn một danh sách duy nhất\n",
    "    # Dòng trùng lặp ở đây có nghĩa là cả title và acronym đều giống hệt nhau\n",
    "    # Nếu bạn muốn giữ lại tất cả các dòng, kể cả trùng lặp, hãy bỏ qua bước này\n",
    "    print(\"Đang loại bỏ các dòng trùng lặp (nếu có)...\")\n",
    "    initial_rows = len(merged_df)\n",
    "    merged_df.drop_duplicates(inplace=True)\n",
    "    rows_after_dedup = len(merged_df)\n",
    "    print(f\"Số dòng sau khi loại bỏ trùng lặp: {rows_after_dedup} (Đã loại bỏ {initial_rows - rows_after_dedup} dòng trùng lặp).\")\n",
    "\n",
    "\n",
    "    print(f\"Đang lưu kết quả vào file: {output_csv_path}\")\n",
    "    try:\n",
    "        merged_df.to_csv(output_csv_path, index=False)\n",
    "        print(\"Đã lưu file thành công.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi lưu file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FULL_CSV_FILE = './src/conference/evaluate/full.csv'\n",
    "    UNIQUE_CSV_FILE = './src/conference/evaluate/recrawl_all_core.csv'\n",
    "    OUTPUT_MERGED_FILE = './src/conference/evaluate/merged_full_and_unique.csv'\n",
    "\n",
    "    # Gọi hàm để gộp các file\n",
    "    merge_and_select_columns(FULL_CSV_FILE, UNIQUE_CSV_FILE, OUTPUT_MERGED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đọc file CSV từ: ./src/conference/evaluate/corrected_full.csv\n",
      "Đã đọc 855 dòng từ ./src/conference/evaluate/corrected_full.csv.\n",
      "Đang tìm kiếm các dòng trùng lặp...\n",
      "Không tìm thấy dòng trùng lặp nào.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_and_export_duplicates(input_csv_path, output_csv_path, subset_columns=None):\n",
    "    \"\"\"\n",
    "    Tìm và xuất các dòng trùng lặp trong một file CSV.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): Đường dẫn đến file CSV đầu vào (ví dụ: full.csv).\n",
    "        output_csv_path (str): Đường dẫn để lưu file CSV chứa các dòng trùng lặp.\n",
    "        subset_columns (list, optional): Danh sách các tên cột để kiểm tra trùng lặp.\n",
    "                                         Nếu None, tất cả các cột sẽ được sử dụng.\n",
    "                                         Ví dụ: ['title', 'acronym']\n",
    "    \"\"\"\n",
    "    print(f\"Đang đọc file CSV từ: {input_csv_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        print(f\"Đã đọc {len(df)} dòng từ {input_csv_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file CSV tại đường dẫn: {input_csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Đang tìm kiếm các dòng trùng lặp...\")\n",
    "\n",
    "    # Tìm các dòng trùng lặp\n",
    "    # keep=False: Đánh dấu TẤT CẢ các lần xuất hiện của một dòng trùng lặp là True\n",
    "    #             (bao gồm cả lần xuất hiện đầu tiên).\n",
    "    #             Nếu muốn chỉ đánh dấu các bản sao (không bao gồm bản gốc đầu tiên),\n",
    "    #             sử dụng keep='first' hoặc keep='last'.\n",
    "    # subset: Các cột để kiểm tra trùng lặp. Nếu None, kiểm tra tất cả các cột.\n",
    "    duplicate_rows = df[df.duplicated(subset=subset_columns, keep=False)]\n",
    "\n",
    "    if not duplicate_rows.empty:\n",
    "        print(f\"Tìm thấy {len(duplicate_rows)} dòng trùng lặp.\")\n",
    "        print(f\"Đang lưu các dòng trùng lặp vào file: {output_csv_path}\")\n",
    "        try:\n",
    "            duplicate_rows.to_csv(output_csv_path, index=False)\n",
    "            print(\"Đã lưu file trùng lặp thành công.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi lưu file trùng lặp: {e}\")\n",
    "    else:\n",
    "        print(\"Không tìm thấy dòng trùng lặp nào.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Cấu hình đường dẫn file của bạn ---\n",
    "    INPUT_CSV_FILE = './src/conference/evaluate/corrected_full.csv'\n",
    "    OUTPUT_DUPLICATES_FILE = 'full_duplicates.csv'\n",
    "\n",
    "    # --- Cấu hình các cột để kiểm tra trùng lặp ---\n",
    "    # Nếu bạn muốn kiểm tra trùng lặp dựa trên TẤT CẢ các cột, hãy để là None\n",
    "    # COLUMNS_TO_CHECK = None\n",
    "\n",
    "    # Nếu bạn muốn kiểm tra trùng lặp chỉ dựa trên 'title' và 'acronym', hãy sử dụng:\n",
    "    COLUMNS_TO_CHECK = ['title', 'acronym']\n",
    "\n",
    "    # Gọi hàm để tìm và xuất các dòng trùng lặp\n",
    "    find_and_export_duplicates(INPUT_CSV_FILE, OUTPUT_DUPLICATES_FILE, COLUMNS_TO_CHECK)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
