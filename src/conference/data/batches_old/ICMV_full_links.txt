Conference full name: International Conference on Machine Vision (ICMV)

1. Website of ICMV_3: https://icmv.org/
Website information of ICMV_3:

Home» | Welcome Letter 
 href="#" - The 18th ICMV+ » | Programme 
 Conference Venue 
 VISA & INVITATION 
 Committee+» | Organizing Committee 
 Technical Committee 
 Speaker+» | Keynote Speakers 
 Invited Speakers 
 Previous Keynote Gallery 
 Special Session+» | href="cfpsessions.html" - Call for Special Sessions 
 Special Sessions 
 href="sub.html" - Submission+ » | href="cfp.html" - Call for Paper 
 href="sub.html" - Submission Guidance 
 href="date.html" - Key Dates 
 Registration+» | Instruction 
 Registration Fee 
 History+» | 2007-2024 Proceedings 
 href="photo2024.html" - ICMV 2024 Edinburgh 
 href="photo2023.html" - ICMV 2023 Yerevan 
 href="photo2022.html" - ICMV 2022 Rome Hybrid Conf. 
 href="photo2021.html" - ICMV 2021 Virtual Conf. 
 href="photo2020.html" - ICMV 2020 Virtual Conf. 
 href="photo2019.html" - ICMV 2019 Amsterdam 
 href="photo2018.html" - ICMV 2018 Munich 
 href="photo2017.html" - ICMV 2017 Vienna 
 href="photo2016.html" - ICMV 2016 Nice 
 href="photo2015.html" - ICMV 2015 Barcelona 
 href="photo2014.html" - ICMV 2014 Milano 
 href="photo2013.html" - ICMV 2013 London 
 Contact Us 
 value="#" - The 18th ICMV+ »
value="cfpsessions.html" - Call for Special Sessions
value="sub.html" - Submission+ »
value="cfp.html" - Call for Paper
value="sub.html" - Submission Guidance
value="date.html" - Key Dates
value="photo2024.html" - ICMV 2024 Edinburgh
value="photo2023.html" - ICMV 2023 Yerevan
value="photo2022.html" - ICMV 2022 Rome Hybrid Conf.
value="photo2021.html" - ICMV 2021 Virtual Conf.
value="photo2020.html" - ICMV 2020 Virtual Conf.
value="photo2019.html" - ICMV 2019 Amsterdam
value="photo2018.html" - ICMV 2018 Munich
value="photo2017.html" - ICMV 2017 Vienna
value="photo2016.html" - ICMV 2016 Nice
value="photo2015.html" - ICMV 2015 Barcelona
value="photo2014.html" - ICMV 2014 Milano
value="photo2013.html" - ICMV 2013 London
Machine Vision 
 This is one of the leading international conferences for presenting novel and fundamental advances in the fields of Machine Vision. 
 Read more 
 See you in Paris 
 Paris, the capital of France, is a city renowned for its rich history, stunning architecture, and vibrant culture. Known as the "City of Light," Paris offers a perfect blend of historical landmarks and modern charm. Visitors can explore iconic attractions such as the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral, etc. With its world-class cuisine, romantic ambiance, and timeless elegance, Paris continues to captivate millions of visitors from around the globe each year. 
 Sponsor Opportunity 
 We are open to any kind of sponsorships. Should you have any further suggestions, please do not hesitate to contact us 
 href="mailto:secretary@icmv.org" - Send Mail 
 Welcome to ICMV 2025, October 19-22, 2025 
 Paris, France 
 202518thInternational Conference on Machine Vision. 
 We are delighted to invite researchers, academics, and industry professionals to the18th International Conference on Machine Vision (ICMV 2025), which will take place fromOctober 19-22, 2025, in the vibrant city ofParis, France. Since its inception in 2007 (Islamabad), ICMV has evolved into a premier platform for exploring the latest advancements in machine vision and its transformative applications.Organized by esteemed institutions such as University of Stuttgart, University of Barcelona, and The Federal Research Center "Computer Science and Control" of the RAS, with support from globally renowned organizations including Aberystwyth University, Skolkovo Institute of Science and Technology, Ecole Nationale Supérieure des Mines de Saint-Etienne, University of Electronic Science and Technology of China, and Sfax University, ICMV 2025 promises a rich, interdisciplinary experience. Building on the momentum from previous editions held in prestigious locations like Edinburgh (2024), Rome (2022), and Amsterdam (2019), this year's conference will delve into the latest research, theories, and applications that are driving innovation in machine vision. 
  
 ICMV 2025 will emphasize the role of machine vision as a core driver of digital transformation, tackling topics such as: Theoretical advancements and new algorithms in image processing and computer vision. Applications in industrial automation, quality inspection, medical diagnostics, robotics, and more. Integrative technologies enabled by mobile and wireless devices. With a focus on cutting-edge research and practical implementations, the conference will provide a platform for leading minds to collaborate, share ideas, and drive progress in this dynamic field. 
  
 We encourage the submission of high-quality, original research papers that address all aspects of machine vision, including theory, principles, algorithms, practices, and applications. Selected papers will be presented through oral sessions, poster presentations, and invited talks, fostering an interactive and engaging environment. 
 Join us inParis, the city of lights and innovation, to connect with experts from around the world and shape the future of machine vision! 
 Conference Proceedings 
 ICMV 2025 accepted and presented papers can be published by SPIE, which will be included inSPIE Digital Library, provided to theWeb of ScienceConference Proceedings Citation Index-Science,Scopus, Ei Compendex, and others, to ensure maximum awareness of the Proceedings. 
  
 Conference proceedings of ICMV 2024 (ISBN: 9-781510-688278)has been onlinehere,index coming soon. 
 Conference proceedings of ICMV 2023 (ISBN: 9-781510-674622)has been onlinehere,successfullyindexed byWeb of SCI-ISI,EI CompendexandSCOPUS. 
 Conference proceedings of ICMV 2022 (ISBN: 9-781510-666184)has been onlinehref="https://www.spiedigitallibrary.org/search?term=ICMV+2022" - here
,successfully indexed byWeb of SCI-ISI,EI CompendexandSCOPUS. 
 Conference proceedings of ICMV 2021(ISBN: 9-781510-650442)has been onlinehref="https://www.spiedigitallibrary.org/search?term=ICMV+2021" - here
, successfullyindexed byWeb of SCI-ISI,EI CompendexandSCOPUS. 
 Conference proceedings of ICMV 2020 (ISBN: 9-781510-640405)has been onlinehere, indexed byWeb of SCI-ISI,EI CompendexandSCOPUSsuccessfully. 
 Conference proceedings of ICMV 2019 (ISBN: 9-781510-636439)has been onlinehere, indexed byEI,SCOPUSandISIsuccessfully 
 (Publication history) 
 The Proceedings of this conference are published in the SPIE 
 Digital Library along with nearly 440,000 papers from other 
 outstanding conferences, SPIE Journals, and chapters from SPIE 
 Press books. 
 Special Issue 
 Excellentpaperswith extensionof ICMV 2025 can be recommended and published in SPIE Journal special issue 
  
 Journal of Electronic Imaging (ISSN: 1017-9909). 
 Indexed by theScience Citation Index Expanded, Scopus, Ei Compendex, etc. 
 ISSN: 1017-9909, E-ISSN: 1560-229X, Copublishers: SPIE and IS&T(href="https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/call-for-papers?SSO=1#MachineVisionSystemsMethodsandApplications" - Check 
					more
) 
 Journal of Applied Remote Sensing (ISSN: 1931-3195) -with Impact Factor*: 1.344. 
 Indexed in theScience Citation Index Expanded, Scopus, Ei Compendex, etc. 
 (Check more) 
 Journal of Medical Imaging (ISSN: 2329-4302). 
 Indexed by theWeb of Science Emerging Sources Citation Index (ESCI), Scopus, Ei Compendex, etc. 
 (Check more) 
 Journal of Optical Engineering (ISSN: 0091-3286). 
 Indexed by theScience Citation Index Expanded, Scopus, Ei Compendex, etc. 
 (Check more) 
 Important Date 
  
 Submission Deadline | June 05, 2025 | Special Session Proposal Deadline | August 05, 2025 
 Notification Day | by July 05, 2025 | Notification | by August 10, 2025 
 Registration Deadline | July 30, 2025 | Complete Set of Special Session | August 15, 2025 
 Conference Date | October 19-22, 2025 
 Further More 
 Conference Venue 
 Pending 
 Add: pending 
 Read more 
 Submission Methods 
 Full paperand abstract are acceptable.Please send your contributions tohref="http://www.easychair.org/conferences/?conf=icmv2025" - Electronic Submission System
. 
 Note: If you are interesting to organize, special issues, please mail tohref="mailto:secretary@icmv.org" - secretary@icmv.org
. 
 Read more 
 "We sincerely invite you and your colleagues immediately mark this event on your calendar and make your plans toParis, France!"Copyright © 2025 18th International Conference on Machine Vision (www.icmv.org)

2. Website of ICMV_3: https://spie.org/Publications/Proceedings/Volume/13517
Website information of ICMV_3:

Events | Events | Events Home 
 Event Resources | Become an Exhibitor 
 Get the SPIE App 
 Event Policies 
 Official Contractors 
 Press Registration 
 Events Calendar | All Upcoming Events 
 Conferences 
 Exhibitions 
 SPIE.Online | Upcoming Webinars 
 Recorded Webinars 
 Featured Exhibitions | Photonics West 
 AR | VR | MR 
 Advanced Lithography + Patterning 
 Defense + Commercial Sensing 
 Optics + Photonics 
 Sensors + Imaging 
 href="//spie.org/conferences-and-exhibitions/event-resources" - For Authors + Volunteers | Manuscript Guidelines and Policies 
 href="//spie.org/conferences-and-exhibitions/for-authors-and-presenters/poster-pdf-guidelines" - Poster Presentation Guidelines 
 Event Volunteer Guidelines 
 Conference Chair Resources 
 Contact SPIE Program Coordinators 
 SPIE Defense + Commercial Sensing | 13 - 17 April 2025 in Orlando, Florida | Learn More | Home 
 Events Home 
 Event Resources | Event Resources | Events 
 Event Resources 
 Become an Exhibitor 
 Get the SPIE App 
 Event Policies 
 Official Contractors 
 Press Registration 
 Events Calendar | Events Calendar | Events 
 Events Calendar 
 All Upcoming Events 
 Conferences 
 Exhibitions 
 SPIE.Online | SPIE.Online | Events 
 SPIE.Online 
 Upcoming Webinars 
 Recorded Webinars 
 Featured Exhibitions | Featured Exhibitions | Events 
 Featured Exhibitions 
 Photonics West 
 AR | VR | MR 
 Advanced Lithography + Patterning 
 Defense + Commercial Sensing 
 Optics + Photonics 
 Sensors + Imaging 
 For Authors + Volunteers | href="//spie.org/conferences-and-exhibitions/event-resources" - For Authors + Volunteers | Events 
 href="//spie.org/conferences-and-exhibitions/event-resources" - For Authors + Volunteers 
 Manuscript Guidelines and Policies 
 href="//spie.org/conferences-and-exhibitions/for-authors-and-presenters/poster-pdf-guidelines" - Poster Presentation Guidelines 
 Event Volunteer Guidelines 
 Conference Chair Resources 
 Contact SPIE Program Coordinators 
 SPIE Defense + Commercial Sensing 
 13 - 17 April 2025 in Orlando, Florida 
 Learn More 
 Publications | Publications | Publications Home 
 Publication Resources | Terms of Use 
 Reprint Permission 
 Contact SPIE Publications 
 SPIE Digital Library 
 SPIE Bookstore | Books 
 Proceedings 
 Apparel and Gifts 
 SPIE Journals | Institutional Subscriptions 
 Individual Subscriptions 
 Conference Proceedings | Conference Content Publication Services 
 SPIE Press Books | href="//spie.org/publications/spie-press-books/book-author-information" - Book Author Information 
 Book Manuscript Guidelines 
 Submit a Book Proposal 
 href="//spie.org/publications/spie-press-books/about-spotlights-and-author-calls" - Spotlights Call for Authors 
 href="//spie.org/publications/spie-press-books/field-guide-author-guidelines" - Field Guide Author Guidelines 
 New Books from SPIE Press | Titles include "Optics using Python" and "Designing Optics Using Zemax OpticStudio" | Visit the Bookstore | Home 
 Publications Home 
 Publication Resources | Publication Resources | Publications 
 Publication Resources 
 Terms of Use 
 Reprint Permission 
 Contact SPIE Publications 
 SPIE Digital Library 
 SPIE Bookstore | SPIE Bookstore | Publications 
 SPIE Bookstore 
 Books 
 Proceedings 
 Apparel and Gifts 
 SPIE Journals | SPIE Journals | Publications 
 SPIE Journals 
 Institutional Subscriptions 
 Individual Subscriptions 
 Conference Proceedings | Conference Proceedings | Publications 
 Conference Proceedings 
 Conference Content Publication Services 
 SPIE Press Books | SPIE Press Books | Publications 
 SPIE Press Books 
 href="//spie.org/publications/spie-press-books/book-author-information" - Book Author Information 
 Book Manuscript Guidelines 
 Submit a Book Proposal 
 href="//spie.org/publications/spie-press-books/about-spotlights-and-author-calls" - Spotlights Call for Authors 
 href="//spie.org/publications/spie-press-books/field-guide-author-guidelines" - Field Guide Author Guidelines 
 New Books from SPIE Press 
 Titles include "Optics using Python" and "Designing Optics Using Zemax OpticStudio" 
 Visit the Bookstore 
 Membership | Membership | Membership Home 
 Member Benefits 
 Join or Renew 
 SPIE Fellows | List of all SPIE Fellows 
 Nominate a Fellow 
 SPIE Senior Members | List of all Senior Members 
 Nominate a Senior Member 
 Student Membership | Student Chapters 
 Student Awards 
 Student Resources 
 SPIE Profiles 
 Corporate Membership | Corporate Member Benefits 
 Corporate Member Directory 
 Become an SPIE Member | Join over 25,000 of your friends and colleagues in the largest global optics and photonics professional society. | Join or Renew Today | Home 
 Membership Home 
 Member Benefits 
 Join or Renew 
 SPIE Fellows | SPIE Fellows | Membership 
 SPIE Fellows 
 List of all SPIE Fellows 
 Nominate a Fellow 
 SPIE Senior Members | SPIE Senior Members | Membership 
 SPIE Senior Members 
 List of all Senior Members 
 Nominate a Senior Member 
 Student Membership | Student Membership | Membership 
 Student Membership 
 Student Chapters 
 Student Awards 
 Student Resources 
 SPIE Profiles 
 Corporate Membership | Corporate Membership | Membership 
 Corporate Membership 
 Corporate Member Benefits 
 Corporate Member Directory 
 Become an SPIE Member 
 Join over 25,000 of your friends and colleagues in the largest global optics and photonics professional society. 
 Join or Renew Today 
 Career + Courses | Career + Courses | Career + Courses Home 
 Career Center | Find a Job 
 Post a Job 
 Career Center FAQs 
 SPIE Job Fairs 
 Career Resources 
 Courses | Find a Course 
 Courses at Conferences 
 Online Courses 
 Group Training 
 Education Webinar Series 
 Teach a Course for SPIE 
 Technician Resources | Technician Training Programs 
 Technician Scholarship 
 AMIP Teaching Modules 
 OP-TEC Course Materials 
 SPIE Career Center | Reimagine your career | Browse Job Listings | Home 
 Career + Courses Home 
 Career Center | Career Center | Career + Courses 
 Career Center 
 Find a Job 
 Post a Job 
 Career Center FAQs 
 SPIE Job Fairs 
 Career Resources 
 Courses | Courses | Career + Courses 
 Courses 
 Find a Course 
 Courses at Conferences 
 Online Courses 
 Group Training 
 Education Webinar Series 
 Teach a Course for SPIE 
 Technician Resources | Technician Resources | Career + Courses 
 Technician Resources 
 Technician Training Programs 
 Technician Scholarship 
 AMIP Teaching Modules 
 OP-TEC Course Materials 
 SPIE Career Center 
 Reimagine your career 
 Browse Job Listings 
 Community Support | Community Support | Community Support Home 
 Equity, Diversity, + Inclusion | Family Care Grants 
 EDI Resources 
 EDI Videos 
 Women in Optics 
 SPIE Society Awards | Award Nomination Guide 
 Awards Banquet 
 href="//spie.org/community-support/research-and-program-funding" - Research + Program Funding | href="//spie.org/community-support/research-and-program-funding/spie-endowment-matching-program" - SPIE Endowment Matching Program 
 SPIE-Franz Hillenkamp Postdoctoral Fellowship 
 href="//spie.org/community-support/research-and-program-funding/ibm-spie-hbcu-faculty-accelerator-award-in-quantum-optics-and-photonics" - IBM SPIE HBCU Faculty Accelerator Award 
 Student Funding | Scholarships 
 Conference Support 
 Industry Resources | Become an Exhibitor 
 Global Industry Report 
 Global Salary Report 
 Partners + Industry Clusters 
 Post a Job 
 Education Outreach | Outreach Grants 
 Posters 
 Optipedia 
 Advocacy + Public Policy | CHIPS for America 
 Policy Position Statements 
 Visit + Contact US Congress 
 International Day of Light | IDL Photo Contest 
 IDL Resources 
 Nominations now open for SPIE Fellows | The deadline for applications is 15 September | Learn More | Home 
 Community Support Home 
 Equity, Diversity, + Inclusion | Equity, Diversity, + Inclusion | Community Support 
 Equity, Diversity, + Inclusion 
 Family Care Grants 
 EDI Resources 
 EDI Videos 
 Women in Optics 
 SPIE Society Awards | SPIE Society Awards | Community Support 
 SPIE Society Awards 
 Award Nomination Guide 
 Awards Banquet 
 Research + Program Funding | href="//spie.org/community-support/research-and-program-funding" - Research + Program Funding | Community Support 
 href="//spie.org/community-support/research-and-program-funding" - Research + Program Funding 
 href="//spie.org/community-support/research-and-program-funding/spie-endowment-matching-program" - SPIE Endowment Matching Program 
 SPIE-Franz Hillenkamp Postdoctoral Fellowship 
 href="//spie.org/community-support/research-and-program-funding/ibm-spie-hbcu-faculty-accelerator-award-in-quantum-optics-and-photonics" - IBM SPIE HBCU Faculty Accelerator Award 
 Student Funding | Student Funding | Community Support 
 Student Funding 
 Scholarships 
 Conference Support 
 Industry Resources | Industry Resources | Community Support 
 Industry Resources 
 Become an Exhibitor 
 Global Industry Report 
 Global Salary Report 
 Partners + Industry Clusters 
 Post a Job 
 Education Outreach | Education Outreach | Community Support 
 Education Outreach 
 Outreach Grants 
 Posters 
 Optipedia 
 Advocacy + Public Policy | Advocacy + Public Policy | Community Support 
 Advocacy + Public Policy 
 CHIPS for America 
 Policy Position Statements 
 Visit + Contact US Congress 
 International Day of Light | International Day of Light | Community Support 
 International Day of Light 
 IDL Photo Contest 
 IDL Resources 
 Nominations now open for SPIE Fellows 
 The deadline for applications is 15 September 
 Learn More 
 News | News | News Home | Community News 
 SPIE Event News 
 SPIE Publication News 
 SPIE Press Releases 
 Photonics Focus 
 Optics.org 
 href="https://spie.org/news/photonics-focus/marchapril-2025" - | Latest Issue of Photonics Focus | March/April issue explores the ever-shrinking world of microelectronics | href="https://spie.org/news/photonics-focus/marchapril-2025" - March April 2025 | Home 
 News Home | News Home | News 
 News Home 
 Community News 
 SPIE Event News 
 SPIE Publication News 
 SPIE Press Releases 
 Photonics Focus 
 Optics.org 
 href="https://spie.org/news/photonics-focus/marchapril-2025" - 
Latest Issue of Photonics Focus 
 March/April issue explores the ever-shrinking world of microelectronics 
 href="https://spie.org/news/photonics-focus/marchapril-2025" - March April 2025 
 About | About | About SPIE Home 
 About the Society | Mission and Values 
 Officers and Directors 
 Committees 
 History 
 Past Officers and Directors 
 Bylaws 
 SPIE Brand and Logos 
 Jobs at SPIE 
 Code of Conduct 
 Policies and Reporting 
 Honor your Heroes | SPIE Society Awards are Open for Nominations | Learn More | Home 
 About SPIE Home 
 About the Society | About the Society | About 
 About the Society 
 Mission and Values 
 Officers and Directors 
 Committees 
 History 
 Past Officers and Directors 
 Bylaws 
 SPIE Brand and Logos 
 Jobs at SPIE 
 Code of Conduct 
 Policies and Reporting 
 Honor your Heroes 
 SPIE Society Awards are Open for Nominations 
 Learn More 
 More 
 Sign In 
 Cart 
 Sign in 
 Unable to load cart 
 More SPIE Websites 
 Explore SPIE websites: 
 Publications 
 Publications Home 
 Publication Resources | Publications 
 Publication Resources 
 Terms of Use 
 Reprint Permission 
 Contact SPIE Publications 
 SPIE Digital Library 
 SPIE Bookstore | Publications 
 SPIE Bookstore 
 Books 
 Proceedings 
 Apparel and Gifts 
 SPIE Journals | Publications 
 SPIE Journals 
 Institutional Subscriptions 
 Individual Subscriptions 
 Conference Proceedings | Publications 
 Conference Proceedings 
 Conference Content Publication Services 
 SPIE Press Books | Publications 
 SPIE Press Books 
 href="//spie.org/publications/spie-press-books/book-author-information" - Book Author Information 
 Book Manuscript Guidelines 
 Submit a Book Proposal 
 href="//spie.org/publications/spie-press-books/about-spotlights-and-author-calls" - Spotlights Call for Authors 
 href="//spie.org/publications/spie-press-books/field-guide-author-guidelines" - Field Guide Author Guidelines 
 PublicationsBookstoreConference ProceedingsProceedings Volume 13517 
 Seventeenth International Conference on Machine Vision (ICMV 2024) 
 Wolfgang OstenProceedings Volume 13517 
 Seventeenth International Conference on Machine Vision (ICMV 2024) 
 Wolfgang OstenPurchase the printed version of this volume atproceedings.comor access the digital version at SPIE Digital Library. 
 Buy Printed VolumeView on SPIE Digital LibraryBuy Printed VolumeView on SPIE Digital LibraryVolume Details 
 Date Published: 25 February 2025 
 Contents: 8 Sessions, 55 Papers, 0 Presentations 
 Conference: Seventeenth International Conference on Machine Vision (ICMV 2024) 2024 
 Volume Number: 13517 
 Table of Contents 
 Table of Contents 
 All links to SPIE Proceedings will open in theSPIE Digital Library.Show all abstracts 
 View SessionFront Matter: Volume 13517 
 Image segmentation and computing 
 Computer vision and image analysis 
 Image-based intelligent detection technology and application 
 Machine Vision and Measurement Based on Machine Learning 
 Detection and classification technology of food and plant diseases based on machine vision 
 Adversarial machine learning in vision, speech, and text 
 Intelligent image recognition and application technology 
 Front Matter: Volume 13517 
 Front Matter: Volume 13517Show abstract 
 This PDF file contains the front matter associated with SPIE Proceedings Volume 13517, including the Title Page, Copyright information, Table of Contents, and Conference Committee information. 
 Image segmentation and computing 
 A deep learning-based retinal disease analysis framework for optical coherence tomographic image generation, segmentation, and classificationAnum Abdul Salam,Mudassar Khan,Farheen Fatima,et al.Show abstract 
 Computer Aided Diagnostics has revolutionised medical diagnosis, however, with the advent of deep learning, the entire process has become data-hungry. Moreover, acquiring medical data requires consent from patients and medical organisations, making the procedure cumbersome. In addition to that, data alone is not sufficient rather it requires ground truth, which involves manual annotations by medical specialists. Entire process of data acquisition and annotation is a huge bottleneck in the autonomous disease diagnosis pipeline. It has caused hindrances to in-time disease diagnosis leading to loss of vision in case of retinal diseases. The proposed framework has addressed the challenges by utilising StyleGaAN-II for image generation followed by autonomous image annotation using segFormer. In addition to generating healthy OCT images, we have targeted three retinal conditions drusen, Choroidal Neovascularization, Diabetic macular edema and healthy eye condition yielding FID scores of 50.12, 38.89, 39.9 and 56.04 respectively. When evaluated on a Likert scale of three, synthetic images resulted in the highest agreement score of 92% as evaluated by an ophthalmologist. Generated images can be autonomously annotated for Ganglion Cell Layer, Retinal Nerve Fiber Layer, Choroidal layer and background by our trained model with an IOU of 0.95. 
 Barrel instance segmentation and grasping for (semi-)autonomous excavatorsFlorian Jordan,Christian Frese,Winfried Baum,et al.Show abstract 
 During accidents involving hazardous chemicals, people in the area may be put at risk of harm. (Semi-)autonomous robots can mitigate this threat by removing leaking containers. However, teleoperation requires extensive training and is difficult in practice. To overcome these limitations, we implemented a perception system on an autonomous excavator that locates individual barrels in chaotic scenes for extraction. Following the human-in-the-loop principle, operators can remotely select which barrel to remove. An efficient U-Net-style, DCAN-flavored neural network is trained using synthetic and collected real-world RGB data (5,000 synthetic and 593 real images) and compared to an inference-heavy Mask R-CNN model. In experiments on a leave-out test set, created from the excavator, our model yielded an ODS mIoU of 85.14% and mAP of 72.19%, while Mask R-CNN achieved an ODS mIoU of 86.6% and mAP of 84.31%. With roughly 0.00584s inference time on 800×576 32-bit tensors, our model is faster than Mask R-CNN with an inference time of roughly 0.0491s. Using the robot calibration data, the point clouds of multiple LiDAR sensors are fused with the RGB segmentation to find local cylinder models for each barrel, delivering the exact poses for extraction using the motion planner to find a collision-free motion plan. Force measurements were included in the gripper to avoid deforming the barrel. Field trials showed that the barrels can be reliably extracted without any damage. 
 Impact of synthetic images in the training of neural networks for airborne vessel segmentationFrancisco Matilde,Gonçalo Cruz,Diogo SilvaShow abstract 
 Synthetic images have appeared as a possible solution to the scarcity of real images for training neural networks. We investigate their use in the training of ship detectors in aerial imagery. We create a synthetic dataset through 2 distinct methods: modeling and rendering with Blender, and a learning-based approach with GauGAN. YOLACT++ was chosen as the detector and its performance was evaluated when synthetic images were included in its training. The results suggest that synthetic augmentations can improve a model's performance, under certain conditions. Adding synthetic data to large training sets degraded performance on the target domain, but significantly improved it on small datasets. 
 Multimodality fusion for enhanced driving scene semantic segmentationNgoc-Linh Nguyen-Ha,Gia-Khan Le,Xuan-Tung Nguyen,et al.Show abstract 
 Semantic segmentation in autonomous driving scenes is essential for precise environment perception and decision-making processes. While single modality systems demonstrate efficacy for specific applications, they frequently fall short in addressing the complexities of diverse data. Multi-modal systems thereby overcome these limitations by integrating multiple data types, providing a more comprehensive and accurate analysis. In this paper, we propose an approach to enhance semantic segmentation performance through multi-modality data fusion, integrating LiDAR point clouds, high-resolution camera images, and range images. The architecture incorporates discrete feature extraction and a fusion mechanism to exploit the synergy between modalities. We conduct study on several benchmark datasets, which encompass diverse driving scenery and scenarios for our extensive experiments to demonstrate the robustness of our multi-modality approach. 
 Computer vision and image analysis 
 Dataset and feature point-based matching algorithm for satellite and aerial remotely sensed imagesA. P. Kutakova,V. V. Kokhan,D. A. BocharovShow abstract 
 Earth remote sensing data is used to solve a wide range of tasks of precision agriculture. For tasks of agriculture fields monitoring, firstly vegetation state analysis, it may not be enough to use only satellite data sources. In satellite images, agriculture fields may be covered by clouds, which significantly complicates, and in some cases makes it impossible, to analyze the data. Moreover, due to technical limitations, there is no possibility to frequently conduct satellite imagery of specific locations. The above factors lead to a lack of data. One of the approaches to overcome the problem of data lacking is extension of the data using other sources. Imagery obtained by aerial vehicles can be a reliable supplement because of more controllable parameters of sensing such as altitude and sensing time. Thus, the problem of satellite and aerial images matching arises that is essential for further task of images historical analysis. In this paper, we study the existing datasets and methods for images matching but for satellite and aerial remote sensing data. We highlight the problem of lack of available datasets for the considered task and propose a new categorized dataset LAVICA-2024 that consists of 18 pairs of images from Landsat-8 and AVIRIS (Airbone visible/infrared imaging spectrometer) programs. Along with the dataset a novel algorithm based on feature points detection and matching approach is proposed and evaluated using the proposed LAVICA-2024 data. 
 VGG16, ResNet50, and Xception feature extractors combined with various classifiers for autism spectrum disorder classification using fMRI dataFatima Ez-Zahraa Bazay,Ahmed Drissi El MalianiShow abstract 
 Autism Spectrum Disorder (ASD) is a complex neuropsychiatric disorder characterized by social deficits and repetitive behaviors. This study explores the effectiveness of different feature extractors and classifiers for the classification of ASD using functional Magnetic Resonance Imaging (fMRI) data from 15 sites in the Autism Brain Imaging Data Exchange I (ABIDE I) database. We used three deep learning models (VGG16, ResNet50, Xception) to extract features from images generated by a single volume image generator. The extracted features were then used as input for four classifiers: Extreme Gradient Boosting (XGBoost), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest Neighbors (KNN). The results show that ResNet50, when combined with SVM or XGBoost, generally offers the best performance in terms of accuracy, specificity, and sensitivity, often achieving accuracy rates in excess of 95% at several sites. VGG16 and Xception also demonstrated competitive performance, albeit slightly inferior to ResNet50 in some cases. In terms of classifiers, XGBoost and SVM stood out for their ability to handle complex data and deliver accurate results. Performance varied from site to site, with certain combinations of extractors and classifiers delivering particularly robust results on some sites, and less so on others. For example, the ResNet50-SVM combination achieved 97.76% accuracy on the Stanford site, while other sites showed different results. The results of this study highlight the importance of choosing the right feature extractors and classifiers according to the particularities of the data to optimize results. ResNet50-based models and SVM or XGBoost classifiers are particularly recommended for ASD classification, offering a promising avenue for improving ASD diagnosis from fMRI data. 
 Infrared domain adaptation with zero-shot quantizationBurak Sevsay,Erdem AkagündüzShow abstract 
 Quantization is one of the most popular techniques for reducing computation time and shrinking model size. However, ensuring the accuracy of quantized models typically involves calibration using training data, which may be inaccessible due to privacy concerns. In such cases, zero-shot quantization, a technique that relies on pretrained models and statistical information without the need for specific training data, becomes valuable. Exploring zero-shot quantization in the infrared domain is important due to the prevalence of infrared imaging in sensitive fields like medical and security applications. In this work, we demonstrate how to apply zero-shot quantization to an object detection model retrained with thermal imagery. We use batch normalization statistics of the model to distill data for calibration. RGB image-trained models and thermal image-trained models are compared in the context of zero-shot quantization. Our investigation focuses on the contributions of mean and standard deviation statistics to zero-shot quantization performance. Additionally, we compare zero-shot quantization with post-training quantization on a thermal dataset. We demonstrated that zero-shot quantization successfully generates data that represents the training dataset for the quantization of object detection models. Our results indicate that our zero-shot quantization framework is effective in the absence of training data and is well-suited for the infrared domain. 
 TSQ-2024: a categorized dataset of 2D LiDAR images of moving dump trucks in various environment conditionsV. V. Kokhan,I. D. Konyushenko,D. A. Bocharov,et al.Show abstract 
 LiDARs are powerful and common sensors used for complex 3D environment analysis tasks. They are widely utilized as data providers in industrial measurements, robotics and unmanned technologies. Due to physical properties of such sensors they suffer from environmental heterogenity resulting in distortions of obtained measurements because of natural clutter such as cloud dust, snow and fog. Nevertheless the problem of robust clutter filtering algorithms is of significant importance, we highlight the lack of LiDAR measurements datasets containing natural clutter. The current study presents a novel publicly available and expertly annotated 2D LiDAR measurements dataset TSQ-2024 that contains 120 measurements of passing trucks under natural conditions including sand dust clutter data. The proposed dataset was utilized in the task of truck body segmentation. The experiments section provides the performance evaluation of the proposed truck body segmentation algorithm on the TSQ-2024 dataset which is also published along with the data. 
 CS-MAT: cross-shaped window mask-aware transformer for large mask image inpaintingTan-Phat Nguyen,Ngoc-Tuong Le,Ngoc-Thao NguyenShow abstract 
 Addressing the challenge of large mask inpainting in high-resolution images, we propose the Cross-Shaped Window Mask-Aware Transformer (CS-MAT). This model integrates the Cross-Shaped Window (CSWin) attention mechanism with full self-attention, creating a highly efficient transformer architecture that captures local and global information. CSMAT effectively reconstructs missing regions in images, producing high-fidelity and diverse inpainted results. Combining CSWin attention with convolutional layers' detailed image generation capabilities, our experimental results on the CelebAHQ dataset demonstrate that CS-MAT outperforms existing state-of-the-art models, delivering superior image quality with high fidelity and diversity while maintaining computational efficiency. Furthermore, our model achieves faster convergence and outperforms other models, demonstrating its ability to produce visually realistic and coherent outputs efficiently. 
 A new method for manually calculating camera poseNizar KhemiriShow abstract 
 The use of digital images in computer vision and machine learning has become increasingly important in the development of innovative applications in robotics, augmented reality, and notably, aircraft inspection. One of the fundamental challenges in the analysis of digital images is accurately determining the camera pose - its position and orientation at the time of image capture. The aim of this paper is to propose a new and simple method for camera pose calculation, which, unlike all existing methods in the literature, can be performed manually using a new formulation different from all existing methods. The new method is not based on an alignment algorithm or any other algorithms, and requires neither optimization of reprojection errors nor initialization or numerical stability analysis. Contrarily to all solutions in literature, this paper gives the exact relationship, in the form of a system of quadratic equations, between the object points and the corresponding 2D image points, printed in the camera image sensor, in the world reference frame. A calculation using this system of quadratic equations in combination with the generic law of cosines of a tetrahedron, gives the exact 3D location of the camera. The orientation of the camera, in the global reference frame, are calculated manually using basic geometric equations. 
 Improving pain classification using spatio-temporal deep learning approaches with facial expressionsAafaf Ridouan,Amine Bohi,Youssef MourchidShow abstract 
 Pain management and severity detection are crucial for effective treatment, yet traditional self-reporting methods are subjective and may be unsuitable for non-verbal individuals (people with limited speaking skills). To address this limitation, we explore automated pain detection using facial expressions. Our study leverages deep learning techniques to improve pain assessment by analyzing facial images from the Pain Emotion Faces Database (PEMF). We propose two novel approaches1: (1) a hybrid ConvNeXt model combined with Long Short-Term Memory (LSTM) blocks to analyze video frames and predict pain presence, and (2) a Spatio-Temporal Graph Convolution Network (STGCN) integrated with LSTM to process landmarks from facial images for pain detection. Our work represents the first use of the PEMF dataset for binary pain classification and demonstrates the effectiveness of these models through extensive experimentation. The results highlight the potential of combining spatial and temporal features for enhanced pain detection, offering a promising advancement in objective pain assessment methodologies. 
 Lightweight adaptive learning algorithm for energy-latency tradeoff in IoMT-enabled edge computingRahul Yadav,Muhammad Shafiq,Mohit Kumar,et al.Show abstract 
 Smart Healthcare is witnessing widespread implementation of Internet-of-Medical-Things (IoMT) devices. These devices play a crucial role in collecting vast amounts of data from various smart healthcare applications, which are then processed to facilitate informed decision-making. Edge computing has emerged as a valuable platform offering computational resources to handle this data collection efficiently. However, a critical challenge arises from the inappropriate and inefficient classical approaches to fair resource allocation in these energy-intensive, short battery life and delay-intolerant portable devices. To address this issue, this paper presents a Lightweight Adaptive Learning Offloading Algorithm designed to optimize energy and latency in an edge computing environment. The proposed algorithm formulates the problem as a combined minimization of latency and energy costs while satisfying constraints related to limited battery capacity and service latency deadlines. To achieve this, the algorithm employs the adaptation parameters (α, β, γ), and the tradeoff target (C′) is configured based on the specific task requirements. Experimental results demonstrate the advantages of the proposed scheme, including energy savings and minimized latency in an edge environment. To validate the effectiveness of the proposed scheme under a realistic scenario using the iFogSim2 simulator. 
 Diagnosing ADHD with 1D-CNN: efficient analysis of fMRI dataQurat Ul Ain,Soyiba JawedShow abstract 
 Attention Deficit Hyperactivity Disorder (ADHD) is a type of neurodevelopmental disease affecting the mental health of children and adults. So accurate diagnosis as early as possible is essential. Several conventional diagnostic methodologies, along with machine learning and deep learning-based methodologies, have been devised. Nevertheless, there are still certain deficiencies. Therefore, this study addresses the need for precise and timely detection of ADHD, emphasizing the shortcomings of existing applications. Utilizing resting-state functional MRIs, this study proposes a method based on a 1-dimensional Convolution Neural Network (1D-CNN) for the diagnosis of ADHD. The dataset employed in this study consists of 192 subjects including 78 ADHD patients and 114 healthy controls. The independent components have been extracted which treated as feature channels using Independent Component Analysis and then passed to 1D-CNN for classification. Experimental results reveal that the proposed model achieves 80.21 % accuracy, 84.21% precision, 82.76% recall, 0.90 AUC, and 3709.12 seconds execution time. The results of this model have been also compared with other methods which indicate that the proposed model outperforms them in terms of all evaluation metrics. 
 SVI: stable video index for video stabilization quality assessmentE. I. Dudenko,M. A. Pavlova,D. A. BocharovShow abstract 
 This paper focuses on objective methods for evaluating the quality of video stabilization using virtual trajectory data. A variety of stable video methods exist that are sensitive to different properties of stabilization results, thus it is common to evaluate the quality using a number of different measures. Comparing stabilization quality for different methods using a number of measures is complicated, so an integral scalar quality measure is preferred, but no such methods exist yet. In this paper a novel stable video index (SVI) is presented. The SVI provides a scalar measure from 0 to 1 and is based on properties of a stable virtual trajectory: mean squared acceleration and deviation from motion trend and also considers a crop budget utilized by the stabilization algorithm. Human studies conducted using 8 videos of different categories and 24 respondents show that the SVI is consistent with subjective assessment. Comparison of the SVI with known objective quality assessment methods demonstrates better consistency of the SVI with subjective assessments. 
 Image-based intelligent detection technology and application 
 A black-box technique to mitigate gender bias for generative AISarel Cohen,Raid SaabniShow abstract 
 Through the generation of visuals that are both creative and realistic, generative artificial intelligence makes a significant contribution to the education of today. On the other hand, prejudices that are inherent in the training data can cause these models to perpetuate gender stereotypes, particularly when it comes to the creation of images associated with occupations that do not include explicit gender advice. These kinds of prejudices not only impede diversity but also run the risk of strengthening gender stereotypes that already exist. This work offers an automated black-box strategy that tries to reduce gender bias by employing face detection and gender classification on images generated by generative artificial intelligence. The strategy is effective because it addresses the problem at the input and output levels by altering the prompts used with the AI models. This is because the method is flexible and may be used with a broad variety of generative artificial intelligence models. 
 Electoral symbols and vote detection in paper ballots: a case from Nepal's electionRaju Shrestha,Suraj AcharyaShow abstract 
 This paper investigates the potential of advanced object detection technologies to automate and enhance the accuracy and efficiency of the vote counting process in democratic elections that utilize paper-based ballots with electoral symbols. The study focuses on detecting electoral symbols and votes on paper ballots by utilizing two state-of-the-art object detection models: Faster R-CNN, and YOLO. These models were fine-tuned by training them with the ballot papers created using the dataset prepared from the electoral symbols used in Nepal’s general election to ensure high accuracy and reliability in recognizing and validating votes. The system’s effectiveness was demonstrated through a comparison of the models, highlighting the superior performance of Faster R-CNN in terms of precision, despite its slower processing speed compared to YOLO. 
  
 The results indicate that incorporating object detection technologies into electoral systems can significantly improve efficiency of vote counting process. The study underscores the potential for broader applications of these technologies in promoting transparent and fair elections, especially in countries like Nepal, where traditional paper ballots are still prevalent. This innovative approach ensures a more reliable and efficient electoral process, reducing human error and increasing trust in election outcomes. 
 X-ray anomaly detection in industrial pipelinesDiamantis Rafail Papadam,Christos Papaioannidis,Alexandros Zamioudis,et al.Show abstract 
 As Deep Neural Network (DNN)-based algorithms are improving, pivotal changes are happening towards efficient and effective automation in the field of industrial inspection. In the scope of our project, we analyze x-ray images of metal pipelines to detect the presence of corrosion in a novel way. In our industrial scenario, a drone lands a crawler that is equipped with an x-ray system on top of insulated pipelines to perform X-ray scans which are able to penetrate only the insulation, due to power consumption limitations. In this paper, we use modern unsupervised anomaly detection algorithms to detect the presence of corrosion, yielding quite promising results. Moreover, to compare several state-of-the-art approaches in terms of robustness to image degradation, we simulate two types of degradation that can occur: (i) Poisson Noise, (ii) Motion Blur Deformation. We conclude that the problem we are dealing with can be handled sufficiently well with state-of-the-art approaches, and that in the scenario of image degradation, the most robust algorithms are based on memory banks and teacher-student architectures. 
 A light perspective for 3D object detectionMarcelo Eduardo Pederiva,José Mario De Martino,Alessandro ZimmerShow abstract 
 Comprehending the environment and accurately detecting objects in 3D space are essential for advancing autonomous vehicle technologies. Integrating Camera and LIDAR data has emerged as an effective approach for achieving high accuracy in 3D Object Detection models. However, existing methodologies often rely on heavy, traditional backbones that are computationally demanding. This paper introduces a novel approach that incorporates cutting-edge Deep Learning techniques into the feature extraction process, aiming to create more efficient models without compromising performance. Our model, NextBEV, surpasses established feature extractors like ResNet50 and MobileNetV2. On the KITTI 3D Monocular detection benchmark, NextBEV achieves an accuracy improvement of 2.39%, having less than 10% of the MobileNetV3 parameters. Moreover, we propose changes in LIDAR backbones that decreased the original inference time to 10 ms. Additionally, by fusing these lightweight proposals, we have enhanced the accuracy of the VoxelNet-based model by 2.93% and improved the F1-score of the PointPillar-based model by approximately 20%. Therefore, this work contributes to establishing lightweight and powerful models for individual or fusion techniques, making them more suitable for onboard implementations. 
 Diffusion model uniform manifold filtering for classification of small datasets with underrepresented classes: application to chromosomal aberration microscopy detectionQuentin Tallon,Juan Martinez,Eric Gregoire,et al.Show abstract 
 A frequent problem in biomedical machine learning is the issue of imbalanced classes, especially when datasets are small, which restricts the performance of deep learning methods. To address this issue, generative models are often used to generate additional synthetic data. Specifically, image-to-image models can transform input images to match the characteristics of target images. However, training such models on small datasets can affect the quality of synthetic samples. We propose a new method to filter generative outputs of an image-to-image Brownian Bridge Diffusion Models (BBDMs) using Uniform Manifold Approximation and Projection (UMAP) dimension reduction of a real data classifier’s feature space. We apply this methodology to filter synthetic chromosomal aberrations generated from the blue DAPI-colored channels in the context of cytogenetic Fluorescence In Situ Hybridization (FISH) microscopy. Our method shows that such filtered synthetic data significantly enhance classification performance compared to the state of the art CycleGAN and could potentially be applied to a variety of other generative models. 
 Convolutional neural network and k-nearest neighbors classifier for enhanced detection of multiple sclerosisChayma Hechmi,Dalenda Bouzidi,Olfa JemaiShow abstract 
 Multiple Sclerosis (MS) can be characterized as a chronic and often disabling disease disorder of the Central Nervous System (CNS) which includes the brain and spinal cord. It is an autoimmune disease wherein the immune system mistakenly attacks the myelin that covers nerve fibers. Demyelination occurs as a result of the disease, affecting mostly the white matter of the brain or spinal cord, characterized as lesions, which can be observed on a Magnetic Resonance Image (MRI). However, the manual analysis of these complex and detailed images is often labor-intensive and timeconsuming. While that early and accurate detection of MS is crucial for effective treatment to impact people’s quality of life. In this study, we propose a novel approach to MS detection using a hybrid model combining Convolutional Neural Networks (CNN) and K-Nearest Neighbors (KNN) classifiers. The CNN is employed to automatically extract high-level features from MRI dataset, which are then fed into the KNN classifier for final diagnosis. We used a dataset which contained axial and sagittal MRI scans of patients diagnosed with MS and healthy controls, prospectively acquired from 72 MS patients and 59 healthy subjects. The dataset was divided into four study subsets: 761 MRI scans in MS-Sagittal, 650 MRI scans in MS-Axial, 1014 MRI scans in Control-Sagittal and 1002 in Control-Axial. We have totally 3427 MRI scans of both MS and healthy classes. All images were resized to 224x224 pixels. Subsequently, our proposed CNN architecture was optimized through rigorous experimentation to ensure robust feature extraction, and the KNN classifier was fine-tuned to achieve high diagnostic accuracy. Finally, a fine-tuned KNN with 10-fold cross-validation was deployed for classification of the brain images into MS or healthy classes. Our hybrid model demonstrated an accuracy of 99.62% and a precision of 99.66% in distinguishing MS patients from healthy individuals. 
 MTFL: multi-timescale feature learning for weakly-supervised anomaly detection in surveillance videosYiling Zhang,Erkut Akdag,Egor Bondarev,et al.Show abstract 
 Detection of anomaly events is relevant for public safety and requires a combination of fine-grained motion information and contextual events at variable time-scales. To this end, we propose a Multi-Timescale Feature Learning (MTFL) method to enhance the representation of anomaly features. Short, medium, and long temporal tubelets are employed to extract spatio-temporal video features using a Video Swin Transformer. Experimental results demonstrate that MTFL outperforms state-of-the-art methods on the UCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC. Moreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech and 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended dataset of the UCF-Crime for development and evaluation on a wider range of anomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591 videos in 18 classes with extensive coverage of realistic anomalies. 
 Soiling detection for advanced driver assistance systemsFilip Beránek,Václav Diviš,Ivan GruberShow abstract 
 Soiling detection for automotive cameras is a crucial part of advanced driver assistance systems to make them more robust to external conditions like weather, dust, etc. In this paper, we regard the soiling detection as a semantic segmentation problem. We provide a comprehensive comparison of popular segmentation methods and show their superiority in performance while comparing them to tile-level classification approaches. Moreover, we present an extensive analysis of the Woodscape dataset showing that the original dataset contains a data leakage and imprecise annotations. To address these problems, we create a new data subset, which, despite being much smaller, provides enough information for the segmentation method to reach comparable results in a much shorter time. 
 Approach to lightweighting backbone network models through quantization and Bayesian optimizationJunewoo Choi,Deokwoo LeeShow abstract 
 Image classification models are mainly used as components called backbone networks in object detectors. The backbone network extracts features from the input image to generate a feature map, which the detector then uses to detect objects. The main area where object detection is applied is autonomous driving of vehicles, and the overall process of receiving and processing images must be accurate and fast. Unlike object detection for other purposes such as classification and detection, object detection used in autonomous driving requires high accuracy for reasons to ensure the safety of drivers and pedestrians and at the same time requires a high-speed inference to process input in real-time. To this end, there are cases where the model is lightened in order to improve the inference speed of the deep learning model and reduce the required computing resources. In the case of the object detection model, by replacing the backbone network, the number of parameters of the model can be drastically reduced. In this paper, we propose a method for reducing the weight of the backbone network using the MobileNetV3 model, which is often used as the backbone network of the object detection model. First, among the activation functions used in the model, ReLU is changed to Leaky ReLU to improve the Dying ReLU phenomenon. After that, learning is conducted by optimizing hyperparameters through Bayesian optimization. Finally, the model was lightened through dynamic quantization. The experimental results confirmed that this approach contributed to maintaining the accuracy while reducing the weight of the model. 
 Single object spectrum training for 2D NMR signals detectionHadeel Saad Alghamdi,Alexei Lisitsa,Igor Barsukov,et al.Show abstract 
 Peak picking in two-dimensional Nuclear Magnetic Resonance (NMR) spectra represents a significant research challenge and requires substantial time and effort for effective analysis. While machine learning methods have been introduced for peak detection and segmentation [1], the variable sizes, intensities, and positions of NMR peaks often lead models to inaccurately identify peaks, resulting in a high rate of false detections. To address this issue, we have adopted a novel training strategy called Single Object Spectrum Training. This method focuses on learning from simpler cases of peaks, enhancing the model's ability to accurately identify individual peaks within complex spectra. 
 Detecting computer-generated images by using only real imagesJi Li,Kai WangShow abstract 
 This paper presents a simple yet effective method to detect fake synthetic images generated by recent deep generative models, the so-called deepfakes. Unlike existing methods that require a relatively large number of real and fake training images, our method follows a novel idea of using only real images during the training phase. Our proposal is to construct proxy negative training samples, representing fake images, by applying an appropriate transformation on the real images in the training set. The training of our detector leverages the popular CLIP model as well as a center loss to encourage clustering of real images, with the aim of obtaining discriminative features for the classification of real and fake images. The proposed forensic detector is conceptually simple and data-efficient, i.e., it can be trained by using a small amount of only 4K real images. Experimental results and comparisons show the effectiveness of our method in terms of generalization capability to detect fake images generated by various deep generative models. 
 An improved 3D skeletons UP-Fall dataset: enhancing data quality for efficient impact fall detectionTresor Y. Koffi,Youssef Mourchid,Mohammed Hindawi,et al.Show abstract 
 Detecting impact where an individual makes contact with the ground within a fall event is crucial in fall detection systems, particularly for elderly care where prompt intervention can prevent serious injuries. The UP-Fall dataset, a key resource in fall detection research, has proven valuable but suffers from limitations in data accuracy and comprehensiveness. These limitations cause confusion in distinguishing between non-impact events, such as sliding, and real falls with impact, where the person actually hits the ground. This confusion compromises the effectiveness of current fall detection systems. This study presents enhancements to the UP-Fall dataset aiming at improving it for impact fall detection by incorporating 3D skeleton data. Our preprocessing techniques ensure high data accuracy and comprehensiveness, enabling a more reliable impact fall detection. Extensive experiments were conducted using various machine learning and deep learning algorithms to benchmark the improved 3D skeletons dataset. The results demonstrate substantial improvements in the performance of fall detection models trained on the enhanced dataset. This contribution aims to enhance the safety and well-being of the elderly population at risk. 
 Machine Vision and Measurement Based on Machine Learning 
 EEPPR: event-based estimation of periodic phenomena rate using correlation in 3DJakub Kolář,Radim Špetlík,Jiří MatasShow abstract 
 We present a novel method for measuring the rate of periodic phenomena (e.g., rotation, flicker, and vibration), by an event camera, a device asynchronously reporting brightness changes at independently operating pixels with high temporal resolution. The approach assumes that for a periodic phenomenon, a highly similar set of events is generated within a spatio-temporal window at a time difference corresponding to its period. The sets of similar events are detected by a correlation in the spatio-temporal event stream space. The proposed method, EEPPR, is evaluated on a dataset of 12 sequences of periodic phenomena,i.e.flashing light and vibration, and periodic motion,e.g., rotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 – 120 000 RPM). EEPPR significantly outperforms published methods on this dataset, achieving a mean relative error of 0.1% setting new state of the art. The dataset and codes are publicly available on GitHub. 
 Graph convolutional neural networks for skull strippingMaria PopaShow abstract 
 Brain disorders are a global concern, affecting individuals with various cognitive challenges. Accurate brain segmentation plays a critical role in treatment, underscoring the urgent need for automated systems capable of precise segmentation. Skull stripping, in particular, is a challenging task due to potential brain abnormalities. While unsupervised brain extraction methods are commonly used as preprocessing steps in various segmentation tasks, supervised models, including deep learning approaches, have also been explored. 
  
 Graph Convolutional Neural Networks (GCNNs) have shown promising results in addressing complex neurological problems, such as brain connectivity and EEG-based emotion recognition. In this paper, we propose the use of GCNNs for skull stripping as a pioneering experiment and analyze the challenges associated with this application. The method is evaluated on three datasets, with both numerical and visual results presented. 
 Blur patch classification approach to single-image depth estimationHuijun Kim,Deokwoo LeeShow abstract 
 Depth information is useful in many image processing and computer vision applications, but in photography, depth information is lost in the process of projecting a real-world scene onto a 2D plane. Extracting depth information from such images is a challenging task. In this paper, we propose a method to train a deep neural network to classify an image patch (16x16 in size) into 15 levels based on the level of blur. Blur is related to the distance between the focal plane and the object. The input image is shifted using a sliding window technique at 8 pixel intervals and the trained blur classifier evaluates each blur level. The obtained blur maps are subjected to a refinement process to quantitatively assess their accuracy and impact on the final result, and the final blur maps are compared with the labels of the actual input data to estimate the depth map. The proposed method demonstrates that depth information can be successfully extracted from a single image by classifying the focus levels. 
 href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13517/135170V/Ablation-study-for-multicamera-vehicle-tracking-using-CityFlow-dataset/10.1117/12.3055212.full" - Ablation study for multicamera vehicle tracking using CityFlow dataset
Yuqiang Lin,Sam Lockyer,Adrian Evans,et al.Show abstract 
 The wide range of potential real-world applications (e.g. smart city, traffic management, crash detection) for the Multi- Camera Vehicle Tracking (MCVT) problem makes it a worthwhile research topic in the computer vision field. In general, there are two approaches to address the MCVT problem: the global approach, which processes detections to create unified tracks directly, and the more commonly used two-step hierarchical approach, which involves separate stages for intracamera and inter-camera tracking. Typically, the two-step hierarchical MCVT approach can be further divided into four modules: object detection, feature extraction, single camera tracking and multi camera tracking. Each module plays a distinct role in enhancing the overall effectiveness of MCVT solutions. To date, there has only been limited research thoroughly examining how these modules individually affect the overall tracking performance. This paper presents an ablation study on the MCVT problem as a case study using the CityFlow V2 dataset. Using a benchmark MCVT framework, various state-of-art algorithms for each module have been implemented back-to-back to assess the impact of these algorithms. The effectiveness of these algorithms is assessed through two key metrics: IDF1 score performance and computational complexity. The study provides a comprehensive comparison study to understand the contributions of different algorithms in each module. Among all those modules, automatically generated spatial-temporal constraints maintains the computational efficiency while also contribute a lot on IDF1 score performance which could be the focusing point for future research on real-time real-world application. 
 Enriching visual information in images through fusion with thermal imaging in low-light surveillance scenesStijn Leenen,Soumya S. Ghosh,Egor BondarevShow abstract 
 Multimodal image fusion regards the combination of information from a pair of sensors operating in different segments of the electromagnetic spectrum. In the surveillance of low-light scenes, the visible spectrum alone does not capture a significant amount of information. Thus, fusing the information available from a far infrared (FIR) sensor can boost the final image to create a more complete scene. Existing thermal and visible image fusion methods are poorly optimized for use in surveillance of dark scenes, where contrast and textures are mostly absent, thus producing an image with low perceptual quality. With the introduction of a novel method, we improved the state-of-the-art (SOTA) multimodal image fusion algorithm. Our proposed method is capable of successfully recovering most textures and produces an image with relatively high contrast, sharpness, and less noise, thus improving the perceptual quality of the final image. This is particularly important for the surveillance of dark scenes in premises that require high security. We experimentally validate our method (qualitatively and quantitatively) to show the increased performance of the proposed method. The results of the mean opinion scoring (MOS) in our method show an improvement of 5.42% compared to the next-best method. 
 High-throughput median filtering for large kernel sizes on CUDASamed Yildirim,Cihan TopalShow abstract 
 In this paper, we propose a novel approach to median filtering, a widely employed technique for mitigating specific types of noise in images and signals. While median filtering is effective, its computational demands, especially with larger filter sizes, pose challenges for real-time applications. To address this, we present a highly efficient, optimized, and parallelized median filtering algorithm tailored for CUDA platform. Leveraging a histogram-based method and operating on 8-bit data, our approach outperforms sorting-based alternatives, particularly for larger filter sizes. Quantitative experiments demonstrate substantial performance improvements, with our algorithm achieving speedup up to 20x and 59x compared to the second best GPU-based and CPU-based algorithms for mid and large filter sizes, respectively. This significant enhancement in processing speed makes our algorithm a compelling choice for real-time applications where rapid noise removal is paramount, thereby extending the practical utility of median filtering in various domains. 
 Detection and classification technology of food and plant diseases based on machine vision 
 Leveraging stable-diffusion generated images to enhance vision transformers for plant disease classificationLuiz H. Mormille,Iskandar Salama,Masayasu AtsumiShow abstract 
 Plant disease classification is a critical task in agriculture, aiding in early detection and mitigation of crop losses. In this paper, we propose a novel methodology that combines Stable-Diffusion Models with Vision Transformer (ViT) networks to improve plant disease classification. We first employ Stable-Diffusion Models to generate a large-scale dataset comprising 500K images encompassing 106 species of plants. Subsequently, we utilize these generated images to pre-train a ViT model, leveraging its capacity for capturing global image features. We then fine-tune the pre-trained ViT model on three distinct downstream tasks related to identifying diseases in plants. Our experimental results demonstrate substantial improvements in plant disease classification accuracy compared to conventional pre-training methods. Furthermore, all our models and synthetic dataset are public available. Overall, our findings highlight the potential of integrating Stable- Diffusion Models with ViT networks for advancing plant disease diagnosis in agricultural systems. 
 Tomato disease detection to reduce food wasteObadah Ghizawi,Hamza Sallam,Elena Battini SonmezShow abstract 
 The issue of detecting diseases in tomato leaves plays a major role in agricultural sustainability and food safety. Tomatoes are the second most important fruit crop after potatoes. Currently, almost 90% of edible tomatoes are thrown away, significantly contributing to overall fruit and vegetable waste. This research compares the performance of several Convolutional Neural Network (CNN) with the aim to efficiently detect diseases in tomatoes and reduce food waste. From VGG-16 to EfficientNet-B0, six different architectures have been tested to classify 10 distinct diseases of tomatoes using a subset of the public PlantVillage dataset. The results concluded that the best-performing model is the EfficientNet-B0 architecture, with a 96.04% test accuracy. Future work includes augmenting the dataset with more images using C-GAN and alternative techniques, as well as testing different tricks to improve the current performance. The final aim is to contribute to the design of a successful and robust artificial intelligence tool capable of reducing loss and waste of tomatoes. 
 Image-based quantification of scale loss in fish using machine learning and computer visionThomas Fjorden,Espen B. Høgstedt,Christian Schellewald,et al.Show abstract 
 Ensuring fish welfare is vital for sustainable salmon farming, with skin condition being a key health indicator. Despite stringent regulations, the salmon farming industry is still experiencing high mortality rates, emphasizing the need for innovative solutions to measure and improve fish welfare along with operational efficiency. This paper proposes a system for detecting and quantifying scale loss in fish by leveraging state-of-the-art artificial intelligence methods. The system first detects the fish in an image using a state-of-the-art object detection model. Further, the fish’s skin and scale loss are segmented using two instance segmentation models. Finally, the relative skin area of the scale loss is determined, effectively quantifying the amount of scale loss. To achieve this, machine-learning models were trained with a specialized new dataset. In the test data set, the fish detection model achieved a mean average precision (mAP)50-95 of 87.5% with 24.69 frames per second (FPS) on a NVIDIA 1660Ti GPU. The skin segmentation model achieved a mAP50-95 of 98.0% with 24.15 FPS. The scale segmentation model achieved an F1 score of 76.3% with 5.94 FPS, resulting in an estimated scale loss percentage with a mean square error (MSE) of 0.278 on 10 test images. This resulted in a workflow that analyzes four fish per second on a low-budget GPU with high accuracy. Furthermore, threshold values mapping scale loss percentage to a discrete LAKSVEL welfare score was established, achieving a welfare score accuracy of 85.7% over 140 test images. The test data used to evaluate these models originated from a controlled setup above water. Additional tests were performed using real-world underwater images, understandingly showing that the models were not immediately applicable to the new data. However, the models showed a remarkable ability to be fine-tuned with as few as 18 images, to result in F1 scores of 88.2%, 98.9%, and 61.5% for fish detection, skin segmentation, and scale loss segmentation, respectively. A user-friendly application, ScaleGuard, was also developed to make these advanced models accessible to non-technical personnel. 
 KonvLiNA: integrating Kolmogorov-Arnold Network with linear Nyström attention for feature fusion in crop field detectionYunusa Haruna,Shiyin Qin,Adamu Lawan,et al.Show abstract 
 Crop field detection is a critical component of precision agriculture, essential for optimizing resource allocation and enhancing agricultural productivity. This study introduces KonvLiNA, a novel framework that integrates Convolutional Kolmogorov-Arnold Networks (cKAN) with Nyström attention mechanisms for effective crop field detection. Leveraging KAN adaptive activation functions and the efficiency of Nyström attention in handling large-scale data, KonvLiNA significantly enhances feature extraction, enabling the model to capture intricate patterns in complex agricultural environments. Experimental results on rice crop dataset demonstrate KonvLiNA superiority over state-of-the-art methods, achieving a 0.415 AP and 0.459 AR with the Swin-L backbone, outperforming traditional YOLOv8 by significant margins. Additionally, evaluation on the COCO dataset showcases competitive performance across small, medium, and large objects, highlighting KonvLiNA efficacy in diverse agricultural settings. This work highlights the potential of hybrid KAN and attention mechanisms for advancing precision agriculture through improved crop field detection and management. 
 iiADET-tiny: enhanced real-time rice leaf disease detection on low-power embedded systemsYunusa Haruna,Shiyin Qin,Abdulrahman Hamman Adama Chukkol,et al.Show abstract 
 Object detection (OD) on low-power embedded systems presents challenges due to computational constraints. This study proposes iiADET-tiny, a lightweight OD model customized for the Raspberry Pi 4B and Navio2 flight controller. Inspired by Inception Inspired Attention Network (iiANET), iiADET-tiny achieves 20.21 frames per second on 640x640 pixel images, enhancing efficiency without compromising accuracy for Rice Leaf Disease detection. Methodologically, we optimize network blocks, dilated and group convolutions, Linformer and leverage quantization, sparsity techniques to reduce computational complexity. Experimental results validate iiADET-tiny superior performance over some state-ofthe- art models like MobileNetV3 and YOLOv8n, showcasing its suitability for real-time applications on resourceconstrained platforms. 
 Protecting beehive health: varroa mite detection using gen-AIMatthias Desplanche,Siba Haidar,Antoun Yaacoub,et al.Show abstract 
 Varroa mites pose a significant threat to global apiculture, necessitating efficient and accurate detection methods to safeguard bee populations. This study explores the application of artificial intelligence (AI) techniques for varroa mite detection, utilizing convolutional neural networks (CNNs) and generative adversarial networks (GANs) to enhance dataset quality and model performance. The dataset, comprising high-resolution images of hive debris, was augmented using GANs, increasing its size and variability. The CNN model was trained with a supervised learning approach, employing the Adam optimizer and categorical cross-entropy loss function, achieving an accuracy of 0.991. Our results indicate that while GAN-generated images did not significantly improve model accuracy beyond the best-performing CNN configuration (0.998), they provide a means of generating unlimited data, particularly beneficial in data- constrained environments. 
 High-resolution leaf disease classification using deep learning advancing agricultural technology through innovative approachesKenza Chenni,Lavdie RadaShow abstract 
 Accurate classification of plant diseases is essential for effective agricultural management and crop protection. Leaf diseases pose serious threats to crop growth, leading to significant challenges and financial losses for farmers. This study presents an approach leveraging combined deep learning techniques for the classification of tomato leaf diseases using transfer learning. We evaluated the performance of two widely used convolutional neural network architectures: DenseNet121 and MobileNetV2, exploring the impact of freezing and unfreezing layers during training, resulting in four distinct model configurations. By selectively unfreezing layers, we enhanced fine-tuning, leading to improved model performance. Our experiments revealed that DenseNet121 with unfrozen layers achieved the highest classification accuracy, with a validation accuracy of 99.88% and a test accuracy of 99.45%. These results underscore that the proposed approach not only excels in accuracy but also provides a reliable and precise solution for managing and controlling leaf plant diseases, making it particularly effective for practical agricultural applications. 
 Adversarial machine learning in vision, speech, and text 
 Generating synthetic sign language datasets using conditional generative adversarial networksYehuda Yadid,Sarel Cohen,Raid SaabniShow abstract 
 Approximately 70 million individuals globally utilize sign languages as a means of communication due to their hearing impairment. The study undertaken in sign languages is extensive and fruitful. However, there are over 300 sign languages worldwide, but most research focuses on a single language [1]. Creating AI models for sign language challenges sometimes requires large datasets, which can be difficult to produce. Various research has produced datasets for sign language using various methodologies; nevertheless, they often focus on specific sign languages. Developing hand skeleton templates for sign languages provides a more efficient method than creating numerous instances of distinct signs. By creating a basic framework or structure, it becomes much simpler to utilize generative models, like GANs[2], to generate a wide range of different versions of the framework. These generative models can effectively reproduce and adjust the fundamental structures into many sign language forms, capturing the diversity in hand shapes, orientations, and movements necessary for precise sign representation. The main objective of our research is to develop a conditional generative adversarial network (cGAN) model that can generate hand images based on hand skeletons; this approach not only improves the capacity to generate sign language data on a larger scale, but also guarantees uniformity across different versions of signs. This makes it easier to create sign language recognition systems that are more reliable and flexible. To train this model, we devised a web scraping technique that produced a significant collection of hand photos taken from TED lecture recordings, together with their corresponding skeletons. Our created cGAN-based model allows researchers to generate artificial hand images by employing target skeleton inputs. This enables the creation of extensive datasets for sign language. Our contribution is expected to streamline the exploration of additional sign languages that encounter challenges in collecting datasets. 
 Enhancing visual odometry performance through fine-tuning: a case study with TartanVOShuran ZhaoShow abstract 
 SLAM (Simultaneous Localization and Mapping) algorithms are crucial for autonomous robotic systems, with Visual Odometry (VO) being a key component. Despite advancements through deep learning, many geometric models struggle in dynamic environments and varying lighting conditions. This study targets the generalization and accuracy issues of deep learning-based VO models in diverse scenarios. Deep learning-based VO models require vast datasets and significant computational resources, often underperforming in new environments. To enhance adaptability and accuracy without extensive retraining, we propose fine-tuning the TartanVO model. By developing fine-tuning, data processing, and evaluation scripts, we aim to improve performance on specific trajectories with minimal data. Our results show that targeted fine-tuning enhances prediction accuracy and stability in varied environments, providing a cost-effective way to adapt existing models to new conditions. 
 Multimodal approach for imbalanced document classificationMohammad Minouei,Mohammad Reza Soheili,Didier StrickerShow abstract 
 The issue of data scarcity in deep learning remains a significant, unresolved problem. Many existing works in this domain operate under the assumption of models having access to a comprehensive and balanced dataset that covers all conceivable class conditions. However, real-world scenarios often involve imbalanced and incomplete data, creating considerable challenges. In this study, we address the problem of document image classification in the context of imbalanced data. We employ a strategy that merges effective techniques for managing data imbalance with a multi-modal approach that incorporates both image and text data. The experiments were carried out using a customized version of the RVL-CDIP benchmark, where our approach was compared against other methods. The results demonstrate substantial performance enhancements, including an overall accuracy boost of 13 percent and more than a 40 percent improvement in certain minority classes. Our research highlights the efficacy of tailored methods in overcoming the difficulties of imbalanced document classification. 
 Visible coalitions of neuronal activities in brain-to-text communication via handwritingJohn Bolognino,Sarel Cohen,Eden Bar,et al.Show abstract 
 The brain’s cortex features complex networks composed of many individual neurons [1, 2]. Various studies have revealed that the connectivity among neurons may vary in relation to behavioral events [3–7]. In a recent study, Willett et al. [8] demonstrated decoding of imagined handwriting movements from neural activity in the motor cortex of a paralyzed patient. We analyzed their data* by representing all neurons as raster displays and trained convolutional neural network (CNN) models to classify different brain states as the characters that the subjects imagined. Our binary classification models had an average accuracy of 96%, which we then fine-grained by training a multi-class CNN on all 31 different characters. This achieved a high success rate of 86% accuracy. Finally, we applied Grad-CAM [9] to explore the emergence of spatiotemporal patterns which are likely to be involved in determining which character the subject was imagining. Our results support the notion that dynamic neuronal correlations are involved in encoding the different characters. 
 Local masking meets progressive freezing: crafting efficient vision transformers for self-supervised learningUtku Mert Topçuoğlu,Erdem AkagündüzShow abstract 
 This paper presents an innovative approach to self-supervised learning for Vision Transformers (ViTs), integrating local masked image modeling with progressive layer freezing. This method enhances the efficiency and speed of initial layer training in ViTs. By systematically freezing specific layers at strategic points during training, we reduce computational demands while maintaining learning capabilities. Our approach employs a novel multi-scale reconstruction process that fosters efficient learning in initial layers and enhances semantic comprehension across scales. The results demonstrate a substantial reduction in training time (12.5%) with a minimal impact on model accuracy (decrease in top-1 accuracy by 0.6%). Our method achieves top-1 and top-5 accuracies of 82.6% and 96.2%, respectively, underscoring its potential in scenarios where computational resources and time are critical. 
 Annotation tools for computer vision tasksChristos Moschidis,Eleni Vrochidou,George A. PapakostasShow abstract 
 Common computer vision (CV) tasks include image classification, object detection, segmentation, and recognition. To handle such tasks, machine learning (ML) models for image processing require a great amount of annotated training data. While datasets are expanding in size and variety, annotation becomes demanding, since its quality can severely affect the models’ performance. Thus, several annotation tools have been developed and designated for specific applications and model requirements. This work aims to provide an overview of the most up-to-date annotation tools for computer vision tasks, including 2D and 3D image data and video, comparatively highlighting their advantages and limitations. The appropriateness of each tool for specific tasks is emphasized, providing a reference map for researchers towards determining the annotation tool best tailored to their needs. Future trends in image annotation are also discussed. 
 Trans-APL: transformer model for audio and prior landmark fusion for talking landmark generationXuan-Nam Cao,Quoc-Huy Trinh,Minh-Triet TranShow abstract 
 The generation of talking landmarks from audio is pivotal for advancing talking head generation. This challenge poses a significant concern in landmark generation from audio and holds potential applications in various domains, including virtual assistants, education, and entertainment. However, existing audio-based methods exhibit limitations, such as inconsistencies in generated landmark frames and a lack of emotion features from the speech. In this research, we propose Trans-APL, a foundational approach that addresses these limitations by integrating a fusion of landmark and audio information. Additionally, we introduce the Conv-Attention module, a combination of the Convolution layer with the Attention mechanism designed to capture features from both low and high frequencies. This capability aids in capturing emotion information from the audio. Our extensive experimental and visualization results demonstrate the enhancement of talking landmark generation by adapting our methods. Consequently, our approach yields competitive results compared to state-of-the-art methods and significantly contributes to the advancement of realistic talking head motion. 
 Using autoencoders to improve classification of online Arabic handwriting: addressing the homogeneity problem in dataHasanien Ali Talib Alothman,Safa Ameur,Wafa Lejmi,et al.Show abstract 
 The problem of heterogeneous datasets is considered one of the major challenges affecting deep learning efficiency, especially for the available online Arabic handwriting datasets. Most of the traditional methods used to enhance and manipulate dimensions do not amount to providing new data mimic real-world data. In this research, a novel method is presented using effective autoencoder networks to solve the problem of heterogeneous data. This is done by creating new data to fill the shortage of data. Autoencoder networks are a type of artificial neural network that can learn to represent data efficiently, which helps improve performance in classification tasks. It is also explained in detail how to perform data pre-processing, including the necessary steps to select and prepare the data that needs to be regenerated to ensure effective performance of autoencoder networks. The ADAB online Arabic handwriting dataset is used to evaluate the performance of our method by training and reconstructing input data using unsupervised learning. The autoencoder can recognize underlying patterns and create data more effectively. The experimental results show that using autoencoders reduces the effect of heterogeneity in the data and improves the accuracy of the classification models used. 
 Intelligent image recognition and application technology 
 E-Gait: enhanced efficient graph convolution network for gait recognitionNeelma Naz,Saim Rasheed,Sara Ali,et al.Show abstract 
 Gait recognition is a promising biometric technology because it can recognize individuals based on their walking patterns, even from a distance. Traditional methods used for gait recognition have relied on extracting gait features from a person’s silhouette sequence. While silhouette-based methods have been effective, they come with several limitations. Silhouette images often lack detailed spatial information or might include irrelevant visual information unrelated to gait, which can affect identification accuracy and introduce system vulnerabilities. In contrast, model-based methods, specifically those leveraging advances in human pose estimation, have gained traction recently. In this paper, we propose E-Gait, which combines individual’s pose information with an Efficient Graph Convolutional Network (GCN). The proposed method uses separable convolution layers based architecture to extract effective gait features . We also use a spatiotemporal joint attention mechanism that directs the model to focus on the most significant joints, thereby learning efficient spatiotemporal patterns. Extensive experimentation on a popular gait dataset Casia-B demonstrates significant performance improvements of 5.11% and 1.05% in terms of average rank-1 accuracy over state-of-the-art (SOTA) posebased methods. Additionally, we provide visualizations of activated joints to illustrate the efficacy of our proposed model in deriving gait specific features. 
 Optical braille recognition of uncontracted unified English and Filipino brailleMiguel Baliog,Angel Lopez,Maria Franchesca Lopez,et al.Show abstract 
 The importance of educational guidance at home has been emphasized since the pandemic. However, for visually impaired individuals, parents and guardians face challenges due to their unfamiliarity with the braille system. As a result, braille modules provided by the teachers are neglected. Leveraging machine learning object detection and classification models, optical braille recognition (OBR) systems could aid in alleviating this communication gap. This study focuses on training an object detection model for an end-to-end OBR. The dataset used is composed of some publicly available datasets, and the braille modules from the Philippine National School for the Blind (PNSB). The PNSB dataset is composed of images taken using a regular mobile phone to simulate the real-world scenario. These are annotated manually due to the absence of transliterated pages. The OBR model is composed of two models: cell segmentation, and cell recognition. These models are trained separately using YOLOv8. The cell segmentation model is trained on the TapVision dataset and the PNSB dataset, while the cell recognition is trained on the AI4SocialGood dataset and PNSB dataset. The discussion delves into the nuances of the performance of cell segmentation, cell recognition, and end-to-end model, highlighting considerations for improvements. Despite challenges, the study underscores the potential of machine learning object segmentation and classification to enhance Braille transliteration, contributing to the broader goal of fostering inclusive education for visually impaired individuals. 
 3D action recognition with motion encoded RGB images using deep learningSaqib Hayat,Mazhar Hussain,Paolo SodaShow abstract 
 In this paper, we develop a novel and efficient scheme for 3D action recognition using deep learning. First, we generate a 3D normalized pose space by eliminating the translational and orientation information from the 3D motion data to avoid ambiguities and complexities. Then, we propose a novel method to encode the spatio-temporal motion into a 2D compact texture RGB image that encapsulates the 3-dimensional information of motion. Furthermore, we employ a Deep Convolutional Neural Network (CNN) to extract view-invariant fixed-size and discriminative features from transformed and encoded motion images. Next, we compare these features to the nearest motion category using the k-Nearest-Neighbor (kNN) similarity search, which enables efficient motion retrieval. The proposed model efficiently transfers the knowledge from seen to unseen views by directly matching the 3-dimensional motion-based features. Moreover, this technique has the advantage of tolerance towards the variance in actor movement speed, as different actors can perform the same motion in various lengths. In addition, the descriptive features are processed by Support Vector Machine (SVM) to learn a classification model. Finally, we evaluated our proposed technique intensively on two benchmark and publicly available datasets, including HDM05 and UTD-MHAD motion capture datasets. The experimental results reveal that our proposed scheme outperformed existing state-of-the-art 3D action recognition approaches on benchmark motion capture datasets. 
 A survey on the online Arabic handwriting recognition: challenges, datasets, and future directionsHasanien Ali Talib Alothman,Wafa Lejmi,Mohamed Ali MahjoubShow abstract 
 The increasing demand for effective methods to recognize digital handwritten Arabic text has led to the creation of numerous online Arabic manuscript datasets to validate handwriting recognition systems. This tendency has been intensified, especially after the increasing use of devices dependent on digital pen writing, which requires continuous development to achieve an advanced stage of discrimination. This research aims to resolve the confusion problem in choosing a convenient open-source dataset that researchers can use to develop and examine handwriting recognition methods. This paper provides inclusive benchmarking of available and state-of-the-art online Arabic manuscript datasets and their unique features. It also provides a brief overview of the preprocessing and feature extraction methods used in the most important research. Research findings include identifying the most efficient databases and providing recommendations for future use. 
 MobileViTv2-MLP: Arabic traffic sign recognition with enhanced lightweight vision transformersA. Alqahtani,E. Alqaysi,W. Alsarhani,et al.Show abstract 
 Traffic Sign Recognition (TSR) plays a pivotal role in advanced driver-assistance systems (ADAS) and autonomous vehicles, serving as a fundamental element for safe and efficient road navigation. Due to a diverse array of factors such as variations in sign appearance, fluctuating lighting conditions, occlusions, and environmental influences, the task of TSR poses significant challenges. These challenges have been addressed with advanced deep learning techniques, particularly with Convolutional Neural Networks (CNNs) and other architectures proficient in handling visual data. While such approaches have shown promise, they are often limited in their ability to effectively manage the aforementioned challenges. To overcome these obstacles, we propose a novel approach called the MobileViTv2-MLP method. This approach aims to develop a resilient model specifically tailored to the task of identifying and categorizing traffic signs encountered during driving. Our methodology involves training the model on an extensive dataset comprising diverse representations of traffic signs under varying conditions, ensuring its adaptability and reliability in real-world scenarios. Through experimentation and evaluation, our refined approach has demonstrated exceptional accuracy, achieving a remarkable 99.75%. This outcome underscores the robustness and effectiveness of our proposed method in addressing the challenges inherent in TSR, thereby contributing to the advancement of intelligent transportation systems. 
 Beehive state and events recognition through sound analysis using TinyMLHadi Al Zein,Antoun Yaacoub,Siba HaidarShow abstract 
 The optimization of neural networks software and hardware techniques has enabled the integration of complex networks onto small devices, giving rise to the TinyML paradigm. This study focuses on applying TinyML to the recognition of beehive states, treating it as an audio event detection problem. Identifying crucial audio features for hive events, we present a machine learning pipeline covering model training, multi-stage evaluation, and conversion for deployment on the Arduino Nano BLE Sense 33. Achieving up to 99% accuracy, our approach facilitates early detection of anomalous behaviors in beehives, crucial for wildlife preservation. Automation in hyperparameter tuning, particularly in audio-related challenges, enhances efficiency in feature extraction, showcasing the potential of TinyML in solving realworld problems with minimal resources and power consumption. 
 Black-box adversarial defense for enhancing robustness in speaker recognition systems with multimodel consensusUmang Patel,Avik Hati,Shruti BhilareShow abstract 
 Speaker recognition systems (SRS) are increasingly deployed in various applications, but their susceptibility to adversarial attacks poses significant security challenges. While defense mechanisms do exist, they often come with a trade-off between decreased system accuracy and substantial computations, making them impractical for real-world applications. Addressing this issue, this study proposes a novel defense strategy that enhances the robustness of SRS against adversarial examples while maintaining high accuracy and computational efficiency. Inspired by multi-version programming (MVP), our multimodel defense approach equips a target model with multiple auxiliary models, all trained on the same dataset. Our method operates in a black-box setting, requiring no prior knowledge of the attack methods, adversarial example generation, or target model parameters. It employs a two-step process, comparing outputs from the target and auxiliary models to classify input audio as clean or adversarial, followed by a voting mechanism among the auxiliary models for accurate detection. Experimental results demonstrate the superior performance of the proposed approach over existing single-model defense systems, achieving a detection accuracy as high as 99.52%, with the best-case detection accuracy of 99.12% for clean examples and 99.92% for adversarial examples. This approach addresses the limitations of previous strategies by offering a practical, scalable solution applicable to various SRS architectures, advancing the development of secure and reliable SRS technologies for diverse real-world applications. 
 ABOUT 
 Mission 
 Leadership 
 Committees 
 History 
 Policies and Reporting 
 Jobs at SPIE 
 Donate to SPIE 
 RESOURCES 
 Join SPIE 
 Publications 
 Public Policy 
 SPIE Brand and Logos 
 SPIE Press Releases 
 SPIE Profiles 
 href="/documents/MediaKit/2025/SPIE-Media-Kit.pdf" - SPIE Media Kit 
 HELP 
 Contact + Help FAQs 
 Report an Incident 
 href="https://spie.org/communication/updates-and-newsletters" - Email Preferences 
 SUBSCRIBE TO OUR EMAILS 
 Receive only the information you want 
 Sign Up 
 Stay Connected | Get the App 
 © 2025 SPIE 
 SPIE Digital Library|SPIE Career Center|optics.org|Privacy PolicyTop of page

3. Website of ICMV_3: https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13072/3023347/EANet-enhanced-attribute-based-RGBT-tracker-network/10.1117/12.3023347.short
Website information of ICMV_3:

Sign InView CartHelpORGANIZATIONAL 
 Sign in with credentials provided by your organization.Organizational UsernameOrganizational PasswordShow/Hide PasswordINSTITUTIONAL 
 Select your institution to access the SPIE Digital Library.href="/Account/institutionalsignin?redirect=https%3a%2f%2fwww.spiedigitallibrary.org%2fconference-proceedings-of-spie%2f13072%2f3023347%2fEANet-enhanced-attribute-based-RGBT-tracker-network%2f10.1117%2f12.3023347.short" - SELECT YOUR INSTITUTION
PERSONAL 
 Sign in with your personal SPIE Account.href="/Account/OauthLoginButtonClick?redirect=https%3a%2f%2fwww.spiedigitallibrary.org%2fconference-proceedings-of-spie%2f13072%2f3023347%2fEANet-enhanced-attribute-based-RGBT-tracker-network%2f10.1117%2f12.3023347.short" - PERSONAL SIGN IN
No SPIE Account?Create one;CONFERENCE PROCEEDINGS 
 href="/conference-proceedings-of-spie" - Papers 
 Presentations 
 Journals | Advanced Photonics 
 Advanced Photonics Nexus 
 Biophotonics Discovery 
 Journal of Applied Remote Sensing 
 Journal of Astronomical Telescopes, Instruments, and Systems 
 Journal of Biomedical Optics 
 Journal of Electronic Imaging 
 Journal of Medical Imaging 
 Journal of Micro/Nanopatterning, Materials, and Metrology 
 Journal of Nanophotonics 
 Journal of Optical Microsystems 
 Journal of Photonics for Energy 
 Neurophotonics 
 Optical Engineering 
 Photonics Insights 
 Ebooks 
 Advanced Search>Home>Proceedings>Volume 13072>Article3 April 2024EANet: enhanced attribute-based RGBT tracker networkAbbas Türkoğlu,Erdem Akagündüzhref="" - Author Affiliations +
Abbas Türkoğlu,1Erdem Akagündüz1 
  
 1Middle East Technical University (Turkey) 
 href="/conference-proceedings-of-spie/13072.toc" - Proceedings Volume 13072, Sixteenth International Conference on Machine Vision (ICMV 2023);
1307218 (2024)https://doi.org/10.1117/12.3023347 
 Event: Sixteenth International Conference on Machine Vision (ICMV 2023), 2023, Yerevan, ArmeniaARTICLE 
 FIGURES & TABLES 
 REFERENCES 
 CITED BY 
 DOWNLOAD PAPERSAVE TO MY LIBRARY 
 ORGANIZATIONAL 
 Sign in with credentials provided by your organization.Organizational UsernameOrganizational PasswordShow/Hide PasswordINSTITUTIONAL 
 Select your institution to access the SPIE Digital Library.href="/Account/institutionalsignin?redirect=https%3a%2f%2fwww.spiedigitallibrary.org%2fconference-proceedings-of-spie%2f13072%2f3023347%2fEANet-enhanced-attribute-based-RGBT-tracker-network%2f10.1117%2f12.3023347.short" - SELECT YOUR INSTITUTION
PERSONAL 
 Sign in with your SPIE account to access your personal subscriptions or to use specific features such as save to my library, sign up for alerts, save searches, etc.href="/Account/OauthLoginButtonClick?redirect=https%3a%2f%2fwww.spiedigitallibrary.org%2fconference-proceedings-of-spie%2f13072%2f3023347%2fEANet-enhanced-attribute-based-RGBT-tracker-network%2f10.1117%2f12.3023347.short" - PERSONAL SIGN IN
No SPIE Account?Create one;PURCHASE THIS CONTENTSUBSCRIBE TO DIGITAL LIBRARY50 downloads per 1-year subscriptionMembers: $195Non-members: $335ADD TO CART25 downloads per 1-year subscriptionMembers: $145Non-members: $250ADD TO CARTPURCHASE SINGLE ARTICLEIncludes PDF, HTML & Video, when availableMembers:Non-members:ADD TO CARTThis will count as one of your downloads. 
 You will have access to both the presentation and article (if available). 
 DOWNLOAD NOWThis content is available for download via your institution's subscription. To access this item, please sign in to your personal account. 
 Email or Usernamehref="https://spie.org/account/forgotusername?redir=https%3a%2f%2fwww.spiedigitallibrary.org%2fconference-proceedings-of-spie%2f13072%2f3023347%2fEANet-enhanced-attribute-based-RGBT-tracker-network%2f10.1117%2f12.3023347.short" - Forgot your username?
Passwordhref="https://spie.org/account/forgotpassword?redir=https%3a%2f%2fwww.spiedigitallibrary.org%2fconference-proceedings-of-spie%2f13072%2f3023347%2fEANet-enhanced-attribute-based-RGBT-tracker-network%2f10.1117%2f12.3023347.short" - Forgot your password?
ShowKeep me signed inNo SPIE account?Create an account 
 My LibraryYou currently do not have any folders to save your paper to! Create a new folder below. 
 Create New FolderSAVE >Folder NameFolder DescriptionSAVEAbstract 
 Tracking objects can be a difficult task in computer vision, especially when faced with challenges such as occlusion, changes in lighting, and motion blur. Recent advances in deep learning have shown promise in challenging these conditions. However, most deep learning-based object trackers only use visible band (RGB) images. Thermal infrared electromagnetic waves (TIR) can provide additional information about an object, including its temperature, when faced with challenging conditions. We propose a deep learning-based image tracking approach that fuses RGB and thermal images (RGBT). The proposed model consists of two main components: a feature extractor and a tracker. The feature extractor encodes deep features from both the RGB and the TIR images. The tracker then uses these features to track the object using an enhanced attribute-based architecture. We propose a fusion of attribute-specific feature selection with an aggregation module. The proposed methods are evaluated on the RGBT234 [1] and LasHeR [2] datasets, which are the most widely used RGBT object-tracking datasets in the literature. The results show that the proposed system outperforms state-of-the-art RGBT object trackers on these datasets, with a relatively smaller number of parameters. 
 (2024) Published by SPIE. Downloading of the abstract is permitted for personal use only.CitationDownload CitationAbbas TürkoğluandErdem Akagündüz"EANet: enhanced attribute-based RGBT tracker network", Proc. SPIE 13072, Sixteenth International Conference on Machine Vision (ICMV 2023), 1307218 (3 April 2024);https://doi.org/10.1117/12.3023347ACCESS THE FULL ARTICLE 
 ORGANIZATIONAL 
 Sign in with credentials provided by your organization.Organizational UsernameOrganizational PasswordShow/Hide PasswordINSTITUTIONAL 
 Select your institution to access the SPIE Digital Library.href="/Account/institutionalsignin?redirect=https%3a%2f%2fwww.spiedigitallibrary.org%2fconference-proceedings-of-spie%2f13072%2f3023347%2fEANet-enhanced-attribute-based-RGBT-tracker-network%2f10.1117%2f12.3023347.short" - SELECT YOUR INSTITUTION
PERSONAL 
 Sign in with your SPIE account to access your personal subscriptions or to use specific features such as save to my library, sign up for alerts, save searches, etc.href="/Account/OauthLoginButtonClick?redirect=https%3a%2f%2fwww.spiedigitallibrary.org%2fconference-proceedings-of-spie%2f13072%2f3023347%2fEANet-enhanced-attribute-based-RGBT-tracker-network%2f10.1117%2f12.3023347.short" - PERSONAL SIGN IN
No SPIE Account?Create one;PURCHASE THIS CONTENTSUBSCRIBE TO DIGITAL LIBRARY50 downloads per 1-year subscriptionMembers: $195Non-members: $335ADD TO CART25 downloads per 1-year subscriptionMembers: $145Non-members: $250ADD TO CARTPURCHASE SINGLE ARTICLEIncludes PDF, HTML & Video, when availableMembers:$17.00Non-members:$21.00ADD TO CARTPROCEEDINGS 
 8 PAGESDOWNLOAD PAPERSAVE TO MY LIBRARYGET CITATION 
 Advertisement 
 Advertisement 
 RIGHTS & PERMISSIONSGet copyright permissionKEYWORDSRGB color modelFeature fusionDetection and tracking algorithmsFeature extractionInfrared radiationLight sources and illuminationMotion blurRELATED CONTENThref="/conference-proceedings-of-spie/12642/126422J/Feature-mutual-reinforcement-learning-and-resampling-for-RGB-T-tracking/10.1117/12.2674770.full" - Feature mutual reinforcement learning and resampling for RGB-T tracking
 
 Proceedings of SPIE (May 02 2023)Design and application of airport face image detection system 
 Proceedings of SPIE (May 16 2024)Detection and feature fusion of trajectory Poisson multi Bernoulli mixture... 
 Proceedings of SPIE (October 24 2024)Prototype transferred diverse features for day night cross domain facial... 
 Proceedings of SPIE (February 13 2025)Gait recognition based on GaitPart and multimodal feature fusion 
 Proceedings of SPIE (December 01 2023)Power equipment target detection algorithm based on improved YOLOV7 
 Proceedings of SPIE (May 22 2024)Novel illumination-normalization method based on region information 
 Proceedings of SPIE (March 01 2005)Subscribe to Digital LibraryReceive Erratum Email AlertErratum Email Alerts notify you when an article has been updated or the paper is withdrawn.VisitMy Accountto manage your email alerts.The alert successfully saved.VisitMy Accountto manage your email alerts.CLOSEThe alert did not successfully save. Please try again later.CLOSEAbbas Türkoğlu, Erdem Akagündüz, "EANet: enhanced attribute-based RGBT tracker network," Proc. SPIE 13072, Sixteenth International Conference on Machine Vision (ICMV 2023), 1307218 (3 April 2024);https://doi.org/10.1117/12.3023347Include:Citation OnlyCitation & AbstractFormat:RISEndNoteBibTexDOWNLOAD CITATIONSite Map 
 Home 
 href="/conference-proceedings-of-spie" - Conference Papers 
 Conference Presentations 
 Journals 
 eBooks 
 About 
 Subscriptions 
 Information for Authors 
 href="/proceedings-authors" - Proceedings Authors 
 href="/journals/journal-authors" - Journal Authors 
 href="https://spie.org/publications/spie-press-books/book-author-information" - eBook Authors 
 Information for Reviewers 
 Reviewer Guidelines 
 href="https://researcher.life/partner/spie" - Reviewer Training Program 
 Information for Librarians 
 Resources 
 Subscriptions 
 Contact & Support 
 TECHNICAL SUPPORT 
 spiedlsupport@spie.org 
 CUSTOMER SERVICE 
 +1 360 676 3290 
 Hours: 
 8:00 am to 5:00 pm PST 
 Help Center|Contact Us 
 Connect 
 SPIE Privacy Policy|Terms of Use© 2025 SPIE 
 CONFERENCE PROCEEDINGS 
 href="/conference-proceedings-of-spie" - Papers 
 Presentations 
 Journals | Advanced Photonics 
 Advanced Photonics Nexus 
 Biophotonics Discovery 
 Journal of Applied Remote Sensing 
 Journal of Astronomical Telescopes, Instruments, and Systems 
 Journal of Biomedical Optics 
 Journal of Electronic Imaging 
 Journal of Medical Imaging 
 Journal of Micro/Nanopatterning, Materials, and Metrology 
 Journal of Nanophotonics 
 Journal of Optical Microsystems 
 Journal of Photonics for Energy 
 Neurophotonics 
 Optical Engineering 
 Photonics Insights 
 Ebooks 
 Help|Advanced Search>Keywords/Phrases 
 Keywords 
 in 
 value="AUTHORNAME" - Author Name
Remove 
 in 
 value="AUTHORNAME" - Author Name
Remove 
 in 
 value="AUTHORNAME" - Author Name
Remove 
 + Add another field 
 Search In: 
 ProceedingsVolumeJournals +VolumeIssuePageAdvanced Photonics 
 Advanced Photonics Nexus 
 Biophotonics Discovery 
 Journal of Applied Remote Sensing 
 Journal of Astronomical Telescopes, Instruments, and Systems 
 Journal of Biomedical Optics 
 Journal of Electronic Imaging 
 Journal of Medical Imaging 
 Journal of Micro/Nanopatterning, Materials, and Metrology 
 Journal of Nanophotonics 
 Journal of Optical Microsystems 
 Journal of Photonics for Energy 
 Neurophotonics 
 Optical Engineering 
 Photonics Insights 
 SPIE Reviews 
 eBooks +Field Guide Series 
 Press Monograph 
 Spotlight 
 Tutorial Text 
 Other Press 
 Publication Years 
 Range 
 <> 
 Single Year 
 Clear Form

4. Website of ICMV_3: https://spie.org/Publications/Proceedings/Volume/13072
Website information of ICMV_3:

This website uses cookies to provide you with a variety of services and to improve the usability of our website. By using the website, you agree to the use of cookies in accordance with ourPrivacy Policy.Close 
 Events | Events | Events Home 
 Event Resources | Become an Exhibitor 
 Get the SPIE App 
 Event Policies 
 Official Contractors 
 Press Registration 
 Events Calendar | All Upcoming Events 
 Conferences 
 Exhibitions 
 SPIE.Online | Upcoming Webinars 
 Recorded Webinars 
 Featured Exhibitions | Photonics West 
 AR | VR | MR 
 Advanced Lithography + Patterning 
 Defense + Commercial Sensing 
 Optics + Photonics 
 Sensors + Imaging 
 href="//spie.org/conferences-and-exhibitions/event-resources" - For Authors + Volunteers | Manuscript Guidelines and Policies 
 href="//spie.org/conferences-and-exhibitions/for-authors-and-presenters/poster-pdf-guidelines" - Poster Presentation Guidelines 
 Event Volunteer Guidelines 
 Conference Chair Resources 
 Contact SPIE Program Coordinators 
 SPIE Defense + Commercial Sensing | 13 - 17 April 2025 in Orlando, Florida | Learn More | Home 
 Events Home 
 Event Resources | Event Resources | Events 
 Event Resources 
 Become an Exhibitor 
 Get the SPIE App 
 Event Policies 
 Official Contractors 
 Press Registration 
 Events Calendar | Events Calendar | Events 
 Events Calendar 
 All Upcoming Events 
 Conferences 
 Exhibitions 
 SPIE.Online | SPIE.Online | Events 
 SPIE.Online 
 Upcoming Webinars 
 Recorded Webinars 
 Featured Exhibitions | Featured Exhibitions | Events 
 Featured Exhibitions 
 Photonics West 
 AR | VR | MR 
 Advanced Lithography + Patterning 
 Defense + Commercial Sensing 
 Optics + Photonics 
 Sensors + Imaging 
 For Authors + Volunteers | href="//spie.org/conferences-and-exhibitions/event-resources" - For Authors + Volunteers | Events 
 href="//spie.org/conferences-and-exhibitions/event-resources" - For Authors + Volunteers 
 Manuscript Guidelines and Policies 
 href="//spie.org/conferences-and-exhibitions/for-authors-and-presenters/poster-pdf-guidelines" - Poster Presentation Guidelines 
 Event Volunteer Guidelines 
 Conference Chair Resources 
 Contact SPIE Program Coordinators 
 SPIE Defense + Commercial Sensing 
 13 - 17 April 2025 in Orlando, Florida 
 Learn More 
 Publications | Publications | Publications Home 
 Publication Resources | Terms of Use 
 Reprint Permission 
 Contact SPIE Publications 
 SPIE Digital Library 
 SPIE Bookstore | Books 
 Proceedings 
 Apparel and Gifts 
 SPIE Journals | Institutional Subscriptions 
 Individual Subscriptions 
 Conference Proceedings | Conference Content Publication Services 
 SPIE Press Books | href="//spie.org/publications/spie-press-books/book-author-information" - Book Author Information 
 Book Manuscript Guidelines 
 Submit a Book Proposal 
 href="//spie.org/publications/spie-press-books/about-spotlights-and-author-calls" - Spotlights Call for Authors 
 href="//spie.org/publications/spie-press-books/field-guide-author-guidelines" - Field Guide Author Guidelines 
 New Books from SPIE Press | Titles include "Optics using Python" and "Designing Optics Using Zemax OpticStudio" | Visit the Bookstore | Home 
 Publications Home 
 Publication Resources | Publication Resources | Publications 
 Publication Resources 
 Terms of Use 
 Reprint Permission 
 Contact SPIE Publications 
 SPIE Digital Library 
 SPIE Bookstore | SPIE Bookstore | Publications 
 SPIE Bookstore 
 Books 
 Proceedings 
 Apparel and Gifts 
 SPIE Journals | SPIE Journals | Publications 
 SPIE Journals 
 Institutional Subscriptions 
 Individual Subscriptions 
 Conference Proceedings | Conference Proceedings | Publications 
 Conference Proceedings 
 Conference Content Publication Services 
 SPIE Press Books | SPIE Press Books | Publications 
 SPIE Press Books 
 href="//spie.org/publications/spie-press-books/book-author-information" - Book Author Information 
 Book Manuscript Guidelines 
 Submit a Book Proposal 
 href="//spie.org/publications/spie-press-books/about-spotlights-and-author-calls" - Spotlights Call for Authors 
 href="//spie.org/publications/spie-press-books/field-guide-author-guidelines" - Field Guide Author Guidelines 
 New Books from SPIE Press 
 Titles include "Optics using Python" and "Designing Optics Using Zemax OpticStudio" 
 Visit the Bookstore 
 Membership | Membership | Membership Home 
 Member Benefits 
 Join or Renew 
 SPIE Fellows | List of all SPIE Fellows 
 Nominate a Fellow 
 SPIE Senior Members | List of all Senior Members 
 Nominate a Senior Member 
 Student Membership | Student Chapters 
 Student Awards 
 Student Resources 
 SPIE Profiles 
 Corporate Membership | Corporate Member Benefits 
 Corporate Member Directory 
 Become an SPIE Member | Join over 25,000 of your friends and colleagues in the largest global optics and photonics professional society. | Join or Renew Today | Home 
 Membership Home 
 Member Benefits 
 Join or Renew 
 SPIE Fellows | SPIE Fellows | Membership 
 SPIE Fellows 
 List of all SPIE Fellows 
 Nominate a Fellow 
 SPIE Senior Members | SPIE Senior Members | Membership 
 SPIE Senior Members 
 List of all Senior Members 
 Nominate a Senior Member 
 Student Membership | Student Membership | Membership 
 Student Membership 
 Student Chapters 
 Student Awards 
 Student Resources 
 SPIE Profiles 
 Corporate Membership | Corporate Membership | Membership 
 Corporate Membership 
 Corporate Member Benefits 
 Corporate Member Directory 
 Become an SPIE Member 
 Join over 25,000 of your friends and colleagues in the largest global optics and photonics professional society. 
 Join or Renew Today 
 Career + Courses | Career + Courses | Career + Courses Home 
 Career Center | Find a Job 
 Post a Job 
 Career Center FAQs 
 SPIE Job Fairs 
 Career Resources 
 Courses | Find a Course 
 Courses at Conferences 
 Online Courses 
 Group Training 
 Education Webinar Series 
 Teach a Course for SPIE 
 Technician Resources | Technician Training Programs 
 Technician Scholarship 
 AMIP Teaching Modules 
 OP-TEC Course Materials 
 SPIE Career Center | Reimagine your career | Browse Job Listings | Home 
 Career + Courses Home 
 Career Center | Career Center | Career + Courses 
 Career Center 
 Find a Job 
 Post a Job 
 Career Center FAQs 
 SPIE Job Fairs 
 Career Resources 
 Courses | Courses | Career + Courses 
 Courses 
 Find a Course 
 Courses at Conferences 
 Online Courses 
 Group Training 
 Education Webinar Series 
 Teach a Course for SPIE 
 Technician Resources | Technician Resources | Career + Courses 
 Technician Resources 
 Technician Training Programs 
 Technician Scholarship 
 AMIP Teaching Modules 
 OP-TEC Course Materials 
 SPIE Career Center 
 Reimagine your career 
 Browse Job Listings 
 Community Support | Community Support | Community Support Home 
 Equity, Diversity, + Inclusion | Family Care Grants 
 EDI Resources 
 EDI Videos 
 Women in Optics 
 SPIE Society Awards | Award Nomination Guide 
 Awards Banquet 
 href="//spie.org/community-support/research-and-program-funding" - Research + Program Funding | href="//spie.org/community-support/research-and-program-funding/spie-endowment-matching-program" - SPIE Endowment Matching Program 
 SPIE-Franz Hillenkamp Postdoctoral Fellowship 
 href="//spie.org/community-support/research-and-program-funding/ibm-spie-hbcu-faculty-accelerator-award-in-quantum-optics-and-photonics" - IBM SPIE HBCU Faculty Accelerator Award 
 Student Funding | Scholarships 
 Conference Support 
 Industry Resources | Become an Exhibitor 
 Global Industry Report 
 Global Salary Report 
 Partners + Industry Clusters 
 Post a Job 
 Education Outreach | Outreach Grants 
 Posters 
 Optipedia 
 Advocacy + Public Policy | CHIPS for America 
 Policy Position Statements 
 Visit + Contact US Congress 
 International Day of Light | IDL Photo Contest 
 IDL Resources 
 Nominations now open for SPIE Fellows | The deadline for applications is 15 September | Learn More | Home 
 Community Support Home 
 Equity, Diversity, + Inclusion | Equity, Diversity, + Inclusion | Community Support 
 Equity, Diversity, + Inclusion 
 Family Care Grants 
 EDI Resources 
 EDI Videos 
 Women in Optics 
 SPIE Society Awards | SPIE Society Awards | Community Support 
 SPIE Society Awards 
 Award Nomination Guide 
 Awards Banquet 
 Research + Program Funding | href="//spie.org/community-support/research-and-program-funding" - Research + Program Funding | Community Support 
 href="//spie.org/community-support/research-and-program-funding" - Research + Program Funding 
 href="//spie.org/community-support/research-and-program-funding/spie-endowment-matching-program" - SPIE Endowment Matching Program 
 SPIE-Franz Hillenkamp Postdoctoral Fellowship 
 href="//spie.org/community-support/research-and-program-funding/ibm-spie-hbcu-faculty-accelerator-award-in-quantum-optics-and-photonics" - IBM SPIE HBCU Faculty Accelerator Award 
 Student Funding | Student Funding | Community Support 
 Student Funding 
 Scholarships 
 Conference Support 
 Industry Resources | Industry Resources | Community Support 
 Industry Resources 
 Become an Exhibitor 
 Global Industry Report 
 Global Salary Report 
 Partners + Industry Clusters 
 Post a Job 
 Education Outreach | Education Outreach | Community Support 
 Education Outreach 
 Outreach Grants 
 Posters 
 Optipedia 
 Advocacy + Public Policy | Advocacy + Public Policy | Community Support 
 Advocacy + Public Policy 
 CHIPS for America 
 Policy Position Statements 
 Visit + Contact US Congress 
 International Day of Light | International Day of Light | Community Support 
 International Day of Light 
 IDL Photo Contest 
 IDL Resources 
 Nominations now open for SPIE Fellows 
 The deadline for applications is 15 September 
 Learn More 
 News | News | News Home | Community News 
 SPIE Event News 
 SPIE Publication News 
 SPIE Press Releases 
 Photonics Focus 
 Optics.org 
 href="https://spie.org/news/photonics-focus/marchapril-2025" - | Latest Issue of Photonics Focus | March/April issue explores the ever-shrinking world of microelectronics | href="https://spie.org/news/photonics-focus/marchapril-2025" - March April 2025 | Home 
 News Home | News Home | News 
 News Home 
 Community News 
 SPIE Event News 
 SPIE Publication News 
 SPIE Press Releases 
 Photonics Focus 
 Optics.org 
 href="https://spie.org/news/photonics-focus/marchapril-2025" - 
Latest Issue of Photonics Focus 
 March/April issue explores the ever-shrinking world of microelectronics 
 href="https://spie.org/news/photonics-focus/marchapril-2025" - March April 2025 
 About | About | About SPIE Home 
 About the Society | Mission and Values 
 Officers and Directors 
 Committees 
 History 
 Past Officers and Directors 
 Bylaws 
 SPIE Brand and Logos 
 Jobs at SPIE 
 Code of Conduct 
 Policies and Reporting 
 Honor your Heroes | SPIE Society Awards are Open for Nominations | Learn More | Home 
 About SPIE Home 
 About the Society | About the Society | About 
 About the Society 
 Mission and Values 
 Officers and Directors 
 Committees 
 History 
 Past Officers and Directors 
 Bylaws 
 SPIE Brand and Logos 
 Jobs at SPIE 
 Code of Conduct 
 Policies and Reporting 
 Honor your Heroes 
 SPIE Society Awards are Open for Nominations 
 Learn More 
 Sign In 
 Cart 
 More | Career + Courses | Career + Courses | Career + Courses Home 
 Career Center | Find a Job 
 Post a Job 
 Career Center FAQs 
 SPIE Job Fairs 
 Career Resources 
 Courses | Find a Course 
 Courses at Conferences 
 Online Courses 
 Group Training 
 Education Webinar Series 
 Teach a Course for SPIE 
 Technician Resources | Technician Training Programs 
 Technician Scholarship 
 AMIP Teaching Modules 
 OP-TEC Course Materials 
 SPIE Career Center | Reimagine your career | Browse Job Listings | Home 
 Career + Courses Home 
 Career Center | Career Center | Career + Courses 
 Career Center 
 Find a Job 
 Post a Job 
 Career Center FAQs 
 SPIE Job Fairs 
 Career Resources 
 Courses | Courses | Career + Courses 
 Courses 
 Find a Course 
 Courses at Conferences 
 Online Courses 
 Group Training 
 Education Webinar Series 
 Teach a Course for SPIE 
 Technician Resources | Technician Resources | Career + Courses 
 Technician Resources 
 Technician Training Programs 
 Technician Scholarship 
 AMIP Teaching Modules 
 OP-TEC Course Materials 
 SPIE Career Center 
 Reimagine your career 
 Browse Job Listings 
 Community Support | Community Support | Community Support Home 
 Equity, Diversity, + Inclusion | Family Care Grants 
 EDI Resources 
 EDI Videos 
 Women in Optics 
 SPIE Society Awards | Award Nomination Guide 
 Awards Banquet 
 href="//spie.org/community-support/research-and-program-funding" - Research + Program Funding | href="//spie.org/community-support/research-and-program-funding/spie-endowment-matching-program" - SPIE Endowment Matching Program 
 SPIE-Franz Hillenkamp Postdoctoral Fellowship 
 href="//spie.org/community-support/research-and-program-funding/ibm-spie-hbcu-faculty-accelerator-award-in-quantum-optics-and-photonics" - IBM SPIE HBCU Faculty Accelerator Award 
 Student Funding | Scholarships 
 Conference Support 
 Industry Resources | Become an Exhibitor 
 Global Industry Report 
 Global Salary Report 
 Partners + Industry Clusters 
 Post a Job 
 Education Outreach | Outreach Grants 
 Posters 
 Optipedia 
 Advocacy + Public Policy | CHIPS for America 
 Policy Position Statements 
 Visit + Contact US Congress 
 International Day of Light | IDL Photo Contest 
 IDL Resources 
 Nominations now open for SPIE Fellows | The deadline for applications is 15 September | Learn More | Home 
 Community Support Home 
 Equity, Diversity, + Inclusion | Equity, Diversity, + Inclusion | Community Support 
 Equity, Diversity, + Inclusion 
 Family Care Grants 
 EDI Resources 
 EDI Videos 
 Women in Optics 
 SPIE Society Awards | SPIE Society Awards | Community Support 
 SPIE Society Awards 
 Award Nomination Guide 
 Awards Banquet 
 Research + Program Funding | href="//spie.org/community-support/research-and-program-funding" - Research + Program Funding | Community Support 
 href="//spie.org/community-support/research-and-program-funding" - Research + Program Funding 
 href="//spie.org/community-support/research-and-program-funding/spie-endowment-matching-program" - SPIE Endowment Matching Program 
 SPIE-Franz Hillenkamp Postdoctoral Fellowship 
 href="//spie.org/community-support/research-and-program-funding/ibm-spie-hbcu-faculty-accelerator-award-in-quantum-optics-and-photonics" - IBM SPIE HBCU Faculty Accelerator Award 
 Student Funding | Student Funding | Community Support 
 Student Funding 
 Scholarships 
 Conference Support 
 Industry Resources | Industry Resources | Community Support 
 Industry Resources 
 Become an Exhibitor 
 Global Industry Report 
 Global Salary Report 
 Partners + Industry Clusters 
 Post a Job 
 Education Outreach | Education Outreach | Community Support 
 Education Outreach 
 Outreach Grants 
 Posters 
 Optipedia 
 Advocacy + Public Policy | Advocacy + Public Policy | Community Support 
 Advocacy + Public Policy 
 CHIPS for America 
 Policy Position Statements 
 Visit + Contact US Congress 
 International Day of Light | International Day of Light | Community Support 
 International Day of Light 
 IDL Photo Contest 
 IDL Resources 
 Nominations now open for SPIE Fellows 
 The deadline for applications is 15 September 
 Learn More 
 News | News | News Home | Community News 
 SPIE Event News 
 SPIE Publication News 
 SPIE Press Releases 
 Photonics Focus 
 Optics.org 
 href="https://spie.org/news/photonics-focus/marchapril-2025" - | Latest Issue of Photonics Focus | March/April issue explores the ever-shrinking world of microelectronics | href="https://spie.org/news/photonics-focus/marchapril-2025" - March April 2025 | Home 
 News Home | News Home | News 
 News Home 
 Community News 
 SPIE Event News 
 SPIE Publication News 
 SPIE Press Releases 
 Photonics Focus 
 Optics.org 
 href="https://spie.org/news/photonics-focus/marchapril-2025" - 
Latest Issue of Photonics Focus 
 March/April issue explores the ever-shrinking world of microelectronics 
 href="https://spie.org/news/photonics-focus/marchapril-2025" - March April 2025 
 About | About | About SPIE Home 
 About the Society | Mission and Values 
 Officers and Directors 
 Committees 
 History 
 Past Officers and Directors 
 Bylaws 
 SPIE Brand and Logos 
 Jobs at SPIE 
 Code of Conduct 
 Policies and Reporting 
 Honor your Heroes | SPIE Society Awards are Open for Nominations | Learn More | Home 
 About SPIE Home 
 About the Society | About the Society | About 
 About the Society 
 Mission and Values 
 Officers and Directors 
 Committees 
 History 
 Past Officers and Directors 
 Bylaws 
 SPIE Brand and Logos 
 Jobs at SPIE 
 Code of Conduct 
 Policies and Reporting 
 Honor your Heroes 
 SPIE Society Awards are Open for Nominations 
 Learn More 
 Sign in 
 0 item added to your cart 
 More SPIE Websites 
 Explore SPIE websites: 
 Publications 
 Publications Home 
 Publication Resources | Publications 
 Publication Resources 
 Terms of Use 
 Reprint Permission 
 Contact SPIE Publications 
 SPIE Digital Library 
 SPIE Bookstore | Publications 
 SPIE Bookstore 
 Books 
 Proceedings 
 Apparel and Gifts 
 SPIE Journals | Publications 
 SPIE Journals 
 Institutional Subscriptions 
 Individual Subscriptions 
 Conference Proceedings | Publications 
 Conference Proceedings 
 Conference Content Publication Services 
 SPIE Press Books | Publications 
 SPIE Press Books 
 href="//spie.org/publications/spie-press-books/book-author-information" - Book Author Information 
 Book Manuscript Guidelines 
 Submit a Book Proposal 
 href="//spie.org/publications/spie-press-books/about-spotlights-and-author-calls" - Spotlights Call for Authors 
 href="//spie.org/publications/spie-press-books/field-guide-author-guidelines" - Field Guide Author Guidelines 
 PublicationsBookstoreConference ProceedingsProceedings Volume 13072 
 Sixteenth International Conference on Machine Vision (ICMV 2023) 
 Wolfgang OstenProceedings Volume 13072 
 Sixteenth International Conference on Machine Vision (ICMV 2023) 
 Wolfgang OstenPurchase the printed version of this volume atproceedings.comor access the digital version at SPIE Digital Library. 
 Buy Printed VolumeView on SPIE Digital LibraryBuy Printed VolumeView on SPIE Digital LibraryVolume Details 
 Date Published: 4 April 2024 
 Contents: 8 Sessions, 48 Papers, 0 Presentations 
 Conference: Sixteenth International Conference on Machine Vision (ICMV 2023) 2023 
 Volume Number: 13072 
 Table of Contents 
 Table of Contents 
 All links to SPIE Proceedings will open in theSPIE Digital Library.Show all abstracts 
 View SessionFront Matter: Volume 13072 
 Image Classification 
 Image Segmentation 
 Pattern Recognition 
 Image Detection and Detection Model 
 Image Analysis and Processing Method 
 Digital Photography Technology and Computer Vision Based on Imaging 
 Data-Based Intelligent Computing and Algorithm 
 Front Matter: Volume 13072 
 Front Matter: Volume 13072Show abstract 
 This PDF file contains the front matter associated with SPIE Proceedings Volume 13072, including the Title Page, Copyright information, Table of Contents, and Conference Committee information. 
 Image Classification 
 On optimizing morphological neural networks for hyperspectral image classificationMaksim Kukushkin,Martin Bogdan,Thomas SchmidShow abstract 
 Convolutional Neural Networks have become an important tool for various Computer Vision tasks. Yet, increasing complexity of such architectures drives computational costs. To this end, we propose two measures to achieve similar classification results as state-of-the-art architectures while at the same time reducing model complexity significantly. Firstly, we describe a novel type of non-linear parameter-efficient morphological layers inspired by concepts that are well-known and widely used with convolutions. Secondly, we present a set of simple network architectures, organized as optimization framework, which is enhanced by neural architecture search and hyperparameter optimization. In experiments with hyperspectral remote sensing data, we demonstrate that the identified optimal morphological architecture produces results not only comparable with other architectures from the optimization framework, but also comparable or better than selected state-of-the-art neural network architectures for image classification. Depending on the performed task, the proposed optimized architecture requires up to 25 times fewer parameters than actual state-of-the-art networks. 
 sMoBYAL: supervised contrastive active learning for image classificationThanh Hong Dang,Thanh Tung Nguyen,Huy Quang Trinh,et al.Show abstract 
 We propose a novel active learning framework for image classification - sMoBYAL. Our contribution is modifying MoBY - one of the highly effective self-supervised learning algorithms to utilize both labeled and unlabeled data for the active learning pipeline. Finally, we thoroughly evaluate and analyze the robustness and performance of our pipeline in image classification tasks. Our approach attains comparative outcomes, surpassing recent AL methods in terms of results. Our code available at: https://github.com/thanhdh-3030/sMoBYAL 
 Quantization method for bipolar morphological neural networksElena Limonova,Michael Zingerenko,Dmitry Nikolaev,et al.Show abstract 
 In the paper, we present a quantization method for bipolar morphological neural networks. Bipolar morphological neural networks use only addition, subtraction, and maximum operations inside the neuron and exponent and logarithm as activation functions of the layers. These operations allow fast and compact gate implementation for FPGA and ASIC, which makes these networks a promising solution for embedded devices. Quantization allows us to reach an additional increase in computational efficiency and reduce the complexity of hardware implementation by using integer values of low bitwidth for computations. We propose an 8-bit quantization scheme based on integer maximum, addition, and lookup tables for non-linear functions and experimentally demonstrate that basic models for image classification can be quantized without noticeable accuracy loss. More advanced models still provide high recognition accuracy but would benefit from further fine-tuning. 
 Fast keypoint filtering for feature-based identity documents classification on complex backgroundNargiza Z. Valishina,Alexander V. Gayer,Natalya S. Skoryukina,et al.Show abstract 
 The initial steps of many computer vision algorithms are local feature extraction and matching. However, in the problem of recognizing objects in images with complex backgrounds, this approach has a weak point since keypoints may be found not only in the object of interest, but also in the background. This leads to redundant calculations and can cause mismatches. In this paper, we propose a keypoints filtering method applicable to the problem of classification and localization of ID documents in the wild. Using a light-weight deep learning model, keypoints are divided into ”document” and ”background” classes, after which the keypoints of the background are removed. Experimental results show that adding the proposed filtering step gives an average speedup of 3.14% on the entire MIDV-500 dataset and 14.77% on MIDV-2020. At the same time, the acceleration on target images with complex backgrounds reaches 81%. 
 Multi-class object classification using deep learning models in automotive object detection scenariosSoumya A.,Linga Reddy Cenkeramaddi,Vishnu Chalavadi,et al.Show abstract 
 This paper presents two deep learning models using a multi-perspective convolutional neural network (CNN) for classifying objects in the context of intelligent transportation systems (ITS). The proposed model categorizes objects accurately, enabling them to make well-informed decisions in multi-object (such as Persons, Trucks, Motorbikes, Cars, and Cyclists.) detection in complex scenarios for automotive applications. The custom backbone model is designed based on experimentation with the VGG backbone network based on the VGG backbone network, incorporating a multilayer prediction head and custom feature extraction blocks for classifying multiple objects in complex scenes. The model is to extract abstract features and features at multiple scales with a custom-designed feature extraction backbone with multiple blocks. The proposed models are lightweight and require fewer computational resources for high classification performance. The automotive publicly available dataset with 19800 images and labels has been used. Results show that when we experimented with the VGG backbone CNN model, the classification accuracy of 99.64% is achieved, and on the other hand, the classification accuracy of custom backbone CNN is 99.46%. The performance of the proposed custom model is also compared to those of pre-trained benchmark models. The experimental findings presented in this paper show that the proposed models achieve higher accuracy than the pre-trained models. 
 Sequence models for drone versus bird classificationFatih C. Akyon,Erdem Akagündüz,Sinan O. Altinuc,et al.Show abstract 
 Drone detection has become an essential task in object detection as drone costs have decreased and drone technology has improved. It is, however, difficult to detect distant drones when there is weak contrast, long range, and low visibility. In this work, we propose several sequence classification architectures to reduce the detected false-positive ratio of drone tracks. Moreover, we propose a new drone vs. bird sequence classification dataset to train and evaluate the proposed architectures. 3D CNN, LSTM, and Transformer based sequence classification architectures have been trained on the proposed dataset to show the effectiveness of the proposed idea. As experiments show, using sequence information, bird classification and overall F1 scores can be increased by up to 73% and 35%, respectively. Among all sequence classification models, R(2+1)D-based fully convolutional model yields the best transfer learning and fine-tuning results. 
 Image Segmentation 
 Enhancing crop segmentation in satellite image time-series with transformer networksI. Gallo,M. Gatti,N. Landro,et al.Show abstract 
 Recent studies have shown that Convolutional Neural Networks (CNNs) achieve impressive results in crop segmentation of Satellite Image Time-Series (SITS). However, the emergence of transformer networks in various vision tasks raises the question of whether they can outperform CNNs in crop segmentation of SITS. This paper presents a revised version of the Transformer-based Swin UNETR model adapted specifically for crop segmentation of SITS. The proposed model demonstrates significant advancements, achieving a validation accuracy of 96.14% and a test accuracy of 95.26% on the Munich dataset, surpassing the previous best results of 93.55% for validation and 92.94% for the test. Additionally, the model’s performance on the Lombardia dataset is comparable to UNet3D and superior to FPN and DeepLabV3. Experiments of this study indicate that the model will likely achieve comparable or superior accuracy to CNNs while requiring significantly less training time. These findings highlight the potential of transformer-based architectures for crop segmentation in SITS, opening new avenues for remote sensing applications. 
 Segmentation of human olfactory bulb glomeruli on its phase-contrast tomographic images with neural networksAleksandr Smolin,Marina Chukalina,Inna Bukreeva,et al.Show abstract 
 The human olfactory bulb (OB), an important part of the brain responsible for the sense of smell, is a complex structure composed of multiple layers and cell types. Studying the OB morphological structure is essential for understanding the decline in olfactory function related to aging, neurodegenerative disorders, and other pathologies. Traditional microscopy methods in which slices are stained with solutions to contrast individual elements of the morphological structure are destructive. Non-destructive high-resolution technique is the X-ray phase-contrast tomography. However, manual segmentation of the reconstructed images are time-consuming due to large amount of data and prone to errors. U-Net-based model to optimize the segmentation of OB morphological structures, focusing specifically on glomeruli, in tomographic images of the human OB is proposed. The strategy to address overfitting and enhance the model's accuracy is described. This method addresses the challenges posed by complex limited data containing abundant details, similar grayscale levels between soft tissues, and blurry image details. Additionally, it successfully overcomes the limitations of a small dataset containing images with extremely dense point clouds, preventing the models from overfitting. 
 CattleDeSegNet: a joint approach to cattle denoising and interpretable segmentationSivaji Retta,Ramarajulu Srinivasan,Shawn TanShow abstract 
 In modern agriculture, livestock monitoring plays a vital role in ensuring animal health, welfare, and production efficiency. Leveraging computer vision and deep learning, this paper presents an innovative framework aimed at enhancing livestock monitoring. Specifically, we address two crucial challenges: denoising and segmentation of cattle in livestock images. The denoising task is fundamental in preprocessing noisy images affected by adverse environmental conditions and equipment limitations. To tackle this, we introduce an encoder-decoder model that effectively denoises cattle images while preserving critical anatomical details. Our framework incorporates a segmentation module inspired by the U-Net architecture. Notably, both denoising and segmentation tasks share a common encoder, optimizing computational efficiency. The segmentation model employs hybrid loss functions and leverages the Grad-CAM technique to provide interpretable insights into the decision-making process. Our approach stands as one of the pioneering joint solutions for cattle denoising and segmentation, particularly focusing on top-view cattle images. 
 Pattern Recognition 
 Training of binary neural network models using continuous approximationDmitrij Pavliuchenkov,Anton Trusov,Elena LimonovaShow abstract 
 The paper is devoted to the training of binary neural networks. They reduce the requirements for computing power and memory, which is especially important in conditions of limited resources. To date, binary networks do not provide sufficient recognition quality comparable to the quality of traditional floating-point networks, so the development of more efficient methods of training networks are highly relevant. In this paper, we propose a probabilistic model of a neural network that can be transformed into a binary network and consider a way of binarization. Experimental results have shown that our model with incremental binarization and subsequent fine-tuning makes it possible to achieve recognition accuracy of 97.5% for MNIST image classification problem when the accuracy of the binary model trained by Straight Through Estimation was 87.5%. 
 SAIGAN: arbitrary length and out-of-vocabulary handwriting synthesis preserving geometrical annotationKonstantin K Suloev,Yulia S. Chernyshova,Alexander V. SheshkusShow abstract 
 Handwritten text recognition (HTR) is a challenging task that requires a large amount of diverse training data. One of the possible approaches to this problem is the adoption of CNNs. The key challenge is that the CNN requires geometrically labeled training data, which may increase the cost and time consumption of labeling. To overcome these limitations we propose the method, based onGenerative Adversarial Network(GAN), which transfers handwriting styles to printed style images, preserving the Same geometrical Annotation as Input - SAIGAN. Taking printed style image as an input, it produces the handwritten image with the same text content located in the same positions. Our method operates on the character-level and can produce sequences of an arbitrary length and any content. Once trained, it is also possible to generate new handwriting styles by simply manipulating latent vectors. Proposed character style supervision allowed our model to surpass the basis method. 
 Building an optimal document authentication systemA. D. Bursikov,S. A. Usilin,I. A. KuninaShow abstract 
 In this work, a probabilistic approach to assessing the quality of a system for determining the authenticity of documents in the images is considered. The considered system is based on the aggregation of responses of various checks of the security elements of the document. We propose a probabilistic model of the document authentication system and the functional for evaluating the quality of the system. Based on them, we offer an approach for assessing the quality of the separate part and the whole system. Finally, the approach to constructing an optimal function of making a final decision is obtained. 
 Multilanguage ID document images synthesis for testing recognition pipelinesYulia S. Chernyshova,Konstantin K. Suloev,Vladimir V. ArlazarovShow abstract 
 Datasets are de facto the only way to test the recognition pipelines and to compare them with each other. To avoid the manual gathering of documents and, moreover, to avoid problems with the law in the case of ID documents researchers create synthetic datasets or datasets of fake documents, but this process is also time-consuming. In this paper, we present a simple method to use when you need to test a recognition pipeline or some part of it. The method employs only the information that the developers of such pipelines use in their work and allows them to create natural-looking images. The quantitative experiments show that the recognition accuracy of the synthesized images corresponds with the recognition accuracy of the MIDV-2020 dataset. The qualitative comparison also demonstrates that such images can be helpful in recognition systems’ development. 
 FARA: fast and accurate RFDoc descriptor approximationArtem Sher,Anton Trusov,Mikhail Maksimenko,et al.Show abstract 
 We present FARA, a novel approach for fast approximation of RFD-like descriptors in the context of document retrieval systems. RFD-like descriptors are widely used for document representation, but their computation is expensive, especially for large document collections. Our method is a CPU-friendly gradient maps computation approximation with sequential memory access and integer-only calculations. There are three types of operations that we use: addition, subtraction, and absolute values. It allows us to effectively use SIMD extensions, resulting in an additional increase in the running speed. Experimental results demonstrate that FARA achieves the same accuracy as RFDoc descriptors and significantly reduces the computational overhead. The proposed approach achieves a twofold speed improvement of gradient maps computation and 25% acceleration of overall descriptor computing time compared to the most efficient RFDoc implementation. 
 Enhanced multiple-instance pruning for learning soft cascade detectorsDaniil P. Matalov,Vladimir V. ArlazarovShow abstract 
 Object detection is one of the most common problems solved by computer vision systems. Even though neural network methods have become a standard tool for solving the problems, these methods have many disadvantages, which include high computational power requirements both for training and inference stages and tremendous training sets. This paper considers such a classical method for object detection as the Viola and Jones method and proposes an enhanced soft cascade calibration method based on Multiple-Instance Pruning to increase detection performance. The proposed method considers a response of the classifier to an image region as a random variable and follows a statistical approach to provide robust detectors. In addition, the paper addresses the problem of non-conformity of detection parameters at training and inference stages and studies performance decline. The performance of the proposed methods is demonstrated in a variety of practical tasks, including identity document detection and document fraud detection. 
 Limitations of anomaly detection: beyond which size defects can be reliably recognizedJan Lehr,Martin Pape,Jan Philipps,et al.Show abstract 
 Anomaly detection is one of the most popular fields for computer vision in industrial applications. The idea of training machine learning only on defect-free objects saves enormous amounts of integration effort. The state of the art shows that current methods on public data sets (e.g. MVTec AD data set) have already solved the problem with AUROC segmentations scores of more than 99%. But how accurate are these methods really? In this paper, one current method from the field of supervised learning and anomaly detection is evaluated on two problems. Each problem contains a defect pattern that grows in 11 steps. This work shows that the defect is already reliably detected from a relative size of 0.03% of the pixels in the image. 
 Image Detection and Detection Model 
 L-shape fitting algorithm for 3D object detection in bird’s-eye-view in an autonomous driving systemMikhail O. Chekanov,Oleg S. ShipitkoShow abstract 
 The ability of autonomous vehicles (AVs) to detect three-dimensional objects is crucial for motion planning, object tracking and safe driving. This task is especially challenging for systems using only monocular cameras, for which depth estimation presents special difficulties. In this paper, we discuss the subsystem of 3D object detection in bird’s-eye-view (BEV) for a single camera in an AV system. The subsystem consists of two parts. First, it estimates the contour of the object’s projection polygon in BEV based on 2D detection and drivable area segmentation (a planar ground model is used). Second, it simplifies the object’s projection by fitting the obtained polygon to a rotated bounding box. For this part we propose a new L-shape model-based fitting algorithm. It assumes that the vertices of the input polygon belong to two adjacent sides of the fitted bounding box. We compared this algorithm with a naive approach which minimizes the bounding box’s area and with adaptations of algorithms from a paper solving a similar problem with LiDAR point clouds. The L-shape algorithm outperformed the alternatives. 
 False positive elimination in object detection methods for videosShubham Kumar Dubey,J. V. Satyanarayana,C. Krishna MohanShow abstract 
 A robust object detection algorithm is essential while detecting objects in videos and real time scenarios, where false positives might result in unwanted outcomes. Our goal here is to observe how Simple Online and Real-time Tracking with a Deep association metric (Deep SORT) algorithm for Multi-Object Tracking (MOT) can be used to minimize false positives, from a state of the art detection algorithm like You Only Look Once (YOLO), by using the Kalman filter approach. An auto encoder based feature extractor has been used, instead of the standard CNN networks like ResNet-50 to further improve speed of the detector. There have been other MOT algorithms in the recent times which give good results, but are not as real time efficient as the simple yet efficient Deep SORT method. Experimental analysis has shown how Autoencoder based Deep SORT performs in contrast to native Deep SORT and YOLO, in eliminating false positive detections. 
 Copy-move document image forgery detection and localization based on JPEG cluesA. V. Chuiko,K. B. Bulatov,D. V. TropinShow abstract 
 The amount of image forgery strongly increases recently. There are different ways to fake an image, one of the common ones is a copy-move manipulation. There are numerous methods for detecting copy-move manipulations on natural images. However, they are difficult to adapt for document images due to their features. This work proposes an algorithm for detecting and localizing copy-move manipulations on digital images of documents. The main idea is to use JPEG artifacts in order to find the target region area and then localize the source and target regions precisely. For the efficient application of the proposed method, firstly, the original image must have been subjected to JPEG compression, and secondly, after the manipulation the image must have been saved in a lossless format. The experiments were carried out on an open set of document images CMID; in the detection task, the recall was 0.992, the specificity was 1.0; in the localization task, the recall was 0.923, the false discovery rate was 0.021, which means that the proposed algorithm successfully detects more than 99% of copy-move manipulations, similar to manipulations in the CMID and does not give false positives. 
 Incremental one-class learning using regularized null-space training for industrial defect detectionMatthias Hermann,Georg Umlauf,Bastian Goldlücke,et al.Show abstract 
 One-class incremental learning is a special case of class-incremental learning, where only a single novel class is incrementally added to an existing classifier instead of multiple classes. This case is relevant in industrial defect detection scenarios, where novel defects usually appear during operation. Existing rolled-out classifiers must be updated incrementally in this scenario with only a few novel examples. In addition, it is often required that the base classifier must not be altered due to approval and warranty restrictions. While simple finetuning often gives the best performance across old and new classes, it comes with the drawback of potentially losing performance on the base classes (catastrophic forgetting [1]). Simple prototype approaches [2] work without changing existing weights and perform very well when the classes are well separated but fail dramatically when not. In theory, null-space training (NSCL) [3] should retain the basis classifier entirely, as parameter updates are restricted to the null space of the network with respect to existing classes. However, as we show, this technique promotes overfitting in the case of one-class incremental learning. In our experiments, we found that unconstrained weight growth in null space is the underlying issue, leading us to propose a regularization term (R-NSCL) that penalizes the magnitude of amplification. The regularization term is added to the standard classification loss and stabilizes null-space training in the one-class scenario by counteracting overfitting. We test the method’s capabilities on two industrial datasets, namely AITEX and MVTec, and compare the performance to state-of-the-art algorithms for class-incremental learning. 
 Detection of fingers in document images captured in uncontrolled environmentL. S. Tolstenko,I. A. KuninaShow abstract 
 This paper proposes an algorithm for detecting finger areas in document images captured in an uncontrolled environment. The main idea of the proposed algorithm is to segment the image on the chromaticity plane and search for a segment that crosses the boundary of the displayed document in the image. It is proposed to use segmentation in combination with the edge analysis to prevent false merging of two segments belonging to the document and the background respectively, being different in the original RGB space, but acquiring similar characteristics on the chromaticity plane. Testing was performed on document images from the MIDV-2020 open dataset. The precision of the proposed algorithm was evaluated as 91.2%, and the recall as 73.5%. 
 Methods for non-intrusive out-of-distribution images detectionAnastasiia V. Vlasova,Aleksandr Yu. Shkanaev,Dmitry L. SholomovShow abstract 
 Selecting representative data is a key factor in improving the performance of machine learning algorithms. In this paper we focus on out-of-distribution (OoD) methods evaluation, which can be integrated into ML project lifecycle in a nonintrusive way, without changing a model architecture. Considered methods are applicable to image classification datasets analysis. In addition to commonly used AUROC metric, we evaluate the number of out-of-distribution samples misclassified with high confidence. Case studies were conducted on benchmark and production datasets. As a result, we provide practical guidance for data evaluation and recommendations on which method to use to detect different types of OoD images. 
 Image edge detection using pseudo-Boolean polynomialsTendai Mapungwana Chikake,Boris GoldengorinShow abstract 
 We introduce a novel approach for image edge detection based on calculating pseudo-Boolean polynomials on image patches whose resulting polynomial degrees determine whether a patch lies over an edge or a blob. In this paper we show that patches coveringedge regionswithin the image result in pseudo-Boolean polynomials of higher degrees compared to patches that coverblobregions. The proposed approach is based onreductionof polynomial degree andequivalenceproperties of penalty-based pseudo-Boolean polynomials. 
 Specialized indoor and outdoor scene-specific object detection modelsMahtab Jamali,Paul Davidsson,Reza Khoshkangini,et al.Show abstract 
 Object detection is a critical task in computer vision with applications across various domains, ranging from autonomous driving to surveillance systems. Despite extensive research on improving the performance of object detection systems, identifying all objects in different places remains a challenge. The traditional object detection approaches focus primarily on extracting and analyzing visual features without considering the contextual information about the places of objects. However, entities in many real-world scenarios closely relate to their surrounding environment, providing crucial contextual cues for accurate detection. This study investigates the importance and impact of places of images (indoor and outdoor) on object detection accuracy. To this purpose, we propose an approach that first categorizes images into two distinct categories: indoor and outdoor. We then train and evaluate three object detection models (indoor, outdoor, and general models) based on YOLOv5 and 19 classes of the PASCAL VOC dataset and 79 classes of COCO dataset that consider places. The experimental evaluations show that the specialized indoor and outdoor models have higher mAP (mean Average Precision) to detect objects in specific environments compared to the general model that detects objects found both indoors and outdoors. Indeed, the network can detect objects more accurately in similar places with common characteristics due to semantic relationships between objects and their surroundings, and the network’s misdetection is diminished. All the results were analyzed statistically with t-tests. 
 Bipolar morphological YOLO network for object detectionMichael Zingerenko,Elena LimonovaShow abstract 
 There are various techniques for decreasing the computational complexity of neural networks, and a number of them use neuron approximations. A bipolar morphological neuron is an approximation of a classical neuron that can be used on FPGAs and ASICs to enhance computational efficiency. It uses 4 distinct computational pathways utilizing addition and maximum functions, in contrast to the traditional neuron which employs multiplication and addition. In this paper, we introduce bipolar morphological YOLO network for object detection task. To train the network, we employ an iterative approach that combines knowledge distillation for backbone and fine-tuning of the network’s head. Our experiments, which were conducted using the COCO dataset, yield results that are on par with classical networks. Specifically, the average recall for large images is 0.393 for the BM network and 0.371 for the classical network. Additionally, the average precision values are 0.088 for the BM network and 0.097 for the classical network. These outcomes establish a baseline for object detection using bipolar morphological networks. 
 Image Analysis and Processing Method 
 Point scene understanding via points-to-mesh reconstruction and multi-level utilization of proposalsMengxiang Hao,Hang Wu,Ruchong Fu,et al.Show abstract 
 Semantic scene reconstruction from sparse and incomplete point clouds is a critical task for point scene understanding. It aims to recognize semantic labels for objects and recover their complete shapes as meshes. Existing methods often fail to realize high-quality instance reconstruction due to inadequate shape representation and underutilization of proposal point clouds. To address these issues, we optimize the previous BSP/occupancy-to-mesh reconstruction framework to points-to-mesh and accomplish multi-level utilization of proposals. We chose point cloud as the representation of completion to reduce the difficulty of restoring curved shallow parts. Benefiting from the optimization, we can match and merge proposal point clouds with the restored ones, avoiding missing parts existing in inputs. We design an effective pose normalization module to extract point-based features from normalized proposals, which are fused with features extracted from voxelized proposals, avoiding the detailed geometry lost in voxelization and enhancing the reconstruction's robustness to different input postures. The suitable points-to-mesh reconstruction framework and full utilization of proposals make our method improve reconstruction results efficiently. Detailed experiments on the challenging ScanNet dataset of the semantic scene reconstruction benchmark show that our network outperforms state-of-the-art methods in both completion and mapping metrics. 
 Search for image quality metrics suitable for assessing images specially precompensated for users with refractive errorsNafe B. Alkzir,Ilya P. Nikolaev,Dmitry P. NikolaevShow abstract 
 Recently, we presented the SCA-2023 dataset, which had been developed specifically to evaluate the quality of various image precompensation algorithms for observers with imperfect vision. Such precompensation makes it possible to bring their image perception closer to that of an observer with the ideal vision. While experimenting with various image quality metrics, we realized that it was not so easy to evaluate the quality provided by different algorithms, since the metrics ''voted'' for different things, and their choice often seemed to contradict the human perception. This is a key motivation for our study, in which we set out to select the metric best correlated with the human perception of precompensated images. We selected a suitable subdataset from our SCA-2023 dataset and, based on it, created 90 grayscale images, which were shown to our colleagues in a pairwise comparison way. More than 2,000 pairwise comparison results were collected from 24 study participants. Further, according to our original methodology, these results were compared with the ''opinion'' of some popular quality metrics, which made it possible to rank these metrics according to their adequacy within the framework of this task. Finally, we showed how to use these results in optimization procedures aimed at improving the quality of precompensation. 
 Threshold U-Net: speed up document binarization with adaptive thresholdsKonstantin E. Lihota,Alexander V. Gayer,Vladimir V. ArlazarovShow abstract 
 U-Net similar architectures are widely used in the task of document image binarization. However, despite the good quality of binarization, they also have high computational complexity, which greatly limits their use on mobile and embedded devices. The performance bottleneck of U-Net architectures is the first encoder layers and the last decoder layers, which operate on high-resolution input data and contain the largest number of operations. Based on this, in this paper we propose a new Threshold U-Net model: instead of predicting the final image, Threshold U-Net predicts a low-resolution adaptive threshold map, with which the input image is binarized. The proposed architecture naturally combines the ideas of classical algorithms that calculate the binarization threshold for a specific image region with an approach based on a deep learning model with a large receptive field and context understanding. Threshold U-Net demonstrates quality of binarization of historical documents comparable to U-Net on the DIBCO-2017 dataset. At the same time, depending on the resolution of the threshold map, Threshold U-Net is up to 2 times faster, requires up to 26% less RAM and consists up to 10% fewer parameters. 
 href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13072/130720U/Integrated-channel-attention-method-for-Siamese-tracker/10.1117/12.3023385.full?webSyncID=0bb55ce6-cc22-203e-13e8-b0564a345d47&sessionGUID=6c5ccae6-7b2c-876a-b288-1a23526adca9" - Integrated channel attention method for Siamese tracker
Ziyi Zhou,Yingran Jin,Yun GaoShow abstract 
 It is critical to make full use of the information of the backbone to improve the performance of object tracking. A common way to mine useful information is to add attention to features. However, most trackers only use single attention to mine the features and fail to utilize the effective information in the backbone. To leverage the useful information of features in multiple ways, this paper proposes an integrated channel attention mechanism based on all kinds of commonly used channel attention methods. First, we used ResNet50 as the backbone, and then we used four attention methods to process the fourth-stage features that were extracted from the backbone to obtain attention factors. Then, through adaptive weighting, we added the four attention methods to the original features. It adaptively adjusts the importance of each channel attention, suppresses redundant information, and better captures key features of the tracked object in different channels. The effectiveness of our approach is validated on five tracking benchmarks. 
 Assessment of the color compatibility of garments for building a recommendation system for an outfitEkaterina P. Gerasimova,Dmitry L. SholomovShow abstract 
 In this paper, we consider the problem of clothes compatibility for total look recommendation systems by means of deep neural networks. This task has become very popular in recent years, primarily due to the growth of online retail sales of clothing. Unlike the existing solutions, we developed a comprehensive model of clothes compatibility evaluation based on color characteristics as well as on the characteristics of the style. As a rule, neural networks are robust to the color characteristics of an image, but color is an extremely important component in the task of a total look evaluation, so such additional branch with color characteristics is well justified. The proposed model uses both: color embedding obtained from color clustering and histograms, and style embedding as an output tensor of ResNet-50 encoder. The paper shows that color embeddings significantly improve the quality of the total look evaluation. The model was trained on Polyvore dataset, which was pre-processed and cleaned from the items not related to the topic of total look compatibility. 
 Maintaining topological maps for mobile robotsKirill Muravyev,Konstantin YakovlevShow abstract 
 Nowadays, mobile robots solve various tasks realted to navigation in an unknown or changing environment. In such conditions, mapping the environment is crucial for mobile robot navigation. Conventionally, maps are built as 2D or 3D dense metric structures which require much memory for storage and much computational time for path planning, especially in large environments. Representing a map as a topological structure (i.e. a graph of locations) allows fast path planning with low memory consumption. In this paper, we present a method of real-time topological map building and updating from odometry measurements and local point clouds. The proposed method guarantees the topological graph connectivity and adds shortcuts to the graph to optimize paths. We tested our method in a simulated environment and measured efficiency of path planning in the obtained graphs. In all the tests, the SPL value exceeded 81%, with 100% success rate. The source code of our method is available at https://github.com/KirillMouraviev/simple_toposlam_model. 
 Spectral filters design for a better hyperspectral reconstructionDaniil Reutskii,Egor ErshovShow abstract 
 Spectral reconstruction (recovering spectra from RGB measurements) is a vital problem of computational photography. As a matter of curiosity, modern mobile devices open a new opportunity to improve the quality of spectral reconstruction by utilizing images from several cameras at once. This leads to the idea of creating a mobile hyperspectral camera for the general public. In this paper we investigate the achievable accuracy when using several identical cameras simultaneously in combination with different spectral filters. To find optimal filters, two algorithms are proposed: one learns spectral transmittance functions simultaneously with spectral reconstruction, the other learns only spectral transmittances by information loss minimization. As a result of numerical experiments, 4 cameras and 4 filters allow us to perform spectral reconstruction two times accurately than from a single RGB image. 
 Method of color image formation taking into account the human perception featuresN. A. Obukhova,A. A. Motyko,A. A. Pozdeev,et al.Show abstract 
 Currently, color image formation is based on the CIE 1931 and CIE 1964 standard colorimetric observer models. Expansion of color gamuts of displays requires narrow spectral power distributions functions of their primary colors. In this case, the use of CIE 1931 and CIE 1964 models leads to color reproduction errors, as these models do not take into account the peculiarities of color vision of a particular user. To reduce color rendering errors caused by individual vision features, a method of color image formation based on the categorical observer model is proposed. The basis of the method is the evaluation of the categorical observer type using binary tests, which can be implemented outside the laboratory conditions on conventional displays with three basic colors. The effectiveness of the method has been confirmed experimentally. 
 Digital Photography Technology and Computer Vision Based on Imaging 
 CT metal artifacts simulation under x-ray total absorptionMikhail Shutov,Marat Gilmanov,Dmitry Polevoy,et al.Show abstract 
 Computed tomography (CT) is a powerful tool for reconstruction and analysis of inner structure of objects applied in various fields. Although many classes of objects of interest may have highly absorbent inclusions, leading to a certain type of distortions on reconstructed volume images (metal-like artifacts). The correction of this type of artifacts can’t be considered a solved task, despite all the efforts in this direction. The development and research of methods for suppressing CT artifacts require high-quality synthetic data which allow for numerical assessment of the accuracy of the metal-like artifacts reduction methods and training of neural networks. Although simplified methods considering only beam hardening and Poisson photon distribution are commonly used to simulate the data with type of distortions. In present work we design experiments using the tomographic scanner of the Federal Research Center “Crystallography and Photonics” of the Russian Academy of Sciences to demonstrate that in some cases beam hardening may not be the dominant reason for the arising of metal-like artifacts. These experiments are closely analyzed and modeled within different approaches. The problems in both simplified and state of the art approaches are emphasized and discussed. The provided results show the importance of paying attention to the dark current modeling for synthesized data generation under the conditions of total photon absorption. 
 HoughToRadon transform: new neural network layer for features improvement in projection spaceAlexandra Zhabitskaya,Alexander Sheshkus,Vladimir L. ArlazarovShow abstract 
 In this paper, we introduce HoughToRadon Transform layer, a novel layer designed to improve the speed of neural networks incorporated with Hough Transform to solve semantic image segmentation problems. By placing it after a Hough Transform layer, ’inner’ convolutions receive modified feature maps with new beneficial properties, such as a smaller area of processed images and parameter space linearity by angle and shift. These properties were not presented in Hough Transform alone. Furthermore, HoughToRadon Transform layer allows us to adjust the size of intermediate feature maps using two new parameters, thus allowing us to balance the speed and quality of the resulting neural network. Our experiments on the open MIDV-500 dataset show that this new approach leads to time savings in document segmentation tasks and achieves state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger computational complexity. 
 Distortion-aware super-resolution for planetary exploration imagesN. Landro,I. Gallo,F. Pelosi,et al.Show abstract 
 Super-resolution is crucial in computer vision and digital image processing, aiming to enhance low-quality images’ resolution and visual quality. This paper focuses on correcting the distortion introduced by fisheye lenses and improving the resolution of images for better detail representation. Specifically, we propose an evaluation approach that benchmarks three state-of-the-art models in different categories: Real-ESRGAN (convolutions), SwinIR (transformers), and SR3 (diffusion). We evaluate their performance in super-resolution and distortion correction tasks using metrics such as PSNR and SSIM. To facilitate this evaluation, we create and release a new dataset of lunar surface images with fisheye distortion applied. Our experiments demonstrate the effectiveness of each model in handling distortion and improving image resolution. The results show that large models generally outperform medium models, and PSNR models achieve higher PSNR and SSIM scores than GAN models. Additionally, we evaluate the distortion correction by comparing the corrected images with ground truth. Our findings contribute to understanding different model categories and their performance in super-resolution and distortion correction tasks. The proposed dataset and evaluation approach can be valuable resources for future research. 
 Robust automatic rotation axis alignment mean projection image method in cone-beam and parallel-beam CTDanil Kazimirov,Anastasia Ingacheva,Alexey Buzmakov,et al.Show abstract 
 The rotation axis position is an important parameter of classical reconstruction algorithms in X-ray computed tomography (CT). The use of incorrect values of the axis position parameters during the reconstruction leads to the appearance of various artifacts distorting the reconstructed image. Therefore, to obtain a reconstruction of better quality, automatic rotation axis position determination and misalignment correction methods are of use. Most of the existing high-precision automatic rotation axis position determination methods are either fast, but suitable only within a parallel-beam geometric scheme, or indifferent to the geometric scheme, but computationally laborious. In this paper, we propose a method for auto-detection of two scalar parameters of rotation axis position — axis shift and tilt in the plane parallel to the detector window plane — using a pixel-wise arithmetically averaged projection image. The described method is highly accurate within both parallel-beam and cone-beam geometric schemes whereas it is characterized by robustness to noise in projection data. The method has performed an increase in reconstruction quality when compared with some well-known and still used in practice methods both on synthetic data and on real data obtained in real laboratory conditions. 
 href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13072/1307213/StereoYoloDeepSORT--a-framework-to-track-fish-from-underwater-stereo/10.1117/12.3023414.full?webSyncID=0bb55ce6-cc22-203e-13e8-b0564a345d47&sessionGUID=6c5ccae6-7b2c-876a-b288-1a23526adca9" - StereoYolo+DeepSORT: a framework to track fish from underwater stereo camera in situ
Aya Saad,Stian Jakobsen,Morten Bondø,et al.Show abstract 
 This paper presents a 3D multiple object detection and tracking framework for identifying and quantifying changes in fish behaviour through tracking the 3D position, distance and speed of fish with respect to an underwater stereo camera. The framework consists of six essential modules based on 3D object detection to identify fish and multiple object tracking algorithms to track the fish in sequential frames. In particular, the latest version of Yolo (Yolov7) is utilised for object detection and the deep SORT algorithm is used for multiple object tracking. The framework was tested using videos captured from an underwater stereo camera in an industrial-scale sea-based fish farm. The results showed that the framework was able to accurately detect and track multiple fish in 3D. The fish position, distance and speed relative to the camera were also successfully detected. The results of this study demonstrate the effectiveness of this framework in identifying and quantifying changes in fish behaviour. The proposed novel framework has the potential to greatly enhance our understanding of fish behaviour in their natural habitats, leading to new insights into fish ecology and behaviour, while at the same time, it can enable researchers to study fish behaviour in a more detailed and accurate way. 
 Embracing uncertainty flexibility: harnessing a supervised tree kernel to empower ensemble modelling for 2D echocardiography-based prediction of right ventricular volumeTuan A. Bohoran,Polydoros N. Kampaktsis,Laura McLaughlin,et al.Show abstract 
 The right ventricular (RV) function deterioration strongly predicts clinical outcomes in numerous circumstances. To boost the clinical deployment of ensemble regression methods that quantify RV volumes using tabular data from the widely available two-dimensional echocardiography (2DE), we propose to complement the volume predictions with uncertainty scores. To this end, we employ an instance-based method which uses the learned tree structure to identify the nearest training samples to a target instance and then uses a number of distribution types to more flexibly model the output. The probabilistic and point-prediction performances of the proposed framework are evaluated on a relatively small-scale dataset, comprising 100 end-diastolic and end-systolic RV volumes. The reference values for point performance were obtained from MRI. The results demonstrate that our flexible approach yields improved probabilistic and point performances over other state-of-the art methods. The appropriateness of the proposed framework is showcased by providing exemplar cases. The estimated uncertainty embodies both aleatoric and epistemic types. This work aligns with trustworthy artificial intelligence since it can be used to enhance the decision-making process and reduce risks. The feature importance scores of our framework can be exploited to reduce the number of required 2DE views which could enhance the proposed pipeline’s clinical application. 
 Efficient single- and multi-DNN inference using TensorRT frameworkVyacheslav Zhdanovskiy,Lev Teplyakov,Philipp BelyaevShow abstract 
 In the recent years, there has been a significant growth of interest in real-world systems based on deep neural networks (DNNs). These systems typically incorporate multiple DNNs running simultaneously. In this paper we propose a novel approach of multi-DNN execution on a single GPU using multiple CUDA contexts and TensorRT, state-of-the-art DNN inference framework. We show that it can lead to more efficient scheduling of multiple DNNs, especially in case when a lightweight and a heavy DNNs are inferred together. We show that our approach can provide an almost 7x increase in the throughput of a lightweight DNN at the cost of neglible throughput drop of a heavy DNN, compared to the baseline. Moreover, we compare two ways of improving throughput of a single DNN by processing multiple images together: standard batching and implicit batching by processing multiple images simultaneously using several TensorRT execution contexts. We show that meanwhile standard batching outperforms implicit batching at larger batch sizes, implicit batching can provide up to 43% more throughput for a smaller DNN using smaller batch size. 
 CADCP: a method for chromatic haze compensation on remotely sensed imagesD. S. Sidorchuk,M. A. Pavlova,D. O. Kushchev,et al.Show abstract 
 Remote sensing images often suffer from different types of haze. Its presence significantly complicates remotely sensed image analysis that is crucial for monitoring of land state and precision agriculture. Currently existing remote sensing dehazing methods are designed for achromatic haze, but in cases such as smoke from fires or sandstorms, the haze may have its own pronounced coloration. In this paper we propose a new hazed image formation model that considers chromatic haze. Using this model we propose a new single image dehazing method CADCP that is based on color attenuation and dark channel priors. For quality assessment of the proposed method we generated a dataset of remotely sensed images with simulated chromatic haze. The generated dataset includes data with various haze spatial distribution and density. Quality evaluation results including qualitative and quantitative approaches demonstrated better results of the proposed method comparing with other existing methods. 
 Data-Based Intelligent Computing and Algorithm 
 Insights of anomaly detection: How does polluted training data influence performance?Jan Lehr,Martin Pape,Samuel Günther,et al.Show abstract 
 Anomaly detection is one of the most popular fields for computer vision in industrial applications. The idea of training machine learning only on defect-free objects saves enormous amounts of integration effort. The state of the art shows that current methods on public data sets (e.g. MVTec AD data set [1]) have already solved the problem with AUROC segmentation scores of more than 99%. In real-world applications training data is not as ”clean” as in public data sets. This work investigates the changes in detection performance when outliers end up in the training data. For this purpose, the training data is enriched step by step with images of defective objects. The AUROC score and the anomaly score is used as a quality criterion for performance measurement. We show that state of the art methods can be very robust, but that in some scenarios a draw down of 15 percentage points is possible. 
 href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13072/1307218/EANet-enhanced-attribute-based-RGBT-tracker-network/10.1117/12.3023347.full?webSyncID=0bb55ce6-cc22-203e-13e8-b0564a345d47&sessionGUID=6c5ccae6-7b2c-876a-b288-1a23526adca9" - EANet: enhanced attribute-based RGBT tracker network
Abbas Türkoğlu,Erdem AkagündüzShow abstract 
 Tracking objects can be a difficult task in computer vision, especially when faced with challenges such as occlusion, changes in lighting, and motion blur. Recent advances in deep learning have shown promise in challenging these conditions. However, most deep learning-based object trackers only use visible band (RGB) images. Thermal infrared electromagnetic waves (TIR) can provide additional information about an object, including its temperature, when faced with challenging conditions. We propose a deep learning-based image tracking approach that fuses RGB and thermal images (RGBT). The proposed model consists of two main components: a feature extractor and a tracker. The feature extractor encodes deep features from both the RGB and the TIR images. The tracker then uses these features to track the object using an enhanced attribute-based architecture. We propose a fusion of attribute-specific feature selection with an aggregation module. The proposed methods are evaluated on the RGBT234 [1] and LasHeR [2] datasets, which are the most widely used RGBT object-tracking datasets in the literature. The results show that the proposed system outperforms state-of-the-art RGBT object trackers on these datasets, with a relatively smaller number of parameters. 
 Portfolio selection via topological data analysisPetr Sokerin,Kristian Kuznetsov,Elizaveta Makhneva,et al.Show abstract 
 Portfolio management is an essential part of investment decision-making. However, traditional methods often fail to deliver reasonable performance. This problem stems from the inability of these methods to account for the unique characteristics of multivariate time series data from stock markets. We present a two-stage method for constructing an investment portfolio of common stocks. The method involves the generation of time series representations followed by their subsequent clustering. Our approach utilizes features based on Topological Data Analysis (TDA) for the generation of representations, allowing us to elucidate the topological structure within the data. Experimental results show that our proposed system outperforms other methods. This superior performance is consistent over different time frames, suggesting the viability of TDA as a powerful tool for portfolio selection. 
 A data parallel approach for distributed neural networks to achieve faster convergenceNagaraju C.,Yenda Ramesh,C. Krishna MohanShow abstract 
 The availability of large datasets has significantly contributed to recent advancements in deep Convolutional Neural Network (CNN) models. However, training a large CNN model using such datasets is a time-consuming task. This issue has been addressed by the parallelization and distribution of data/model during the training process. There are two ways to implement distributed deep learning processes: data parallelism and model parallelism. Data parallelism involves distributing the dataset across multiple workers, allowing them to process different portions simultaneously. While increasing the number of workers can reduce computation time, it also introduces additional communication time. In some cases, the increased communication time can outweigh the benefits gained from reduced computation time. In this paper, our focus is on reducing the overall computation time of data parallel approach by employing two strategies. First, we emphasize the preservation of dataset distribution across all workers, ensuring that each worker has access to representative data. Second, we explore the localization of parameters and the quantization of gradients to three levels: {-1, 0, 1} to reduce communication delays between the server and workers, as well as between workers themselves. By adopting these two strategies, we aim to enhance the performance of data parallel approach in the distributed deep learning processes. As a result of preserving the distribution of the data while sampling the entire data, each partition retains a similar mean and variance (capturing important first and second-order statistics). This approach guarantees that all worker machines train their local models on uniformly distributed data instead of random distribution. Additionally, localizing parameters limits the communication between the server and workers to gradients only. Furthermore, by quantizing gradients to 2-bits, we successfully achieve our objective of reducing computation time by enabling faster convergence without compromising test or validation accuracy. The experimental results demonstrate that employing these strategies in distributed deep learning effectively reduces communication overhead and leads to faster convergence when compared to methods that utilize random data sampling. These improvements were observed across multiple datasets such as MNIST, CIFAR-10, and Tiny ImageNet. 
 Quantum time series forecastingPrashant Gohel,Manjunath JoshiShow abstract 
 Merger of Quantum computing and Machine learning explores a new shift in artificial intelligence. Quantum neural networks and parameterized quantum circuits are tools that enable the merger of these two branches. Here, we use Quantum circuit enabled long short term memory (QLSTM) neural network to forecast time series data. We show the efficacy of quantum computing by conducting experiments on two different time series data sets. In our first experiment, we are predicting the amount of rainfall fall and the second experiment has electric load (power) prediction. Our dataset for rainfall prediction includes hourly information on the weather conditions i.e., wind speed, wind direction, minimum and maximum temperatures, and pressure with the amount of rain falls. For electric load (power generation) dataset, few of the features include amount of biomass, amount of fossil brown coal/ignite, amount of fossil coal derived gas, nuclear power, solar power, and the corresponding wind velocity forecasts. We compare the training as well as the test loss of classical Bidirectional LSTM (BILSTM) and the Quantum BILSTM and observe that LSTM based on quantum approach reduces both the training and test loss considerably when compared to its classical part with very few epochs of training. 
 CG-MER: a card game-based multimodal dataset for emotion recognitionNisrine Farhat,Amine Bohi,Leila Ben Letaifa,et al.Show abstract 
 The field of affective computing has seen significant advancements in exploring the relationship between emotions and emerging technologies. This paper presents a novel and valuable contribution to this field with the introduction of a comprehensive French multimodal dataset designed specifically for emotion recognition. The dataset encompasses three primary modalities: facial expressions, speech, and gestures, providing a holistic perspective on emotions. Moreover, the dataset has the potential to incorporate additional modalities, such as Natural Language Processing (NLP) to expand the scope of emotion recognition research. The dataset was curated through engaging participants in card game sessions, where they were prompted to express a range of emotions while responding to diverse questions. The study included 10 sessions with 20 participants (9 females and 11 males). The dataset serves as a valuable resource for furthering research in emotion recognition and provides an avenue for exploring the intricate connections between human emotions and digital technologies. 
 ABOUT 
 Mission 
 Leadership 
 Committees 
 History 
 Policies and Reporting 
 Jobs at SPIE 
 Donate to SPIE 
 RESOURCES 
 Join SPIE 
 Publications 
 Public Policy 
 SPIE Brand and Logos 
 SPIE Press Releases 
 SPIE Profiles 
 href="/documents/MediaKit/2025/SPIE-Media-Kit.pdf" - SPIE Media Kit 
 HELP 
 Contact + Help FAQs 
 Report an Incident 
 href="https://spie.org/communication/updates-and-newsletters" - Email Preferences 
 SUBSCRIBE TO OUR EMAILS 
 Receive only the information you want 
 Sign Up 
 Stay Connected | Get the App 
 © 2025 SPIE 
 SPIE Digital Library|SPIE Career Center|optics.org|Privacy PolicyTop of page

