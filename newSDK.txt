Hiện tại live chat của tôi đang dùng SDK cũ là google generative ai, hãy điều chỉnh để dùng bộ SDK mới genai của google

và điều chỉnh bổ sung thêm các tính năng mới của SDK gồm:

language code trong speech config để điều chỉnh đầu ra ngôn ngữ của audio
transcription cho input audio và output audio





// src/app/[locale]/chatbot/layout.tsx
"use client";

import { LiveAPIProvider } from '@/src/app/[locale]/chatbot/livechat/contexts/LiveAPIContext';
import MainLayoutComponent from '@/src/app/[locale]/chatbot/MainLayout';
import { LiveChatSettingsProvider } from '@/src/app/[locale]/chatbot/livechat/contexts/LiveChatSettingsContext';
import { API_URI } from '@/src/app/[locale]/chatbot/lib/constants';
import { usePathname } from '@/src/navigation';
import { useSettingsStore } from './stores/setttingsStore';
import { useEffect } from 'react';
import ChatbotErrorDisplay from './ChatbotErrorDisplay';

const API_KEY = process.env.NEXT_PUBLIC_GEMINI_API_KEY;

export default function ChatbotRootLayout({
  children,
}: {
  children: React.ReactNode;
}) {
  const unlocalizedPathname = usePathname();
  const setChatMode = useSettingsStore(state => state.setChatMode);
  const currentStoreChatMode = useSettingsStore(state => state.chatMode);

  const isLiveChatPage = unlocalizedPathname.includes('/chatbot/livechat');
  const isLandingChatbotPage = unlocalizedPathname === '/chatbot/landingchatbot'; // Thêm biến này cho rõ ràng

  useEffect(() => {
    const newMode = isLiveChatPage ? 'live' : 'regular';
    if (currentStoreChatMode !== newMode) {
      setChatMode(newMode);
    }
  }, [isLiveChatPage, currentStoreChatMode, setChatMode, unlocalizedPathname]);


  if (typeof API_KEY !== "string" || !API_KEY) {
    console.error("NEXT_PUBLIC_GEMINI_API_KEY is not set or invalid.");
    return (
      <div className= "flex h-screen items-center justify-center text-lg font-semibold text-red-600" >
      Configuration Error: Live Chat API Key is missing.Please contact support.
            </div>
        );
  }

  // Chỉ hiển thị ChatbotErrorDisplay nếu không phải là trang landing
  // Và chỉ khi các trang chatbot khác đang hoạt động (ví dụ, không phải lỗi config API_KEY)
  const shouldShowChatbotErrorDisplay = !isLandingChatbotPage;

  return (
    <>
    {/* Chỉ render ChatbotErrorDisplay nếu cần */ }
            { shouldShowChatbotErrorDisplay && <ChatbotErrorDisplay /> }

  {
    isLiveChatPage ? (
      <LiveAPIProvider url= { API_URI } apiKey = { API_KEY } >
        <LiveChatSettingsProvider>
        <MainLayoutComponent isLiveChatContextActive={ true }>
          { children }
          </MainLayoutComponent>
          </LiveChatSettingsProvider>
          </LiveAPIProvider>
            ) : isLandingChatbotPage ? ( // Sử dụng biến đã tạo
      // Trang landing, không có ChatbotErrorDisplay bao bọc trực tiếp ở đây nữa
      // (nó sẽ được bao bọc bởi điều kiện shouldShowChatbotErrorDisplay ở trên nếu logic thay đổi)
      // Với logic hiện tại, nó sẽ không có ChatbotErrorDisplay
      <>{ children } </>
    ) : (
      // Các trang chatbot regular khác
      <MainLayoutComponent isLiveChatContextActive= { false} >
      { children }
    </MainLayoutComponent>
            )
  }
  </>
    );
}







// src/app/[locale]/chatbot/livechat/contexts/LiveAPIContext.tsx
import { createContext, FC, ReactNode, useContext } from "react";
import { useLiveAPI, UseLiveAPIResults } from "../hooks/useLiveApi";

const LiveAPIContext = createContext<UseLiveAPIResults | undefined>(undefined);

export type LiveAPIProviderProps = {
  children: ReactNode;
  url?: string;
  apiKey: string;
};

export const LiveAPIProvider: FC<LiveAPIProviderProps> = ({
  url,
  apiKey,
  children,
}) => {
  const liveAPI = useLiveAPI({ url, apiKey });

  return (
    <LiveAPIContext.Provider value= { liveAPI } >
    { children }
    </LiveAPIContext.Provider>
  );
};

export const useLiveAPIContext = () => {
  const context = useContext(LiveAPIContext);
  if (!context) {
    throw new Error("useLiveAPIContext must be used wihin a LiveAPIProvider");
  }
  return context;
};




// src/app/[locale]/chatbot/livechat/contexts/LiveChatSettingsContext.tsx

"use client";

import React, { createContext, useContext, useState, ReactNode } from 'react';
import { OutputModality, PrebuiltVoice } from '../../lib/live-chat.types';
import { AVAILABLE_VOICES_LIVE_CHAT, DEFAULT_MODALITY_LIVE_CHAT, DEFAULT_VOICE_LIVE_CHAT } from '../../lib/constants';

// --- Interface cho Context Value ---
interface LiveChatSettingsContextType {
  currentModality: OutputModality;
  setCurrentModality: (modality: OutputModality) => void;
  currentVoice: PrebuiltVoice;
  setCurrentVoice: (voice: PrebuiltVoice) => void;
  availableVoices: PrebuiltVoice[];
  // --- THÊM TRẠNG THÁI KẾT NỐI VÀ SETTER ---
  isLiveChatConnected: boolean;
  setLiveChatConnected: (isConnected: boolean) => void;
}

interface LiveChatSettingsProviderProps {
  children: ReactNode;
}

const LiveChatSettingsContext = createContext<LiveChatSettingsContextType | undefined>(undefined);

export const useLiveChatSettings = (): LiveChatSettingsContextType => {
  const context = useContext(LiveChatSettingsContext);
  if (!context) {
    throw new Error('useLiveChatSettings must be used within a LiveChatSettingsProvider');
  }
  return context;
};

export const LiveChatSettingsProvider: React.FC<LiveChatSettingsProviderProps> = ({ children }) => {
  const [currentModality, setCurrentModality] = useState<OutputModality>(DEFAULT_MODALITY_LIVE_CHAT);
  const [currentVoice, setCurrentVoice] = useState<PrebuiltVoice>(DEFAULT_VOICE_LIVE_CHAT);
  // --- STATE MỚI CHO KẾT NỐI LIVE CHAT ---
  const [isLiveChatConnected, setLiveChatConnected] = useState<boolean>(false);

  return (
    <LiveChatSettingsContext.Provider
            value= {{
    currentModality,
      setCurrentModality,
      currentVoice,
      setCurrentVoice,
      availableVoices: AVAILABLE_VOICES_LIVE_CHAT,
        // --- TRUYỀN STATE VÀ SETTER MỚI VÀO CONTEXT VALUE ---
        isLiveChatConnected,
        setLiveChatConnected,
            }
}
        >
  { children }
  </LiveChatSettingsContext.Provider>
    );
};





// src/app/[locale]/chatbot/livechat/LiveChat.tsx
'use client'

import React, { useState, useRef, useEffect } from 'react'
import { useLiveAPIContext } from './contexts/LiveAPIContext'
import { useLoggerStore } from './lib/store-logger'
import { LiveChatAPIConfig } from './LiveChatAPIConfig'

// Hooks
import useConnection from './hooks/useConnection'
import useTimer from './hooks/useTimer'
import useLoggerScroll from './hooks/useLoggerScroll'
import useLoggerEvents from './hooks/useLoggerEvents'
import useAudioRecorder from './hooks/useAudioRecorder'
import useModelAudioResponse from './hooks/useModelAudioResponse'
import useVolumeControl from './hooks/useVolumeControl'
import useInteractionHandlers from './hooks/useInteractionHandlers'
import { useMessageSendingManager } from './hooks/useMessageSendingManager'
import { useInteractionState } from './hooks/useInteractionState'

// Layout Components
import ChatStatusDisplay from './layout/ChatStatusDisplay'
import ChatArea from './layout/ChatArea'
import ChatInputBar from './layout/ChatInputBar'

// Types and Constants
import { getSystemInstructions } from '../lib/instructions'
import { AudioRecorder } from './lib/audio-recorder'

// Contexts and Stores
import { useLiveChatSettings } from './contexts/LiveChatSettingsContext'
import { useChatSettingsState } from '@/src/app/[locale]/chatbot/stores/storeHooks'

export default function LiveChatExperience() {
  const {
    currentModality,
    currentVoice,
    setLiveChatConnected
  } = useLiveChatSettings()

  const { currentLanguage: currentLanguageOptionFromStore } =
    useChatSettingsState()
  const currentLanguageCode = currentLanguageOptionFromStore.code

  const {
    connected,
    isConnecting,
    streamStartTime,
    connectionStatusMessage,
    connectWithPermissions,
    handleDisconnect,
    handleReconnect,
    error: connectionError
  } = useConnection()

  const { elapsedTime, showTimer, handleCloseTimer } = useTimer(
    isConnecting,
    connected,
    streamStartTime
  )

  const { client, volume: micVolume, on, off } = useLiveAPIContext() // Renamed 'volume' to 'micVolume' for clarity
  const { log, clearLogs, logs } = useLoggerStore()

  const loggerRef = useRef<HTMLDivElement>(null)
  const [inVolume, setInVolume] = useState(0) // For microphone input volume visualization
  const [audioRecorder] = useState(() => new AudioRecorder())
  const [muted, setMuted] = useState(false)

  const { hasInteracted, recordInteraction } = useInteractionState({
    connected,
    isConnecting,
    streamStartTime,
  });

  const { isSendingMessage, startSending, stopSending } =
    useMessageSendingManager({ logs });

  const hasClearedLogsOnConnectRef = useRef(false); // <-- Thêm ref này


  const { handleSendMessage, handleStartVoice } = useInteractionHandlers({
    connected,
    connectWithPermissions,
    setMuted,
    client,
    log,
    startLoading: (sentLogIndex: number) => { // Pass startSending from the new hook
      startSending(sentLogIndex);
    },
    stopLoading: () => { // Pass stopSending from the new hook
      stopSending('direct send error');
    }
  })

  useEffect(() => {
    setLiveChatConnected(connected)
  }, [connected, setLiveChatConnected])

  useEffect(() => {
    if (connected) {
      if (!hasClearedLogsOnConnectRef.current) { // <-- Chỉ clear nếu chưa clear
        clearLogs();
        hasClearedLogsOnConnectRef.current = true; // <-- Đánh dấu đã clear
      }
      // Reset sending state if connection is re-established or established
      if (isSendingMessage) {
        stopSending('connection established/re-established');
      }
    } else {
      // Khi ngắt kết nối, reset lại cờ để lần kết nối sau sẽ clear log
      hasClearedLogsOnConnectRef.current = false;
    }
  }, [connected, isSendingMessage, clearLogs, stopSending]); // Thêm clearLogs, stopSending vào deps vì chúng được gọi


  // Core Hooks
  useLoggerScroll(loggerRef)
  useLoggerEvents(on, off, log)
  useAudioRecorder(connected, muted, audioRecorder, client, log, setInVolume)
  useModelAudioResponse(on, off, log)
  useVolumeControl(inVolume) // This likely controls the display of input volume

  const systemInstructions = getSystemInstructions(currentLanguageCode)

  // Determine connection status for UI
  let connectionStatusType: 'connected' | 'error' | 'info' | 'connecting' =
    'info'
  if (isConnecting) connectionStatusType = 'connecting'
  else if (connected) connectionStatusType = 'connected'
  else if (connectionStatusMessage || connectionError)
    connectionStatusType = streamStartTime !== null ? 'error' : 'info'

  const effectiveStatusMessage =
    connectionStatusMessage || connectionError?.message || null
  const shouldShowExternalStatus: boolean =
    showTimer ||
    !!(effectiveStatusMessage && connectionStatusType !== 'connected');
  const shouldShowRestartButton = connectionStatusType === 'error';


  const handleStartVoiceAndInteract = () => {
    handleStartVoice();
    recordInteraction();
  }

  return (
    <div className= 'relative flex h-full flex-col rounded-xl border-2 bg-white-pure shadow-inner' >
    <LiveChatAPIConfig
        outputModality={ currentModality }
  selectedVoice = { currentVoice }
  language = { currentLanguageCode }
  systemInstructions = { systemInstructions }
    />

    <ChatStatusDisplay
        statusType={ connectionStatusType }
  statusMessage = { effectiveStatusMessage }
  elapsedTime = { elapsedTime }
  onCloseTimer = { handleCloseTimer }
  showRestartButton = { shouldShowRestartButton }
  onRestartStream = { handleReconnect }
  showExternalStatus = { shouldShowExternalStatus }
    />

    <ChatArea
        connected={ connected }
  hasInteracted = { hasInteracted }
  onStartVoice = { handleStartVoiceAndInteract }
  loggerRef = { loggerRef }
    />

    <ChatInputBar
        connected={ connected }
  isConnecting = { isConnecting }
  connect = { connectWithPermissions }
  disconnect = { handleDisconnect }
  muted = { muted }
  setMuted = { setMuted }
  micVolume = { micVolume } // Pass the renamed micVolume
  onSendMessage = { handleSendMessage }
  isSendingMessage = { isSendingMessage }
    />
    </div>
  )
}






// src/app/[locale]/chatbot/livechat/LiveChatAPIConfig.tsx
"use client";
import { useEffect, memo } from "react";
import { useLiveAPIContext } from '@/src/app/[locale]/chatbot/livechat/contexts/LiveAPIContext';
import {
  englishGetConferencesDeclaration,
  englishGetJournalsDeclaration,
  englishGetWebsiteInfoDeclaration,
  englishNavigationDeclaration,
  englishManageFollowDeclaration,
  englishManageCalendarDeclaration,
  englishSendEmailToAdminDeclaration, // Retained for completeness, ensure handler exists if used
  englishOpenGoogleMapDeclaration,
} from "../lib/functions";
import { OutputModality, PrebuiltVoice, Language, ToolCall } from '@/src/app/[locale]/chatbot/lib/live-chat.types';
import { FunctionDeclaration as LiveChatFunctionDeclaration } from '@google/generative-ai';
import { appConfig } from "@/src/middleware";
import { usePathname } from 'next/navigation';
import { useLoggerStore } from '@/src/app/[locale]/chatbot/livechat/lib/store-logger';
import { ToolResponseMessage } from "./multimodal-live-types";
import { toolHandlers } from './services/tool.handlers'; // Import the handlers

// Define types for props
export type LiveChatAPIConfigProps = {
  outputModality: OutputModality;
  selectedVoice: PrebuiltVoice;
  language: Language;
  systemInstructions: string;
};

function LiveChatAPI({ outputModality, selectedVoice, language, systemInstructions }: LiveChatAPIConfigProps) {
  const { client, setConfig } = useLiveAPIContext();
  const { log: logToStore } = useLoggerStore();
  const pathname = usePathname();
  const currentLocale = pathname.split('/')[1];

  // Effect for setting up initial Live API client configuration
  useEffect(() => {
    console.log(`Configuring API for: Modality=${outputModality}, Voice=${selectedVoice}, Language=${language}, Locale=${currentLocale}`);
    const modalitiesConfig = outputModality === 'audio' ? ["AUDIO"] : ["TEXT"];
    const generationConfig: any = {
      responseModalities: modalitiesConfig,
    };
    if (outputModality === 'audio') {
      generationConfig.speechConfig = {
        voiceConfig: { prebuiltVoiceConfig: { voiceName: selectedVoice } },
      };
    }

    // Active function declarations (Consider making this dynamic based on language if needed in future)
    const activeFunctionDeclarations: LiveChatFunctionDeclaration[] = [
      englishGetConferencesDeclaration,
      englishGetJournalsDeclaration,
      englishGetWebsiteInfoDeclaration,
      englishNavigationDeclaration,
      englishManageFollowDeclaration,
      englishManageCalendarDeclaration,
      englishSendEmailToAdminDeclaration, // Keep if it's still a declared function, otherwise remove
      englishOpenGoogleMapDeclaration,
    ];

    const finalConfig = {
      model: "models/gemini-2.0-flash-live-001",
      generationConfig: generationConfig,
      systemInstruction: {
        parts: [{ text: systemInstructions }],
      },
      tools: [
        {
          functionDeclarations: activeFunctionDeclarations,
        },
      ],
    };
    console.log("Setting config with functions:", JSON.stringify(finalConfig.tools, null, 2));
    setConfig(finalConfig);
  }, [setConfig, outputModality, selectedVoice, language, systemInstructions, currentLocale]);

  // Effect for handling tool calls
  useEffect(() => {
    const onToolCall = async (toolCall: ToolCall) => {
      console.log(`Got toolcall`, toolCall);
      const NEXT_PUBLIC_DATABASE_URL = `${appConfig.NEXT_PUBLIC_DATABASE_URL}/api/v1` || "https://confhub.westus3.cloudapp.azure.com/api/v1";
      const NEXT_PUBLIC_FRONTEND_URL = appConfig.NEXT_PUBLIC_FRONTEND_URL || "http://localhost:8386";

      const handlerConfig = {
        databaseUrl: NEXT_PUBLIC_DATABASE_URL,
        frontendUrl: NEXT_PUBLIC_FRONTEND_URL,
        currentLocale: currentLocale,
      };

      const responses = [];

      for (const fc of toolCall.functionCalls) {
        try {
          const handler = toolHandlers[fc.name];
          if (handler) {
            console.log(`Executing handler for ${fc.name}`);
            const result = await handler(fc, handlerConfig);
            responses.push(result);
          } else {
            console.warn(`Unknown function call name received: ${fc.name}`);
            responses.push({
              response: { content: { type: "error", message: `Unknown function: ${fc.name}` } },
              id: fc.id
            });
          }
        } catch (error: any) {
          console.error(`Error processing function call ${fc.name}:`, error);
          let errorMessage = error.message || "An unexpected error occurred.";
          if (errorMessage.length > 300) errorMessage = errorMessage.substring(0, 297) + "...";
          responses.push({
            response: { content: { type: "error", message: `Failed to execute ${fc.name}: ${errorMessage}` } },
            id: fc.id
          });
        }
      }

      if (responses.length > 0) {
        console.log("Sending tool responses:", { functionResponses: responses });
        const toolResponseMessageForLog: ToolResponseMessage = {
          toolResponse: { functionResponses: responses }
        };
        logToStore({
          date: new Date(),
          type: "client.toolResponse",
          message: toolResponseMessageForLog,
          count: 1
        });
        client.sendToolResponse({ functionResponses: responses });
      } else {
        console.log("No responses generated for this tool call batch.");
      }
    };

    client.on("toolcall", onToolCall);
    return () => {
      client.off("toolcall", onToolCall);
    };
  }, [client, currentLocale, logToStore]); // Removed setConfig from dependencies as it's stable

  return null; // This component does not render UI
}

export const LiveChatAPIConfig = memo(LiveChatAPI);







// src/app/[locale]/chatbot/livechat/lib/multimodal-live-client.ts
import { Content, GenerativeContentBlob, Part } from "@google/generative-ai";
import { EventEmitter } from "eventemitter3";
import { difference } from "lodash";
import {
  ClientContentMessage,
  isInterrupted,
  isModelTurn,
  isServerContentMessage,
  isSetupCompleteMessage,
  isToolCallCancellationMessage,
  isToolCallMessage,
  isTurnComplete,
  LiveIncomingMessage,
  ModelTurn,
  RealtimeInputMessage,
  ServerContent,
  SetupMessage,
  StreamingLog,
  ToolCall,
  ToolCallCancellation,
  ToolResponseMessage,
  type LiveConfig,
  ServerAudioMessage,
} from "../multimodal-live-types";
import { blobToJSON, base64ToArrayBuffer } from "./utils";

/**
 * the events that this client will emit
 */
interface MultimodalLiveClientEventTypes {
  open: () => void;
  log: (log: StreamingLog) => void;
  close: (event: CloseEvent) => void;
  audio: (data: ArrayBuffer) => void;
  content: (data: ServerContent) => void;
  interrupted: () => void;
  setupcomplete: () => void;
  turncomplete: () => void;
  toolcall: (toolCall: ToolCall) => void;
  toolcallcancellation: (toolcallCancellation: ToolCallCancellation) => void;
}

export type MultimodalLiveAPIClientConnection = {
  url?: string;
  apiKey: string;
};

/**
 * A event-emitting class that manages the connection to the websocket and emits
 * events to the rest of the application.
 * If you dont want to use react you can still use this.
 */
export class MultimodalLiveClient extends EventEmitter<MultimodalLiveClientEventTypes> {
  public ws: WebSocket | null = null;
  protected config: LiveConfig | null = null;
  public url: string = "";
  public getConfig() {
    return { ...this.config };
  }

  constructor({ url, apiKey }: MultimodalLiveAPIClientConnection) {
    super();
    url =
      url ||
      `wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent`;
    url += `?key=${apiKey}`;
    this.url = url;
    this.send = this.send.bind(this);
  }

  // No longer needed, log events are handled in use-live-api
  // log(type: string, message: StreamingLog["message"]) {
  //   const log: StreamingLog = {
  //     date: new Date(),
  //     type,
  //     message,
  //   };
  //   this.emit("log", log);
  // }

  connect(config: LiveConfig): Promise<boolean> {
    this.config = config;

    const ws = new WebSocket(this.url);

    ws.addEventListener("message", async (evt: MessageEvent) => {
      if (evt.data instanceof Blob) {
        this.receive(evt.data);
      } else {
        console.log("non blob message", evt);
      }
    });
    return new Promise((resolve, reject) => {
      const onError = (ev: Event) => {
        this.disconnect(ws);
        const message = `Could not connect to "${this.url}"`;
        // this.log(`server.${ev.type}`, message);  // No longer needed
        reject(new Error(message));
      };
      ws.addEventListener("error", onError);
      ws.addEventListener("open", (ev: Event) => {
        if (!this.config) {
          reject("Invalid config sent to `connect(config)`");
          return;
        }
        // this.log(`client.${ev.type}`, `connected to socket`); // No longer needed
        this.emit("open");

        this.ws = ws;

        const setupMessage: SetupMessage = {
          setup: this.config,
        };
        this._sendDirect(setupMessage);
        // this.log("client.send", "setup"); // No longer needed

        ws.removeEventListener("error", onError);
        ws.addEventListener("close", (ev: CloseEvent) => {
          console.log(ev);
          this.disconnect(ws);
          let reason = ev.reason || "";
          if (reason.toLowerCase().includes("error")) {
            const prelude = "ERROR]";
            const preludeIndex = reason.indexOf(prelude);
            if (preludeIndex > 0) {
              reason = reason.slice(
                preludeIndex + prelude.length + 1,
                Infinity,
              );
            }
          }
          // this.log( // No longer needed
          //   `server.${ev.type}`,
          //   `disconnected ${reason ? `with reason: ${reason}` : ``}`,
          // );
          this.emit("close", ev);
        });
        resolve(true);
      });
    });
  }

  disconnect(ws?: WebSocket) {
    // could be that this is an old websocket and theres already a new instance
    // only close it if its still the correct reference
    if ((!ws || this.ws === ws) && this.ws) {
      this.ws.close();
      this.ws = null;
      // this.log("client.close", `Disconnected`); // No longer needed
      return true;
    }
    return false;
  }

  protected async receive(blob: Blob) {
    const response: LiveIncomingMessage = (await blobToJSON(
      blob,
    )) as LiveIncomingMessage;
    if (isToolCallMessage(response)) {
      // this.log("server.toolCall", response); // No longer needed
      this.emit("toolcall", response.toolCall);
      return;
    }
    if (isToolCallCancellationMessage(response)) {
      // this.log("receive.toolCallCancellation", response); // No longer needed
      this.emit("toolcallcancellation", response.toolCallCancellation);
      return;
    }

    if (isSetupCompleteMessage(response)) {
      // this.log("server.send", "setupComplete"); // No longer needed
      this.emit("setupcomplete");
      return;
    }

    if (isServerContentMessage(response)) {
      const { serverContent } = response;
      if (isInterrupted(serverContent)) {
        // this.log("receive.serverContent", "interrupted"); // No longer needed
        this.emit("interrupted");
        return;
      }
      if (isTurnComplete(serverContent)) {
        // this.log("server.send", "turnComplete"); // No longer needed
        this.emit("turncomplete");
        //plausible theres more to the message, continue
      }

      if (isModelTurn(serverContent)) {
        let parts: Part[] = serverContent.modelTurn.parts;

        const audioParts = parts.filter(
          (p) => p.inlineData && p.inlineData.mimeType.startsWith("audio/pcm"),
        );
        const base64s = audioParts.map((p) => p.inlineData?.data);

        const otherParts = difference(parts, audioParts);

        base64s.forEach((b64) => {
          if (b64) {
            const data = base64ToArrayBuffer(b64);
            this.emit("audio", data);
            // this.log(`server.audio`, `buffer (${data.byteLength})`); // No longer needed
          }
        });
        if (!otherParts.length) {
          return;
        }

        parts = otherParts;

        const content: ModelTurn = { modelTurn: { parts } };
        this.emit("content", content);
        // this.log(`server.content`, response); // No longer needed
      }
    } else {
      console.log("received unmatched message", response);
    }
  }

  sendRealtimeInput(chunks: GenerativeContentBlob[]) {
    let hasAudio = false;
    let hasVideo = false;
    for (let i = 0; i < chunks.length; i++) {
      const ch = chunks[i];
      if (ch.mimeType.includes("audio")) {
        hasAudio = true;
      }
      if (ch.mimeType.includes("image")) {
        hasVideo = true;
      }
      if (hasAudio && hasVideo) {
        break;
      }
    }
    const message =
      hasAudio && hasVideo
        ? "audio + video"
        : hasAudio
          ? "audio"
          : hasVideo
            ? "video"
            : "unknown";

    const data: RealtimeInputMessage = {
      realtimeInput: {
        mediaChunks: chunks,
      },
    };
    this._sendDirect(data);
    // this.log(`client.realtimeInput`, message); // No longer needed here
  }

  sendToolResponse(toolResponse: ToolResponseMessage["toolResponse"]) {
    const message: ToolResponseMessage = {
      toolResponse,
    };

    this._sendDirect(message);
    // this.log(`client.toolResponse`, message); // No longer needed
  }

  send(parts: Part | Part[], turnComplete: boolean = true) {
    parts = Array.isArray(parts) ? parts : [parts];
    const content: Content = {
      role: "user",
      parts,
    };

    const clientContentRequest: ClientContentMessage = {
      clientContent: {
        turns: [content],
        turnComplete,
      },
    };

    this._sendDirect(clientContentRequest);
    // this.log(`client.send`, clientContentRequest); // No longer needed
  }

  _sendDirect(request: object) {
    if (!this.ws) {
      throw new Error("WebSocket is not connected");
    }
    const str = JSON.stringify(request);
    this.ws.send(str);
  }
}








// src/app/[locale]/chatbot/livechat/hooks/useLiveApi.ts 
import { useCallback, useEffect, useMemo, useRef, useState } from "react";
import {
  MultimodalLiveAPIClientConnection,
  MultimodalLiveClient,
} from "../lib/multimodal-live-client";
import { LiveConfig, ServerAudioMessage, ServerContentMessage, ServerContent, ModelTurn } from "../multimodal-live-types"; // Import ModelTurn
import { AudioStreamer } from "../lib/audio-streamer";
import { audioContext } from "../lib/utils";
import VolMeterWorket from "../lib/worklets/vol-meter";
import EventEmitter from "eventemitter3";
import { debounce } from 'lodash';
import { Part } from "@google/generative-ai";

export type EventType =
  | "close"
  | "open"
  | "config"
  | "log"
  | "interrupted"
  | "audio"
  | "audioResponse"
  | "toolcall"
  | "text"
  | "serverError"
  | "generate"
  | "reset";


export type UseLiveAPIResults = {
  client: MultimodalLiveClient;
  setConfig: (config: LiveConfig) => void;
  config: LiveConfig;
  connected: boolean;
  connect: () => Promise<void>;
  disconnect: () => Promise<void>;
  volume: number;
  on: (event: EventType, callback: (...args: any[]) => void) => void;
  off: (event: EventType, callback: (...args: any[]) => void) => void;
};

export function useLiveAPI({
  url,
  apiKey,
}: MultimodalLiveAPIClientConnection): UseLiveAPIResults {
  const client = useMemo(
    () => new MultimodalLiveClient({ url, apiKey }),
    [url, apiKey],
  );
  const audioStreamerRef = useRef<AudioStreamer | null>(null); // Keep

  const [connected, setConnected] = useState(false);
  const [config, setConfig] = useState<LiveConfig>({
    model: "models/gemini-2.0-flash-live-001",
  });
  const [volume, setVolume] = useState(0);
  const [accumulatedServerAudio, setAccumulatedServerAudio] = useState("");
  const accumulatedServerAudioRef = useRef("");

  // *** NEW: Ref to accumulate text parts ***
  const accumulatedTextPartsRef = useRef<Part[]>([]);

  const emitter = useRef(new EventEmitter()).current;



  const debouncedEmitServerAudioLog = useCallback(
    debounce((audioData: string) => {
      const serverAudioMessage: ServerAudioMessage = {
        serverAudio: { audioData },
      };
      emitter.emit("log", {
        date: new Date(),
        type: "receive.serverAudio",
        message: serverAudioMessage,
      });
    }, 500),
    [emitter]
  );


  useEffect(() => {
    if (!audioStreamerRef.current) {
      audioContext({ id: "audio-out" }).then((audioCtx: AudioContext) => {
        audioStreamerRef.current = new AudioStreamer(audioCtx);
        audioStreamerRef.current
          .addWorklet<any>("vumeter-out", VolMeterWorket, (ev: any) => {
            setVolume(ev.data.volume);
          })
          .then(() => {
          });
      });
    }
  }, [audioStreamerRef]);

  useEffect(() => {
    const onClose = () => {
      setConnected(false);
      emitter.emit("close");
      setAccumulatedServerAudio("");
      accumulatedServerAudioRef.current = "";
      debouncedEmitServerAudioLog.cancel();
      accumulatedTextPartsRef.current = []; // Clear accumulated parts

    };

    const stopAudioStreamer = () => audioStreamerRef.current?.stop();

    const onAudio = (data: ArrayBuffer) => {
      audioStreamerRef.current?.addPCM16(new Uint8Array(data));
      const audioDataBase64 = btoa(String.fromCharCode(...new Uint8Array(data)));
      accumulatedServerAudioRef.current += audioDataBase64;
      setAccumulatedServerAudio((prev) => prev + audioDataBase64);
    };


    const onGenerate = (data: any) => {
      accumulatedServerAudioRef.current = "";
      setAccumulatedServerAudio("");
      accumulatedTextPartsRef.current = []; // Clear accumulated parts

      if (data.responseModalities && data.responseModalities.includes("audio")) {
        if (
          data.multimodalResponse &&
          data.multimodalResponse.parts &&
          data.multimodalResponse.parts.length
        ) {
          data.multimodalResponse.parts.forEach((part: Part) => {
            if (part.inlineData) {
              emitter.emit("audioResponse", {
                data: part.inlineData.data,
              });
            }
          });
        }
      }
      emitter.emit("generate", data);
    };

    const onInterrupted = () => {
      stopAudioStreamer();
      emitter.emit("interrupted");
      setAccumulatedServerAudio("");
      accumulatedServerAudioRef.current = "";
      debouncedEmitServerAudioLog.cancel();
      accumulatedTextPartsRef.current = []; // Clear accumulated parts
    };


    const onTurnComplete = () => {
      const audioData = accumulatedServerAudioRef.current;
      if (audioData) {
        const serverAudioMessage: ServerAudioMessage = {
          serverAudio: { audioData },
        };
        emitter.emit("log", {
          date: new Date(),
          type: "receive.serverAudio",
          message: serverAudioMessage,
        });
      }

      accumulatedServerAudioRef.current = "";
      setAccumulatedServerAudio("");
      emitter.emit("turncomplete");

      // *** NEW: Log accumulated text parts ***
      if (accumulatedTextPartsRef.current.length > 0) {
        const completeModelTurn: ModelTurn = {
          modelTurn: { parts: accumulatedTextPartsRef.current },
        };
        const serverContentMessage: ServerContentMessage = {
          serverContent: completeModelTurn,
        };
        emitter.emit("log", {
          date: new Date(),
          type: "receive.content",
          message: serverContentMessage,
        });
      }
      accumulatedTextPartsRef.current = []; // Clear for the next turn
    };

    const onContent = (data: ServerContent) => {
      // *** MODIFIED: Accumulate parts instead of logging immediately ***
      if (data && 'modelTurn' in data && data.modelTurn.parts) {
        accumulatedTextPartsRef.current = accumulatedTextPartsRef.current.concat(data.modelTurn.parts);
      }
    };


    client.on("close", onClose);
    client.on("interrupted", onInterrupted);
    client.on("audio", onAudio);
    client.on("log", (logData) => {
      emitter.emit("log", logData);
    });
    client.on("toolcall", (toolCallData) => {
      emitter.emit("toolcall", toolCallData);
    });
    client.on("setupcomplete", () => {
      emitter.emit("setupcomplete");
    });
    client.on("turncomplete", onTurnComplete);
    client.on("open", () => emitter.emit("open"));
    client.on("content", onContent);


    emitter.on("generate", onGenerate);

    return () => {
      client.off("close", onClose);
      client.off("interrupted", onInterrupted);
      client.off("audio", onAudio);
      client.off("log", (logData) => {
        emitter.emit("log", logData);
      });
      client.off("toolcall", (toolCallData) => {
        emitter.emit("toolcall", toolCallData);
      });
      client.off("setupcomplete", () => {
        emitter.emit("setupcomplete");
      });
      client.off("turncomplete", onTurnComplete);
      client.off("open", () => emitter.emit("open"));
      client.off("content", onContent);

      emitter.off("generate", onGenerate);
      debouncedEmitServerAudioLog.cancel();
      accumulatedTextPartsRef.current = []; // Clear accumulated parts

    };
  }, [client, audioStreamerRef, emitter, debouncedEmitServerAudioLog]);

  const connect = useCallback(async () => {
    if (!config) {
      throw new Error("config has not been set");
    }
    client.disconnect();
    await client.connect(config);
    setConnected(true);
  }, [client, setConnected, config]);

  const disconnect = useCallback(async () => {
    client.disconnect();
    setConnected(false);
  }, [setConnected, client]);

  return {
    client,
    config,
    setConfig,
    connected,
    connect,
    disconnect,
    volume,
    on: (event: EventType, callback: (...args: any[]) => void) => {
      emitter.on(event, callback);
    },
    off: (event: EventType, callback: (...args: any[]) => void) => {
      emitter.off(event, callback);
    },
  };
}









// src/app/[locale]/chatbot/livechat/lib/utils.ts
export type GetAudioContextOptions = AudioContextOptions & {
  id?: string;
};

const map: Map<string, AudioContext> = new Map();

export const audioContext: (
  options?: GetAudioContextOptions,
) => Promise<AudioContext> = (() => {
  const didInteract = new Promise((res) => {
    window.addEventListener("pointerdown", res, { once: true });
    window.addEventListener("keydown", res, { once: true });
  });

  return async (options?: GetAudioContextOptions) => {
    try {
      const a = new Audio();
      a.src =
        "data:audio/wav;base64,UklGRigAAABXQVZFZm10IBIAAAABAAEARKwAAIhYAQACABAAAABkYXRhAgAAAAEA";
      await a.play();
      if (options?.id && map.has(options.id)) {
        const ctx = map.get(options.id);
        if (ctx) {
          return ctx;
        }
      }
      const ctx = new AudioContext(options);
      if (options?.id) {
        map.set(options.id, ctx);
      }
      return ctx;
    } catch (e) {
      await didInteract;
      if (options?.id && map.has(options.id)) {
        const ctx = map.get(options.id);
        if (ctx) {
          return ctx;
        }
      }
      const ctx = new AudioContext(options);
      if (options?.id) {
        map.set(options.id, ctx);
      }
      return ctx;
    }
  };
})();

export const blobToJSON = (blob: Blob) =>
  new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onload = () => {
      if (reader.result) {
        const json = JSON.parse(reader.result as string);
        resolve(json);
      } else {
        reject("oops");
      }
    };
    reader.readAsText(blob);
  });

export function base64ToArrayBuffer(base64: string) {
  var binaryString = atob(base64);
  var bytes = new Uint8Array(binaryString.length);
  for (let i = 0; i < binaryString.length; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes.buffer;
}





// src/app/[locale]/chatbot/livechat/multimodal-live-types.ts
import type {
  Content,
  FunctionCall,
  GenerationConfig,
  GenerativeContentBlob,
  Part,
  Tool,
} from "@google/generative-ai";

/**
 * this module contains type-definitions and Type-Guards
 */

// Type-definitions

/* outgoing types */

/**
 * the config to initiate the session
 */
export type LiveConfig = {
  model: string;
  systemInstruction?: { parts: Part[] };
  generationConfig?: Partial<LiveGenerationConfig>;
  tools?: Array<Tool | { googleSearch: {} } | { codeExecution: {} }>;
};

export type PrebuiltVoice = "Puck" | "Charon" | "Kore" | "Fenrir" | "Aoede" | "Orus" | "Zephyr";
export type OutputModality = "text" | "audio" | "image";
export type Language = 'en' | 'vi' | 'zh';
export type ChatMode = 'live' | 'regular';


export type LiveGenerationConfig = GenerationConfig & {
  responseModalities: OutputModality;
  speechConfig?: {
    voiceConfig?: {
      prebuiltVoiceConfig?: {
        voiceName: PrebuiltVoice
      };
    };
  };
};


export type LiveOutgoingMessage =
  | SetupMessage
  | ClientContentMessage
  | RealtimeInputMessage
  | ToolResponseMessage;

export type SetupMessage = {
  setup: LiveConfig;
};

export type ClientContentMessage = {
  clientContent: {
    turns: Content[];
    turnComplete: boolean;
  };
};

export type RealtimeInputMessage = {
  realtimeInput: {
    mediaChunks: GenerativeContentBlob[];
  };
};

export type ToolResponseMessage = {
  toolResponse: {
    functionResponses: LiveFunctionResponse[];
  };
};

export type ToolResponse = ToolResponseMessage["toolResponse"];

export type LiveFunctionResponse = {
  response: object;
  id: string;
};

/** Incoming types */

export type LiveIncomingMessage =
  | ToolCallCancellationMessage
  | ToolCallMessage
  | ServerContentMessage
  | SetupCompleteMessage;

export type SetupCompleteMessage = { setupComplete: {} };

export type ServerContentMessage = {
  serverContent: ServerContent;
};

export type ServerContent = ModelTurn | TurnComplete | Interrupted;

export type ModelTurn = {
  modelTurn: {
    parts: Part[];
  };
};

export type TurnComplete = { turnComplete: boolean };

export type Interrupted = { interrupted: true };

export type ToolCallCancellationMessage = {
  toolCallCancellation: {
    ids: string[];
  };
};

export type ToolCallCancellation =
  ToolCallCancellationMessage["toolCallCancellation"];

export type ToolCallMessage = {
  toolCall: ToolCall;
};

export type LiveFunctionCall = FunctionCall & {
  id: string;
};

/**
 * A `toolCall` message
 */
export type ToolCall = {
  functionCalls: LiveFunctionCall[];
};

// ========= Audio Player Types ============
export type ClientAudioMessage = {
  clientAudio: {
    audioData: string; // Base64 encoded audio
  };
};

export type ServerAudioMessage = {
  serverAudio: {
    audioData: string;
  }
}

export function isClientAudioMessage(msg: any): msg is ClientAudioMessage {
  return msg && typeof msg === "object" && msg.clientAudio && typeof msg.clientAudio.audioData === 'string';
}

export function isServerAudioMessage(msg: any): msg is ServerAudioMessage {
  return msg && typeof msg === "object" && msg.serverAudio && typeof msg.serverAudio.audioData === 'string';
}

// =========================================

/** log types */
export type StreamingLog = {
  date: Date;
  type: string;
  count?: number;
  message: string | LiveOutgoingMessage | LiveIncomingMessage | ClientAudioMessage | ServerAudioMessage; // Include Audio Messages
};

// Type-Guards

const prop = (a: any, prop: string, kind: string = "object") =>
  typeof a === "object" && typeof a[prop] === "object";

// outgoing messages
export const isSetupMessage = (a: unknown): a is SetupMessage =>
  prop(a, "setup");

export const isClientContentMessage = (a: unknown): a is ClientContentMessage =>
  prop(a, "clientContent");

export const isRealtimeInputMessage = (a: unknown): a is RealtimeInputMessage =>
  prop(a, "realtimeInput");

export const isToolResponseMessage = (a: unknown): a is ToolResponseMessage =>
  prop(a, "toolResponse");

// incoming messages
export const isSetupCompleteMessage = (a: unknown): a is SetupCompleteMessage =>
  prop(a, "setupComplete");

export const isServerContentMessage = (a: any): a is ServerContentMessage =>
  prop(a, "serverContent");

export const isToolCallMessage = (a: any): a is ToolCallMessage =>
  prop(a, "toolCall");

export const isToolCallCancellationMessage = (
  a: unknown,
): a is ToolCallCancellationMessage =>
  prop(a, "toolCallCancellation") &&
  isToolCallCancellation((a as any).toolCallCancellation);

export const isModelTurn = (a: any): a is ModelTurn =>
  typeof (a as ModelTurn).modelTurn === "object";

export const isTurnComplete = (a: any): a is TurnComplete =>
  typeof (a as TurnComplete).turnComplete === "boolean";

export const isInterrupted = (a: any): a is Interrupted =>
  (a as Interrupted).interrupted;

export function isToolCall(value: unknown): value is ToolCall {
  if (!value || typeof value !== "object") return false;

  const candidate = value as Record<string, unknown>;

  return (
    Array.isArray(candidate.functionCalls) &&
    candidate.functionCalls.every((call) => isLiveFunctionCall(call))
  );
}

export function isToolResponse(value: unknown): value is ToolResponse {
  if (!value || typeof value !== "object") return false;

  const candidate = value as Record<string, unknown>;

  return (
    Array.isArray(candidate.functionResponses) &&
    candidate.functionResponses.every((resp) => isLiveFunctionResponse(resp))
  );
}

export function isLiveFunctionCall(value: unknown): value is LiveFunctionCall {
  if (!value || typeof value !== "object") return false;

  const candidate = value as Record<string, unknown>;

  return (
    typeof candidate.name === "string" &&
    typeof candidate.id === "string" &&
    typeof candidate.args === "object" &&
    candidate.args !== null
  );
}

export function isLiveFunctionResponse(
  value: unknown,
): value is LiveFunctionResponse {
  if (!value || typeof value !== "object") return false;

  const candidate = value as Record<string, unknown>;

  return (
    typeof candidate.response === "object" && typeof candidate.id === "string"
  );
}

export const isToolCallCancellation = (
  a: unknown,
): a is ToolCallCancellationMessage["toolCallCancellation"] =>
  typeof a === "object" && Array.isArray((a as any).ids);








// src/app/[locale]/chatbot/MainLayout.tsx
'use client'

import React, { useEffect, useRef } from 'react';
import LeftPanel from './LeftPanel';
import RightSettingsPanel from './RightPanel';
import { useTranslations } from 'next-intl';
import { AppPathname } from '@/src/navigation';
import { useSearchParams } from 'next/navigation';
import {
  useConversationStore,
} from './stores';
import { useShallow } from 'zustand/react/shallow';

// Import custom hooks
import { useAppInitialization } from '@/src/hooks/regularchat/useAppInitialization';
import { useUrlConversationSync } from '@/src/hooks/regularchat/useUrlConversationSync';
import { useConversationLifecycleManager } from '@/src/hooks/regularchat/useConversationLifecycleManager';
import { useChatViewManager } from '@/src/hooks/regularchat/useChatViewManager';
import { useConversationActions } from '@/src/hooks/regularchat/useConversationActions';

// Import child components
import DeletionOverlay from './regularchat/DeletionOverlay';
import SettingsToggleButton from './regularchat/SettingToggleButton';

interface MainLayoutComponentProps {
  children: React.ReactNode;
  isLiveChatContextActive?: boolean;
}

export const CHATBOT_HISTORY_PATH: AppPathname = '/chatbot/history';
export const CHATBOT_LIVECHAT_PATH: AppPathname = '/chatbot/livechat';
export const CHATBOT_REGULARCHAT_PATH: AppPathname = '/chatbot/regularchat';


export default function MainLayoutComponent({
  children,
  isLiveChatContextActive
}: MainLayoutComponentProps) {
  const t = useTranslations();
  const searchParamsHook = useSearchParams();

  // --- Store Hooks  ---
  const { activeConversationId } = useConversationStore(
    useShallow(state => ({
      activeConversationId: state.activeConversationId,
    }))
  );

  // --- Custom Hooks for Logic Management ---
  useAppInitialization();
  const { currentView } = useChatViewManager();
  const {
    handleSelectConversation,
    handleStartNewConversation,
    handleDeleteConversation,
    isProcessingDeletion,
    idBeingDeleted,
    resetDeletionState,
  } = useConversationActions({ currentView });


  const { didAttemptLoadFromUrlRef } = useUrlConversationSync({
    currentView,
    isProcessingDeletion,
    idBeingDeleted,
  });


  // --- Refs needed by hooks ---
  const prevActiveIdRef = useRef<string | null>(null);

  useEffect(() => {
    prevActiveIdRef.current = activeConversationId;
  }, [activeConversationId]);

  useConversationLifecycleManager({
    currentView,
    isProcessingDeletion,
    idBeingDeleted,
    prevActiveIdRef,
    urlIdParam: searchParamsHook.get('id'),
    searchParamsString: searchParamsHook.toString(),
    onDeletionProcessed: () => {
      console.log("[MainLayout] onDeletionProcessed called from LifecycleManager. Resetting deletion state.");
      resetDeletionState();
    },
    didAttemptLoadFromUrlRef: didAttemptLoadFromUrlRef,
    onNotFoundProcessed: () => {
      console.log("[MainLayout] onNotFoundProcessed called from LifecycleManager.");
      if (didAttemptLoadFromUrlRef.current) {
        didAttemptLoadFromUrlRef.current = false;
        console.log("[MainLayout] Reset didAttemptLoadFromUrlRef to false.");
      }
    },
  });


  return (
    <div className= 'bg-gray-10 flex h-screen overflow-hidden' >
    <LeftPanel
        onSelectConversation={ handleSelectConversation }
  onStartNewConversation = { handleStartNewConversation }
  onDeleteConversation = { handleDeleteConversation }
  currentView = { currentView }
  deletingConversationId = { isProcessingDeletion? idBeingDeleted: null }
    />

    <main className='relative flex-1 overflow-hidden transition-all duration-300 ease-in-out' >
      <DeletionOverlay
          isVisible={ isProcessingDeletion && idBeingDeleted !== null && currentView === 'chat' && activeConversationId === idBeingDeleted }
        />
    < div className = 'h-full w-full' > { children } </div>
      </main>

      < RightSettingsPanel
  isLiveChatContextActive = { isLiveChatContextActive }
    />

    <SettingsToggleButton isProcessingDeletion={ isProcessingDeletion } />
      </div>
  );
}